"title","year","venue","authors","type","url","abstract"
"Analyzing Quantum Programs with LintQ: A Static Analysis Framework for Qiskit.","2024","Proc. ACM Softw. Eng.","Matteo Paltenghi, Michael Pradel","Journal Articles","https://dblp.org/rec/journals/pacmse/PaltenghiP24","As quantum computing is rising in popularity, the amount of quantum programs and the number of developers writing them are increasing rapidly. Unfortunately, writing correct quantum programs is challenging due to various subtle rules developers need to be aware of. Empirical studies show that 40–82% of all bugs in quantum software are specific to the quantum domain. Yet, existing static bug detection frameworks are mostly unaware of quantum-specific concepts, such as circuits, gates, and qubits, and hence miss many bugs. This paper presents LintQ, a comprehensive static analysis framework for detecting bugs in quantum programs. Our approach is enabled by a set of abstractions designed to reason about common concepts in quantum computing without referring to the details of the underlying quantum computing platform. Built on top of these abstractions, LintQ offers an extensible set of ten analyses that detect likely bugs, such as operating on corrupted quantum states, redundant measurements, and incorrect compositions of sub-circuits. We apply the approach to a newly collected dataset of 7,568 real-world Qiskit-based quantum programs, showing that LintQ effectively identifies various programming problems, with a precision of 91.0% in its default configuration with the six best performing analyses. Comparing to a general-purpose linter and two existing quantum-aware techniques shows that almost all problems (92.1%) found by LintQ during our evaluation are missed by prior work. LintQ hence takes an important step toward reliable software in the growing field of quantum computing."
"Calibration and Correctness of Language Models for Code.","2024","CoRR","Claudio Spiess, David Gros, Kunal Suresh Pai, Michael Pradel, Md. Rafiqul Islam Rabin, Amin Alipour, Susmit Jha, Prem Devanbu, Toufique Ahmed","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2402-02047",""
"Can LLMs Replace Manual Annotation of Software Engineering Artifacts?","2024","CoRR","Toufique Ahmed, Premkumar T. Devanbu, Christoph Treude, Michael Pradel","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2408-05534","Experimental evaluations of software engineering innovations, e.g., tools and processes, often include human-subject studies as a component of a multi-pronged strategy to obtain greater generalizability of the findings. However, human-subject studies in our field are challenging, due to the cost and difficulty of finding and employing suitable subjects, ideally, professional programmers with varying degrees of experience. Meanwhile, large language models (LLMs) have recently started to demonstrate human-level performance in several areas. This paper explores the possibility of substituting costly human subjects with much cheaper LLM queries in evaluations of code and code-related artifacts. We study this idea by applying six state-of-the-art LLMs to ten annotation tasks from five datasets created by prior work, such as judging the accuracy of a natural language summary of a method or deciding whether a code change fixes a static analysis warning. Our results show that replacing some human annotation effort with LLMs can produce inter-rater agreements equal or close to human-rater agreement. To help decide when and how to use LLMs in human-subject studies, we propose model-model agreement as a predictor of whether a given task is suitable for LLMs at all, and model confidence as a means to select specific samples where LLMs can safely replace human annotators. Overall, our work is the first step toward mixed human-LLM evaluations in software engineering."
"Code Search (Dagstuhl Seminar 24172).","2024","Dagstuhl Reports","Satish Chandra 0001, Michael Pradel, Kathryn T. Stolee","Informal and Other Publications","https://dblp.org/rec/journals/dagstuhl-reports/0001PS24",""
"De-Hallucinator: Iterative Grounding for LLM-Based Code Completion.","2024","CoRR","Aryaz Eghbali, Michael Pradel","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2401-01701","Large languages models (LLMs) trained on datasets of publicly available source code have established a new state-of-the-art in code completion. However, these models are mostly unaware of the code that already exists within a specific project, preventing the models from making good use of existing APIs. Instead, LLMs often invent, or “hallucinate”, non-existent APIs or produce variants of already existing code. Although the API information is available to IDEs, the input size limit of LLMs prevents code completion techniques from including all relevant context into the prompt. This paper presents De-Hallucinator, an LLM-based code completion technique that grounds the predictions of a model through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt. The approach exploits the observation that LLMs often predict code that resembles the desired completion, but that fails to correctly refer to already existing APIs. De-Hallucinator automatically identifies project-specific API references related to the code prefix and to the model’s initial predictions and adds these references into the prompt. Our evaluation applies the approach to the task of predicting API usages in open-source Python projects. We show that De-Hallucinator consistently improves the predicted code across four state-of-the-art LLMs compared to querying the model only with the code before the cursor. In particular, the approach improves the edit distance of the predicted code by 23–51% and the recall of correctly predicted API usages by 24–61% relative to the baseline."
"DyPyBench: A Benchmark of Executable Python Software.","2024","Proc. ACM Softw. Eng.","Islem Bouzenia, Bajaj Piyush Krishan, Michael Pradel","Journal Articles","https://dblp.org/rec/journals/pacmse/BouzeniaKP24","Python has emerged as one of the most popular programming languages, extensively utilized in domains such 
 
as machine learning, data analysis, and web applications. Python’s dynamic nature and extensive usage make 
 
it an attractive candidate for dynamic program analysis. However, unlike for other popular languages, there 
 
currently is no comprehensive benchmark suite of executable Python projects, which hinders the development 
 
of dynamic analyses. This work addresses this gap by presenting DyPyBench, the first benchmark of Python 
 
projects that is large-scale, diverse, ready-to-run (i.e., with fully configured and prepared test suites), and ready- 
 
to-analyze (by integrating with the DynaPyt dynamic analysis framework). The benchmark encompasses 50 
 
popular open-source projects from various application domains, with a total of 681k lines of Python code, 
 
and 30k test cases. DyPyBench enables various applications in testing and dynamic analysis, of which we 
 
explore three in this work: (i) Gathering dynamic call graphs and empirically comparing them to statically 
 
computed call graphs, which exposes and quantifies limitations of existing call graph construction techniques 
 
for Python. (ii) Using DyPyBench to build a training data set for LExecutor, a neural model that learns to 
 
predict values that otherwise would be missing at runtime. (iii) Using dynamically gathered execution traces 
 
to mine API usage specifications, which establishes a baseline for future work on specification mining for 
 
Python. We envision DyPyBench to provide a basis for other dynamic analyses and for studying the runtime 
 
behavior of Python code."
"Fuzz4All: Universal Fuzzing with Large Language Models.","2024","ICSE","Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, Lingming Zhang 0001","Conference and Workshop Papers","https://dblp.org/rec/conf/icse/XiaPTP024",""
"Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2024, Vienna, Austria, September 16-20, 2024","2024","ISSTA","Maria Christakis, Michael Pradel","Editorship","https://dblp.org/rec/conf/issta/2024",""
"PyTy: Repairing Static Type Errors in Python.","2024","ICSE","Yiu Wai Chow, Luca Di Grazia, Michael Pradel","Conference and Workshop Papers","https://dblp.org/rec/conf/icse/ChowGP24","Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based re-pair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice."
"RepairAgent: An Autonomous, LLM-Based Agent for Program Repair.","2024","CoRR","Islem Bouzenia, Premkumar T. Devanbu, Michael Pradel","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2403-17134","Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270,000 tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering."
"Resource Usage and Optimization Opportunities in Workflows of GitHub Actions.","2024","ICSE","Islem Bouzenia, Michael Pradel","Conference and Workshop Papers","https://dblp.org/rec/conf/icse/BouzeniaP24",""
"Robust and fast stochastic 4D flow vector-field signature technique for quantifying composite flow dynamics from 4D flow MRI: Application to left atrial flow in atrial fibrillation.","2024","Medical Image Anal.","Thara Nallamothu, Maurice Pradella, Michael Markl 0001, Philip Greenland, Rod S. Passman, Mohammed S. M. ElBaz","Journal Articles","https://dblp.org/rec/journals/mia/NallamothuPMGPE24",""
"Wasm-R3: Record-Reduce-Replay for Realistic and Standalone WebAssembly Benchmarks.","2024","CoRR","Doehyun Baek, Jakob Getz, Yusung Sim, Daniel Lehmann 0002, Ben L. Titzer, Sukyoung Ryu, Michael Pradel","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2409-00708","WebAssembly (Wasm for short) brings a new, powerful capability to the web as well as Edge, IoT, and embedded systems. Wasm is a portable, compact binary code format with high performance and robust sandboxing properties. As Wasm applications grow in size and importance, the complex performance characteristics of diverse Wasm engines demand robust, representative benchmarks for proper tuning. Stopgap benchmark suites, such as PolyBenchC and libsodium, continue to be used in the literature, though they are known to be unrepresentative. Porting of more complex suites remains difficult because Wasm lacks many system APIs and extracting real-world Wasm benchmarks from the web is difficult due to complex host interactions. To address this challenge, we introduce Wasm-R3, the first record and replay technique for Wasm. Wasm-R3 transparently injects instrumentation into Wasm modules to record an execution trace from inside the module, then reduces the execution trace via several optimizations, and finally produces a replay module that is executable standalone without any host environment-on any engine. The benchmarks created by our approach are (i) realistic, because the approach records real-world web applications, (ii) faithful to the original execution, because the replay benchmark includes the unmodified original code, only adding emulation of host interactions, and (iii) standalone, because the replay benchmarks run on any engine. Applying Wasm-R3 to web-based Wasm applications in the wild demonstrates the correctness of our approach as well as the effectiveness of our optimizations, which reduce the recorded traces by 99.53% and the size of the replay benchmark by 9.98%. We release the resulting benchmark suite of 27 applications, called Wasm-R3-Bench, to the community, to inspire a new generation of realistic and standalone Wasm benchmarks."
"Code Search: A Survey of Techniques for Finding Code.","2023","ACM Comput. Surv.","Luca Di Grazia, Michael Pradel","Journal Articles","https://dblp.org/rec/journals/csur/GraziaP23","The immense amounts of source code provide ample challenges and opportunities during software development. To handle the size of code bases, developers commonly search for code, e.g., when trying to find where a particular feature is implemented or when looking for code examples to reuse. To support developers in finding relevant code, various code search engines have been proposed. This article surveys 30 years of research on code search, giving a comprehensive overview of challenges and techniques that address them. We discuss the kinds of queries that code search engines support, how to preprocess and expand queries, different techniques for indexing and retrieving code, and ways to rank and prune search results. Moreover, we describe empirical studies of code search in practice. Based on the discussion of prior work, we conclude the article with an outline of challenges and opportunities to be addressed in the future."
"DiffSearch: A Scalable and Precise Search Engine for Code Changes.","2023","IEEE Trans. Software Eng.","Luca Di Grazia, Paul Bredl, Michael Pradel","Journal Articles","https://dblp.org/rec/journals/tse/GraziaBP23",""
"MorphQ: Metamorphic Testing of the Qiskit Quantum Computing Platform.","2023","ICSE","Matteo Paltenghi, Michael Pradel","Conference and Workshop Papers","https://dblp.org/rec/conf/icse/PaltenghiP23","As quantum computing is becoming increasingly popular, the underlying quantum computing platforms are growing both in ability and complexity. Unfortunately, testing these platforms is challenging due to the relatively small number of existing quantum programs and because of the oracle problem, i.e., a lack of specifications of the expected behavior of programs. This paper presents MorphQ, the first metamorphic testing approach for quantum computing platforms. Our two key contributions are (i) a program generator that creates a large and diverse set of valid (i.e., non-crashing) quantum programs, and (ii) a set of program transformations that exploit quantum-specific metamorphic relationships to alleviate the oracle problem. Evaluating the approach by testing the popular Qiskit platform shows that the approach creates over 8k program pairs within two days, many of which expose crashes. Inspecting the crashes, we find 13 bugs, nine of which have already been confirmed. MorphQ widens the slim portfolio of testing techniques of quantum computing platforms, helping to create a reliable software stack for this increasingly important field."
"SecBench.js: An Executable Security Benchmark Suite for Server-Side JavaScript.","2023","ICSE","Masudul Hasan Masud Bhuiyan, Adithya Srinivas Parthasarathy, Nikos Vasilakis, Michael Pradel, Cristian-Alexandru Staicu","Conference and Workshop Papers","https://dblp.org/rec/conf/icse/BhuiyanPVPS23","NPM is the largest software ecosystem in the world, offering millions of free, reusable packages. In recent years, various security threats to packages published on npm have been reported, including vulnerabilities that affect millions of users. To continuously improve techniques for detecting vulnerabilities and mitigating attacks that exploit them, a reusable benchmark of vulnerabilities would be highly desirable. Ideally, such a benchmark should be realistic, come with executable exploits, and include fixes of vulnerabilities. Unfortunately, there currently is no such benchmark, forcing researchers to repeatedly develop their own evaluation datasets and making it difficult to compare techniques with each other. This paper presents SecBench.js,, the first comprehensive benchmark suite of vulnerabilities and executable exploits for npm. The benchmark comprises 600 vulnerabilities, which cover the five most common vulnerability classes for server-side JavaScript. Each vulnerability comes with a payload that exploits the vulnerability and an oracle that validates successful exploitation. SecBench.js, enables various applications, of which we explore three in this paper: (i) cross-checking SecBench.js, against public security advisories reveals 168 vulnerable versions in 19 packages that are mislabeled in the advisories; (ii) applying simple code transformations to the exploits in our suite helps identify flawed fixes of vulnerabilities; (iii) dynamically analyzing calls to common sink APIs, e.g., exec(), yields a ground truth of code locations for evaluating vulnerability detectors. Beyond providing a reusable benchmark to the community, our work identified 20 zero-day vulnerabilities, most of which are already acknowledged by practitioners."
"That's a Tough Call: Studying the Challenges of Call Graph Construction for WebAssembly.","2023","ISSTA","Daniel Lehmann 0002, Michelle Thalakottur, Frank Tip, Michael Pradel","Conference and Workshop Papers","https://dblp.org/rec/conf/issta/0002TTP23",""
"VULGEN: Realistic Vulnerability Generation Via Pattern Mining and Deep Learning.","2023","ICSE","Yu Nong, Yuzhe Ou, Michael Pradel, Feng Chen 0001, Haipeng Cai","Conference and Workshop Papers","https://dblp.org/rec/conf/icse/NongOPCC23","Building new, powerful data-driven defenses against prevalent software vulnerabilities needs sizable, quality vulnerability datasets, so does large-scale benchmarking of existing defense solutions. Automatic data generation would promisingly meet the need, yet there is little work aimed to generate much-needed quality vulnerable samples. Meanwhile, existing similar and adaptable techniques suffer critical limitations for that purpose. In this paper, we present VULGEN, the first injection-based vulnerability-generation technique that is not limited to a particular class of vulnerabilities. VULGEN combines the strengths of deterministic (pattern-based) and probabilistic (deep-learning/DL-based) program transformation approaches while mutually overcoming respective weaknesses. This is achieved through close collaborations between pattern mining/application and DL-based injection localization, which separates the concerns with how and where to inject. By leveraging large, pretrained programming language modeling and only learning locations, VULGEN mitigates its own needs for quality vulnerability data (for training the localization model). Extensive evaluations show that VULGEN significantly outperforms a state-of-the-art (SOTA) pattern-based peer technique as well as both Transformer- and GNN-based approaches in terms of the percentages of generated samples that are vulnerable and those also exactly matching the ground truth (by 38.0-430.1% and 16.3-158.2%, respectively). The VULGEN-generated samples led to substantial performance improvements for two SOTA DL-based vulnerability detectors (by up to 31.8% higher in F1), close to those brought by the ground-truth real-world samples and much higher than those by the same numbers of existing synthetic samples."
"When to Say What: Learning to Find Condition-Message Inconsistencies.","2023","ICSE","Islem Bouzenia, Michael Pradel","Conference and Workshop Papers","https://dblp.org/rec/conf/icse/BouzeniaP23","Programs often emit natural language messages, e.g., in logging statements or exceptions raised on unexpected paths. To be meaningful to users and developers, the message, i.e., what to say, must be consistent with the condition under which it gets triggered, i.e., when to say it. However, checking for inconsistencies between conditions and messages is challenging because the conditions are expressed in the logic of the programming language, while messages are informally expressed in natural language. This paper presents CMI-Finder, an approach for detecting condition-message inconsistencies. CMI-Finder is based on a neural model that takes a condition and a message as its input and then predicts whether the two are consistent. To address the problem of obtaining realistic, diverse, and large-scale training data, we present six techniques to generate large numbers of inconsistent examples to learn from automatically. Moreover, we describe and compare three neural models, which are based on binary classification, triplet loss, and fine-tuning, respectively. Our evaluation applies the approach to 300K condition-message statements extracted from 42 million lines of Python code. The best model achieves a precision of 78% at a recall of 72% on a dataset of past bug fixes. Applying the approach to the newest versions of popular open-source projects reveals 50 previously unknown bugs, 19 of which have been confirmed by the developers so far."
"Beware of the Unexpected: Bimodal Taint Analysis.","2023","ISSTA","Yiu Wai Chow, Max Schäfer, Michael Pradel","Conference and Workshop Papers","https://dblp.org/rec/conf/issta/ChowSP23","Static analysis is a powerful tool for detecting security vulnerabilities and other programming problems. Global taint tracking, in particular, can spot vulnerabilities arising from complicated data flow across multiple functions. However, precisely identifying which flows are problematic is challenging, and sometimes depends on factors beyond the reach of pure program analysis, such as conventions and informal knowledge. For example, learning that a parameter name of an API function locale ends up in a file path is surprising and potentially problematic. In contrast, it would be completely unsurprising to find that a parameter command passed to an API function execaCommand is eventually interpreted as part of an operating-system command. This paper presents Fluffy, a bimodal taint analysis that combines static analysis, which reasons about data flow, with machine learning, which probabilistically determines which flows are potentially problematic. The key idea is to let machine learning models predict from natural language information involved in a taint flow, such as API names, whether the flow is expected or unexpected, and to inform developers only about the latter. We present a general framework and instantiate it with four learned models, which offer different trade-offs between the need to annotate training data and the accuracy of predictions. We implement Fluffy on top of the CodeQL analysis framework and apply it to 250K JavaScript projects. Evaluating on five common vulnerability types, we find that Fluffy achieves an F1 score of 0.85 or more on four of them across a variety of datasets."
"LExecutor: Learning-Guided Execution.","2023","ESEC/SIGSOFT FSE","Beatriz Souza, Michael Pradel","Conference and Workshop Papers","https://dblp.org/rec/conf/sigsoft/SouzaP23","Executing code is essential for various program analysis tasks, e.g., to detect bugs that manifest through exceptions or to obtain execution traces for further dynamic analysis. How- ever, executing an arbitrary piece of code is often difficult in practice, e.g., because of missing variable definitions, miss- ing user inputs, and missing third-party dependencies. This paper presents LExecutor, a learning-guided approach for executing arbitrary code snippets in an underconstrained way. The key idea is to let a neural model predict missing values that otherwise would cause the program to get stuck, and to inject these values into the execution. For example, LExecutor injects likely values for otherwise undefined vari- ables and likely return values of calls to otherwise missing functions. We evaluate the approach on Python code from popular open-source projects and on code snippets extracted from Stack Overflow. The neural model predicts realistic values with an accuracy between 79.5% and 98.2%, allowing LExecutor to closely mimic real executions. As a result, the approach successfully executes significantly more code than any available technique, such as simply executing the code as-is. For example, executing the open-source code snippets as-is covers only 4.1% of all lines, because the code crashes early on, whereas LExecutor achieves a coverage of 51.6%."
"LintQ: A Static Analysis Framework for Qiskit Quantum Programs.","2023","CoRR","Matteo Paltenghi, Michael Pradel","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2310-00718","As quantum computing is rising in popularity, the amount of quantum programs and the number of developers writing them are increasing rapidly. Unfortunately, writing correct quantum programs is challenging due to various subtle rules developers need to be aware of. Empirical studies show that 40–82% of all bugs in quantum software are specific to the quantum domain. Yet, existing static bug detection frameworks are mostly unaware of quantum-specific concepts, such as circuits, gates, and qubits, and hence miss many bugs. This paper presents LintQ, a comprehensive static analysis framework for detecting bugs in quantum programs. Our approach is enabled by a set of abstractions designed to reason about common concepts in quantum computing without referring to the details of the underlying quantum computing platform. Built on top of these abstractions, LintQ offers an extensible set of ten analyses that detect likely bugs, such as operating on corrupted quantum states, redundant measurements, and incorrect compositions of sub-circuits. We apply the approach to a newly collected dataset of 7,568 real-world Qiskit-based quantum programs, showing that LintQ effectively identifies various programming problems, with a precision of 91.0% in its default configuration with the six best performing analyses. Comparing to a general-purpose linter and two existing quantum-aware techniques shows that almost all problems (92.1%) found by LintQ during our evaluation are missed by prior work. LintQ hence takes an important step toward reliable software in the growing field of quantum computing."
"Practical Privacy-Preserving Machine Learning using Fully Homomorphic Encryption.","2023","IACR Cryptol. ePrint Arch.","Michael Brand, Gaëtan Pradel","Informal and Other Publications","https://dblp.org/rec/journals/iacr/BrandP23a","Machine learning is a widely-used tool for analysing large datasets, but increasing public demand for privacy preservation and the corresponding introduction of privacy regulations have severely limited what data can be analysed, even when this analysis is for societal benefit. Homomorphic encryption, which allows computation on encrypted data, is a natural solution to this dilemma, allowing data to be analysed without sacrificing privacy. Because homomorphic encryption is computationally expensive, however, current solutions are mainly restricted to use it for inference and not training. In this work, we present a practically viable approach to privacy-preserving machine learning training using fully homomorphic encryption. Our method achieves fast training speeds, taking less than 45 seconds to train a binary classifier over thousands of samples on a single mid-range computer, significantly outperforming state-of-the-art results."
"Programming Language Processing (Dagstuhl Seminar 23062).","2023","Dagstuhl Reports","Michael Pradel, Baishakhi Ray, Charles Sutton, Eran Yahav","Informal and Other Publications","https://dblp.org/rec/journals/dagstuhl-reports/PradelRSY23",""
"TraceFixer: Execution Trace-Driven Program Repair.","2023","CoRR","Islem Bouzenia, Yangruibo Ding, Kexin Pei, Baishakhi Ray, Michael Pradel","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2304-12743","When debugging unintended program behavior, developers can often identify the point in the execution where the actual behavior diverges from the desired behavior. For example, a variable may get assigned a wrong value, which then negatively influences the remaining computation. Once a developer identifies such a divergence, how to fix the code so that it provides the desired behavior? This paper presents TraceFixer, a technique for predicting how to edit source code so that it does not diverge from the expected behavior anymore. The key idea is to train a neural program repair model that not only learns from source code edits but also exploits excerpts of runtime traces. The input to the model is a partial execution trace of the incorrect code, which can be obtained automatically through code instrumentation, and the correct state that the program should reach at the divergence point, which the user provides, e.g., in an interactive debugger. Our approach fundamentally differs from current program repair techniques, which share a similar goal but exploit neither execution traces nor information about the desired program state. We evaluate TraceFixer on single-line mistakes in Python code. After training the model on hundreds of thousands of code edits created by a neural model that mimics real-world bugs, we find that exploiting execution traces improves the bug-fixing ability by 13% to 20% (depending on the dataset, within the top-10 predictions) compared to a baseline that learns from source code edits only. Applying TraceFixer to 20 real-world Python bugs shows that the approach successfully fixes 10 of them."
"Universal Fuzzing via Large Language Models.","2023","CoRR","Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, Lingming Zhang 0001","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2308-04748",""
"Where to Look When Repairing Code? Comparing the Attention of Neural Models and Developers.","2023","CoRR","Dominik Huber, Matteo Paltenghi, Michael Pradel","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2305-07287","Neural network-based techniques for automated program repair are becoming increasingly effective. Despite their success, little is known about why they succeed or fail, and how their way of reasoning about the code to repair compares to human developers. This paper presents the first in-depth study comparing human and neural program repair. In particular, we investigate what parts of the buggy code humans and two state of the art neural repair models focus on. This comparison is enabled by a novel attention-tracking interface for human code editing, based on which we gather a dataset of 98 bug fixing sessions, and on the attention layers of neural repair models. Our results show that the attention of the humans and both neural models often overlaps (0.35 to 0.44 correlation). At the same time, the agreement between humans and models still leaves room for improvement, as evidenced by the higher human-human correlation of 0.56. While the two models either focus mostly on the buggy line or on the surrounding context, the developers adopt a hybrid approach that evolves over time, where 36.8% of the attention is given to the buggy line and the rest to the context. Overall, we find the humans to still be clearly more effective at finding a correct fix, with 67.3% vs. less than 3% correctly predicted patches. The results and data of this study are a first step into a deeper understanding of the internal process of neural program repair, and offer insights inspired by the behavior of human developers on how to further improve neural repair models."
