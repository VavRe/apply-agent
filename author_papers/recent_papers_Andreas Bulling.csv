"title","year","venue","authors","type","url","abstract"
"ActionDiffusion: An Action-aware Diffusion Model for Procedure Planning in Instructional Videos.","2024","CoRR","Lei Shi, Paul C. Bürkner, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2403-08591","We present ActionDiffusion -- a novel diffusion model for procedure planning in instructional videos that is the first to take temporal inter-dependencies between actions into account in a diffusion model for procedure planning. This approach is in stark contrast to existing methods that fail to exploit the rich information content available in the particular order in which actions are performed. Our method unifies the learning of temporal dependencies between actions and denoising of the action plan in the diffusion process by projecting the action information into the noise space. This is achieved 1) by adding action embeddings in the noise masks in the noise-adding phase and 2) by introducing an attention mechanism in the noise prediction network to learn the correlations between different action steps. We report extensive experiments on three instructional video benchmark datasets (CrossTask, Coin, and NIV) and show that our method outperforms previous state-of-the-art methods on all metrics on CrossTask and NIV and all metrics except accuracy on Coin dataset. We show that by adding action embeddings into the noise mask the diffusion model can better learn action temporal dependencies and increase the performances on procedure planning."
"DisMouse: Disentangling Information from Mouse Movement Data.","2024","UIST","Guanhua Zhang, Zhiming Hu, Andreas Bulling","Conference and Workshop Papers","https://dblp.org/rec/conf/uist/ZhangHB24",""
"Explaining Disagreement in Visual Question Answering Using Eye Tracking.","2024","ETRA","Susanne Hindennach, Lei Shi, Andreas Bulling","Conference and Workshop Papers","https://dblp.org/rec/conf/etra/HindennachSB24","When presented with the same question about an image, human annotators often give valid but disagreeing answers indicating that their reasoning was different. Such differences are lost in a single ground truth label used to train and evaluate visual question answering (VQA) methods. In this work, we explore whether visual attention maps, created using stationary eye tracking, provide insight into the reasoning underlying disagreement in VQA. We first manually inspect attention maps in the recent VQA-MHUG dataset and find cases in which attention differs consistently for disagreeing answers. We further evaluate the suitability of four different similarity metrics to detect attention differences matching the disagreement. We show that attention maps plausibly surface differences in reasoning underlying one type of disagreement, and that the metrics complementarily detect them. Taken together, our results represent an important first step to leverage eye-tracking to explain disagreement in VQA."
"Explicit Modelling of Theory of Mind for Belief Prediction in Nonverbal Social Interactions.","2024","ECAI","Matteo Bortoletto, Constantin Ruhdorfer, Lei Shi, Andreas Bulling","Conference and Workshop Papers","https://dblp.org/rec/conf/ecai/BortolettoRSB24","We propose MToMnet - a Theory of Mind (ToM) neural network for predicting beliefs and their dynamics during human social interactions from multimodal input. ToM is key for effective nonverbal human communication and collaboration, yet, existing methods for belief modelling have not included explicit ToM modelling or have typically been limited to one or two modalities. MToMnet encodes contextual cues (scene videos and object locations) and integrates them with person-specific cues (human gaze and body language) in a separate MindNet for each person. Inspired by prior research on social cognition and computational ToM, we propose three different MToMnet variants: two involving fusion of latent representations and one involving re-ranking of classification scores. We evaluate our approach on two challenging real-world datasets, one focusing on belief prediction, while the other examining belief dynamics prediction. Our results demonstrate that MToMnet surpasses existing methods by a large margin while at the same time requiring a significantly smaller number of parameters. Taken together, our method opens up a highly promising direction for future work on artificial intelligent systems that can robustly predict human beliefs from their non-verbal behaviour and, as such, more effectively collaborate with humans."
"GazeMotion: Gaze-guided Human Motion Forecasting.","2024","CoRR","Zhiming Hu, Syn Schmitt, Daniel F. B. Haeufle, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2403-09885","We present GazeMotion, a novel method for human motion forecasting that combines information on past human poses with human eye gaze. Inspired by evidence from behavioural sciences showing that human eye and body movements are closely coordinated, GazeMotion first predicts future eye gaze from past gaze, then fuses predicted future gaze and past poses into a gaze-pose graph, and finally uses a residual graph convolutional network to forecast body motion. We extensively evaluate our method on the MoGaze, ADT, and GIMO benchmark datasets and show that it outperforms state-of-the-art methods by up to 7.4% improvement in mean per joint position error. Using head direction as a proxy to gaze, our method still achieves an average improvement of 5.5%. We finally report an online user study showing that our method also outperforms prior methods in terms of perceived realism. These results show the significant information content available in eye gaze for human motion forecasting as well as the effectiveness of our method in exploiting this information."
"Generating Realistic Arm Movements in Reinforcement Learning: A Quantitative Comparison of Reward Terms and Task Requirements.","2024","CoRR","Jhon Charaja, Isabell Wochner, Pierre Schumacher, Winfried Ilg, Martin A. Giese, Christophe Maufroy, Andreas Bulling, Syn Schmitt, Daniel F. B. Haeufle","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2402-13949","Mimicking of human-like arm movement characteristics involves considering three factors during control policy synthesis: (a) task requirements, (b) noise during movement execution, and (c) optimality principles. Previous studies showed that when these factors (a-c) are considered individually, it is possible to synthesize arm movements that either kinematically match experimental data or reproduce the stereotypical triphasic muscle activation pattern. However, no quantitative comparison has assessed the realism of arm movements generated by each factor, nor has it been determined whether combining these factors results in movements with human-like kinematic characteristics and the triphasic muscle pattern. To investigate this, we used reinforcement learning to learn a control policy for a musculoskeletal arm model, aiming to discern which combination of factors (a-c) results in realistic arm movements according to four frequently reported stereotypical characteristics. Our findings indicate that incorporating velocity and acceleration requirements into the reaching task, employing reward terms that minimize mechanical work, hand jerk, and control effort, along with the inclusion of noise during movement, leads to realistic human arm movements by reinforcement learning. We expect that the gained insights will help in the future to better predict desired arm movements and corrective forces in wearable assistive devices."
"InteRead: An Eye Tracking Dataset of Interrupted Reading.","2024","LREC/COLING","Francesca Zermiani, Prajit Dhar, Ekta Sood, Fabian Kögel, Andreas Bulling, Maria Wirzberger","Conference and Workshop Papers","https://dblp.org/rec/conf/coling/ZermianiDSKBW24","Eye movements during reading offer a window into cognitive processes and language comprehension, but the scarcity of reading data with interruptions – which learners frequently encounter in their everyday learning environments – hampers advances in the development of intelligent learning technologies. We introduce InteRead – a novel 50-participant dataset of gaze data recorded during self-paced reading of real-world text. InteRead further offers fine-grained annotations of interruptions interspersed throughout the text as well as resumption lags incurred by these interruptions. Interruptions were triggered automatically once readers reached predefined target words. We validate our dataset by reporting interdisciplinary analyses on different measures of gaze behavior. In line with prior research, our analyses show that the interruptions as well as word length and word frequency effects significantly impact eye movements during reading. We also explore individual differences within our dataset, shedding light on the potential for tailored educational solutions. InteRead is accessible from our datasets web-page: https://www.ife.uni-stuttgart.de/en/llis/research/datasets/."
"Learning User Embeddings from Human Gaze for Personalised Saliency Prediction.","2024","Proc. ACM Hum. Comput. Interact.","Florian Strohm, Mihai Bâce, Andreas Bulling","Journal Articles","https://dblp.org/rec/journals/pacmhci/StrohmBB24","Reusable embeddings of user behaviour have shown significant performance improvements for the personalised saliency prediction task. However, prior works require explicit user characteristics and preferences as input, which are often difficult to obtain. We present a novel method to extract user embeddings from pairs of natural images and corresponding saliency maps generated from a small amount of user-specific eye tracking data. At the core of our method is a Siamese convolutional neural encoder that learns the user embeddings by contrasting the image and personal saliency map pairs of different users. Evaluations on two public saliency datasets show that the generated embeddings have high discriminative power, are effective at refining universal saliency maps to the individual users, and generalise well across users and images. Finally, based on our model's ability to encode individual user characteristics, our work points towards other applications that can benefit from reusable embeddings of gaze behaviour."
"Limits of Theory of Mind Modelling in Dialogue-Based Collaborative Plan Acquisition.","2024","ACL","Matteo Bortoletto, Constantin Ruhdorfer, Adnen Abdessaied, Lei Shi, Andreas Bulling","Conference and Workshop Papers","https://dblp.org/rec/conf/acl/BortolettoRASB24","Recent work on dialogue-based collaborative plan acquisition (CPA) has suggested that Theory of Mind (ToM) modelling can improve missing knowledge prediction in settings with asymmetric skill-sets and knowledge. Although ToM was claimed to be important for effective collaboration, its real impact on this novel task remains under-explored. By representing plans as graphs and by exploiting task-specific constraints we show that, as performance on CPA nearly doubles when predicting one's own missing knowledge, the improvements due to ToM modelling diminish. This phenomenon persists even when evaluating existing baseline methods. To better understand the relevance of ToM for CPA, we report a principled performance comparison of models with and without ToM features. Results across different models and ablations consistently suggest that learned ToM features are indeed more likely to reflect latent patterns in the data with no perceivable link to ToM. This finding calls for a deeper understanding of the role of ToM in CPA and beyond, as well as new methods for modelling and evaluating mental states in computational collaborative agents."
"Mindful Explanations: Prevalence and Impact of Mind Attribution in XAI Research.","2024","Proc. ACM Hum. Comput. Interact.","Susanne Hindennach, Lei Shi, Filip Miletic 0002, Andreas Bulling","Journal Articles","https://dblp.org/rec/journals/pacmhci/HindennachSMB24","When users perceive AI systems as mindful, independent agents, they hold them responsible instead of the AI experts who created and designed these systems. So far, it has not been studied whether explanations support this shift in responsibility through the use of mind-attributing verbs like ""to think"". To better understand the prevalence of mind-attributing explanations we analyse AI explanations in 3,533 explainable AI (XAI) research articles from the Semantic Scholar Open Research Corpus (S2ORC). Using methods from semantic shift detection, we identify three dominant types of mind attribution: (1) metaphorical (e.g. ""to learn"" or ""to predict""), (2) awareness (e.g. ""to consider""), and (3) agency (e.g. ""to make decisions""). We then analyse the impact of mind-attributing explanations on awareness and responsibility in a vignette-based experiment with 199 participants. We find that participants who were given a mind-attributing explanation were more likely to rate the AI system as aware of the harm it caused. Moreover, the mind-attributing explanation had a responsibility-concealing effect: Considering the AI experts' involvement lead to reduced ratings of AI responsibility for participants who were given a non-mind-attributing or no explanation. In contrast, participants who read the mind-attributing explanation still held the AI system responsible despite considering the AI experts' involvement. Taken together, our work underlines the need to carefully phrase explanations about AI systems in scientific writing to reduce mind attribution and clearly communicate human responsibility."
"Mouse2Vec: Learning Reusable Semantic Representations of Mouse Behaviour.","2024","CHI","Guanhua Zhang, Zhiming Hu, Mihai Bâce, Andreas Bulling","Conference and Workshop Papers","https://dblp.org/rec/conf/chi/ZhangHBB24","The mouse is a pervasive input device used for a wide range of interactive applications. However, computational modelling of mouse behaviour typically requires time-consuming design and extraction of handcrafted features, or approaches that are application-specific. We instead propose Mouse2Vec – a novel self-supervised method designed to learn semantic representations of mouse behaviour that are reusable across users and applications. Mouse2Vec uses a Transformer-based encoder-decoder architecture, which is specifically geared for mouse data: During pretraining, the encoder learns an embedding of input mouse trajectories while the decoder reconstructs the input and simultaneously detects mouse click events. We show that the representations learned by our method can identify interpretable mouse behaviour clusters and retrieve similar mouse trajectories. We also demonstrate on three sample downstream tasks that the representations can be practically used to augment mouse data for training supervised methods and serve as an effective feature extractor."
"Multi-modal Video Dialog State Tracking in the Wild.","2024","ECCV","Adnen Abdessaied, Lei Shi, Andreas Bulling","Conference and Workshop Papers","https://dblp.org/rec/conf/eccv/AbdessaiedSB24","We present MST-MIXER - a novel video dialog model operating over a generic multi-modal state tracking scheme. Current models that claim to perform multi-modal state tracking fall short of two major aspects: (1) They either track only one modality (mostly the visual input) or (2) they target synthetic datasets that do not reflect the complexity of real-world in the wild scenarios. Our model addresses these two limitations in an attempt to close this crucial research gap. Specifically, MST-MIXER first tracks the most important constituents of each input modality. Then, it predicts the missing underlying structure of the selected constituents of each modality by learning local latent graphs using a novel multi-modal graph structure learning method. Subsequently, the learned local graphs and features are parsed together to form a global graph operating on the mix of all modalities which further refines its structure and node embeddings. Finally, the fine-grained graph node features are used to enhance the hidden states of the backbone Vision-Language Model (VLM). MST-MIXER achieves new state-of-the-art results on five challenging benchmarks."
"Neural Reasoning about Agents' Goals, Preferences, and Actions.","2024","AAAI","Matteo Bortoletto, Lei Shi, Andreas Bulling","Conference and Workshop Papers","https://dblp.org/rec/conf/aaai/BortolettoSB24","We propose the Intuitive Reasoning Network (IRENE) - a novel neural model for intuitive psychological reasoning about agents' goals, preferences, and actions that can generalise previous experiences to new situations. IRENE combines a graph neural network for learning agent and world state representations with a transformer to encode the task context. When evaluated on the challenging Baby Intuitions Benchmark, IRENE achieves new state-of-the-art performance on three out of its five tasks - with up to 48.9% improvement. In contrast to existing methods, IRENE is able to bind preferences to specific agents, to better distinguish between rational and irrational agents, and to better understand the role of blocking obstacles. We also investigate, for the first time, the influence of the training tasks on test performance. Our analyses demonstrate the effectiveness of IRENE in combining prior knowledge gained during training for unseen evaluation tasks."
"OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog.","2024","LREC/COLING","Adnen Abdessaied, Manuel von Hochmeister, Andreas Bulling","Conference and Workshop Papers","https://dblp.org/rec/conf/coling/AbdessaiedHB24","We present the Object Language Video Transformer (OLViT) – a novel model for video dialog operating over a multi-modal attention-based dialog state tracker. Existing video dialog models struggle with questions requiring both spatial and temporal localization within videos, long-term temporal reasoning, and accurate object tracking across multiple dialog turns. OLViT addresses these challenges by maintaining a global dialog state based on the output of an Object State Tracker (OST) and a Language State Tracker (LST): while the OST attends to the most important objects within the video, the LST keeps track of the most important linguistic co-references to previous dialog turns. In stark contrast to previous works, our approach is generic by nature and is therefore capable of learning continuous multi-modal dialog state representations of the most relevant objects and rounds. As a result, they can be seamlessly integrated into Large Language Models (LLMs) and offer high flexibility in dealing with different datasets and tasks. Evaluations on the challenging DVD (response classification) and SIMMC 2.1 (response generation) datasets show that OLViT achieves new state-of-the-art performance across both datasets."
"PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure Multi-Party Computation.","2024","Proc. ACM Hum. Comput. Interact.","Mayar Elfares, Pascal Reisert, Zhiming Hu, Wenwu Tang, Ralf Küsters, Andreas Bulling","Journal Articles","https://dblp.org/rec/journals/pacmhci/ElfaresRHTKB24","Latest gaze estimation methods require large-scale training data but their collection and exchange pose significant privacy risks. We propose PrivatEyes - the first privacy-enhancing training approach for appearance-based gaze estimation based on federated learning (FL) and secure multi-party computation (MPC). PrivatEyes enables training gaze estimators on multiple local datasets across different users and server-based secure aggregation of the individual estimators' updates. PrivatEyes guarantees that individual gaze data remains private even if a majority of the aggregating servers is malicious. We also introduce a new data leakage attack DualView that shows that PrivatEyes limits the leakage of private training data more effectively than previous approaches. Evaluations on the MPIIGaze, MPIIFaceGaze, GazeCapture, and NVGaze datasets further show that the improved privacy does not lead to a lower gaze estimation accuracy or substantially higher computational costs - both of which are on par with its non-secure counterparts."
"SalChartQA: Question-driven Saliency on Information Visualisations.","2024","CHI","Yao Wang, Weitian Wang, Abdullah Abdelhafez, Mayar Elfares, Zhiming Hu, Mihai Bâce, Andreas Bulling","Conference and Workshop Papers","https://dblp.org/rec/conf/chi/WangWAEHBB24",""
"Saliency3D: A 3D Saliency Dataset Collected on Screen.","2024","ETRA","Yao Wang, Qi Dai, Mihai Bâce, Karsten Klein 0001, Andreas Bulling","Conference and Workshop Papers","https://dblp.org/rec/conf/etra/WangDB0B24","While visual saliency has recently been studied in 3D, the experimental setup for collecting 3D saliency data can be expensive and cumbersome. To address this challenge, we propose a novel experimental design that utilises an eye tracker on a screen to collect 3D saliency data, which could reduce the cost and complexity of data collection. We first collected gaze data on a computer screen and then mapped the 2D points to 3D saliency data through perspective transformation. Using this method, we propose Saliency3D, a 3D saliency dataset (49,276 fixations) comprising 10 participants looking at sixteen objects. We examined the viewing preferences for objects and our results indicate potential preferred viewing directions and a correlation between salient features and the variation in viewing directions."
"Scanpath Prediction on Information Visualisations.","2024","IEEE Trans. Vis. Comput. Graph.","Yao Wang, Mihai Bâce, Andreas Bulling","Journal Articles","https://dblp.org/rec/journals/tvcg/WangBB24","We propose Unified Model of Saliency and Scanpaths (UMSS)– a model that learns to predict multi-duration saliency and scanpaths (i.e. sequences of eye fixations) on information visualisations. Although scanpaths provide rich information about the importance of different visualisation elements during the visual exploration process, prior work has been limited to predicting aggregated attention statistics, such as visual saliency. We present in-depth analyses of gaze behaviour for different information visualisation elements (e.g. Title, Label, Data) on the popular MASSVIS dataset. We show that while, overall, gaze patterns are surprisingly consistent across visualisations and viewers, there are also structural differences in gaze dynamics for different elements. Informed by our analyses, UMSS first predicts multi-duration element-level saliency maps, then probabilistically samples scanpaths from them. Extensive experiments on MASSVIS show that our method consistently outperforms state-of-the-art methods with respect to several, widely used scanpath and saliency evaluation metrics. Our method achieves a relative improvement in sequence score of 11.5% for scanpath prediction, and a relative improvement in Pearson correlation coefficient of up to 23.6% for saliency prediction. These results are auspicious and point towards richer user models and simulations of visual attention on visualisations without the need for any eye tracking equipment."
"VD-GR: Boosting Visual Dialog with Cascaded Spatial-Temporal Multi-Modal GRaphs.","2024","WACV","Adnen Abdessaied, Lei Shi, Andreas Bulling","Conference and Workshop Papers","https://dblp.org/rec/conf/wacv/AbdessaiedSB24","We propose $\mathbb{V}\mathbb{D}{\text{ - }}\mathbb{G}\mathbb{R}$ – a novel visual dialog model that combines pre-trained language models (LMs) with graph neural networks (GNNs). Prior works mainly focused on one class of models at the expense of the other, thus missing out on the opportunity of combining their respective benefits. At the core of $\mathbb{V}\mathbb{D}{\text{ - }}\mathbb{G}\mathbb{R}$ is a novel integration mechanism that alternates between spatial-temporal multi-modal GNNs and BERT layers, and that covers three distinct contributions: First, we use multi-modal GNNs to process the features of each modality (image, question, and dialog history) and exploit their local structures before performing BERT global attention. Second, we propose hub-nodes that link to all other nodes within one modality graph, allowing the model to propagate information from one GNN (modality) to the other in a cascaded manner. Third, we augment the BERT hidden states with fine-grained multi-modal GNN features before passing them to the next $\mathbb{V}\mathbb{D}{\text{ - }}\mathbb{G}\mathbb{R}$ layer. Evaluations on VisDial v1.0, VisDial v0.9, VisDialConv, and VisPro show that $\mathbb{V}\mathbb{D}{\text{ - }}\mathbb{G}\mathbb{R}$ achieves new state-of-the-art results across all four datasets."
"VisRecall++: Analysing and Predicting Visualisation Recallability from Gaze Behaviour.","2024","Proc. ACM Hum. Comput. Interact.","Yao Wang, Yue Jiang 0002, Zhiming Hu, Constantin Ruhdorfer, Mihai Bâce, Andreas Bulling","Journal Articles","https://dblp.org/rec/journals/pacmhci/WangJHRBB24","Question answering has recently been proposed as a promising means to assess the recallability of information visualisations. However, prior works are yet to study the link between visually encoding a visualisation in memory and recall performance. To fill this gap, we propose VisRecall++ -- a novel 40-participant recallability dataset that contains gaze data on 200 visualisations and 1,000 questions, including identifying the title and retrieving values. We measured recallability by asking participants questions after they observed the visualisation for 10 seconds. Our analyses reveal several insights, such as saccade amplitude, number of fixations, and fixation duration significantly differ between high and low recallability groups. Finally, we propose GazeRecallNet -- a novel computational method to predict recallability from gaze behaviour that outperforms the state-of-the-art model RecallNet and three other baselines on this task. Taken together, our results shed light on assessing recallability from gaze behaviour and inform future work on recallability-based visualisation optimisation."
"Amortized Bayesian Multilevel Models.","2024","CoRR","Daniel Habermann, Marvin Schmitt, Lars Kühmichel, Andreas Bulling, Stefan T. Radev, Paul-Christian Bürkner","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2408-13230",""
"Benchmarking Mental State Representations in Language Models.","2024","CoRR","Matteo Bortoletto, Constantin Ruhdorfer, Lei Shi, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2406-17513","While numerous works have assessed the generative performance of language models (LMs) on tasks requiring Theory of Mind reasoning, research into the models' internal representation of mental states remains limited. Recent work has used probing to demonstrate that LMs can represent beliefs of themselves and others. However, these claims are accompanied by limited evaluation, making it difficult to assess how mental state representations are affected by model design and training choices. We report an extensive benchmark with various LM types with different model sizes, fine-tuning approaches, and prompt designs to study the robustness of mental state representations and memorisation issues within the probes. Our results show that the quality of models' internal representations of the beliefs of others increases with model size and, more crucially, with fine-tuning. We are the first to study how prompt variations impact probing performance on theory of mind tasks. We demonstrate that models' representations are sensitive to prompt variations, even when such variations should be beneficial. Finally, we complement previous activation editing experiments on Theory of Mind tasks and show that it is possible to improve models' reasoning performance by steering their activations without the need to train any probe."
"DiffEyeSyn: Diffusion-based User-specific Eye Movement Synthesis.","2024","CoRR","Chuhan Jiao, Guanhua Zhang, Zhiming Hu, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2409-01240","High-frequency components in eye gaze data contain user-specific information promising for various applications, but existing gaze modelling methods focus on low frequencies of typically not more than 30 Hz. We present DiffEyeSyn -- the first computational method to synthesise high-frequency gaze data, including eye movement characteristics specific to individual users. The key idea is to consider the high-frequency, user-specific information as a special type of noise in eye movement data. This perspective reshapes eye movement synthesis into the task of injecting this user-specific noise into any given eye movement sequence. We formulate this injection task as a conditional diffusion process in which the synthesis is conditioned on user-specific embeddings extracted from the gaze data using pre-trained models for user authentication. We propose user identity guidance -- a novel loss function that allows our model to preserve user identity while generating human-like eye movements in the spatial domain. Experiment results on two public high-frequency eye movement biometric datasets show that our synthetic eye movements are indistinguishable from real human eye movements. Furthermore, we demonstrate that DiffEyeSyn can be used to synthesise eye gaze data at scale and for different downstream tasks, such as gaze data imputation and gaze data super-resolution. As such, our work lays the methodological foundations for personalised eye movement synthesis that has significant application potential, such as for character animation, eye movement biometrics, or gaze-based activity and context recognition."
"DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on 360° Images.","2024","CoRR","Chuhan Jiao, Yao Wang, Guanhua Zhang, Mihai Bâce, Zhiming Hu, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2403-17477","In the rapidly evolving domain of medical imaging, there's an increasing interest in harnessing deep learning models for enhanced diagnosis and prognosis. Among these, the Variational Autoencoder (VAE) and the Diffusion model stand out for their potential in generating synthetic lung cancer images. This research article delves into a comparative analysis of both models, focusing on their application in lung cancer imaging. Drawing from the ""Iraq-Oncology Teaching Hospital/National Center for Cancer Diseases (IQ-OTH/NCCD) lung cancer dataset,"" the study investigates the efficiency, accuracy, and fidelity of the images generated by each model. The findings suggest that while the VAE model offers faster image generation, its output is notably blurrier than its counterpart. Conversely, the Diffusion model, despite its relatively slower speed, is capable of producing highly detailed synthetic images even with limited epochs. This comprehensive comparison not only highlights the strengths and shortcomings of each model but also lays the groundwork for further refinements and potential clinical implementations. The broader objective is to catalyze advancements in lung cancer diagnosis, ultimately leading to better patient outcomes."
"HOIMotion: Forecasting Human Motion During Human-Object Interactions Using Egocentric 3D Object Bounding Boxes.","2024","CoRR","Zhiming Hu, Zheming Yin, Daniel F. B. Haeufle, Syn Schmitt, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2407-02633",""
"MultiMediate'24: Multi-Domain Engagement Estimation.","2024","CoRR","Philipp Müller 0001, Michal Balazia, Tobias Baur 0001, Michael Dietz, Alexander Heimerl, Anna Penzkofer, Dominik Schiller, François Brémond, Jan Alexandersson, Elisabeth André, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2408-16625","Estimating the momentary level of participant's engagement is an important prerequisite for assistive systems that support human interactions. Previous work has addressed this task in within-domain evaluation scenarios, i.e. training and testing on the same dataset. This is in contrast to real-life scenarios where domain shifts between training and testing data frequently occur. With MultiMediate'24, we present the first challenge addressing multi-domain engagement estimation. As training data, we utilise the NOXI database of dyadic novice-expert interactions. In addition to within-domain test data, we add two new test domains. First, we introduce recordings following the NOXI protocol but covering languages that are not present in the NOXI training data. Second, we collected novel engagement annotations on the MPIIGroupInteraction dataset which consists of group discussions between three to four people. In this way, MultiMediate'24 evaluates the ability of approaches to generalise across factors such as language and cultural background, group size, task, and screen-mediated vs. face-to-face interaction. This paper describes the MultiMediate'24 challenge and presents baseline results. In addition, we discuss selected challenge solutions."
"SeFFeC: Semantic Facial Feature Control for Fine-grained Face Editing.","2024","CoRR","Florian Strohm, Mihai Bâce, Markus Kaltenecker, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2403-13972","We present User-predictable Face Editing (UP-FacE) -- a novel method for predictable face shape editing. In stark contrast to existing methods for face editing using trial and error, edits with UP-FacE are predictable by the human user. That is, users can control the desired degree of change precisely and deterministically and know upfront the amount of change required to achieve a certain editing result. Our method leverages facial landmarks to precisely measure facial feature values, facilitating the training of UP-FacE without manually annotated attribute labels. At the core of UP-FacE is a transformer-based network that takes as input a latent vector from a pre-trained generative model and a facial feature embedding, and predicts a suitable manipulation vector. To enable user-predictable editing, a scaling layer adjusts the manipulation vector to achieve the precise desired degree of change. To ensure that the desired feature is manipulated towards the target value without altering uncorrelated features, we further introduce a novel semantic face feature loss. Qualitative and quantitative results demonstrate that UP-FacE enables precise and fine-grained control over 23 face shape features."
"The Overcooked Generalisation Challenge.","2024","CoRR","Constantin Ruhdorfer, Matteo Bortoletto, Anna Penzkofer, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2406-17949","We introduce the Overcooked Generalisation Challenge (OGC) - the first benchmark to study agents' zero-shot cooperation abilities when faced with novel partners and levels in the Overcooked-AI environment. This perspective starkly contrasts a large body of previous work that has trained and evaluated cooperating agents only on the same level, failing to capture generalisation abilities required for real-world human-AI cooperation. Our challenge interfaces with state-of-the-art dual curriculum design (DCD) methods to generate auto-curricula for training general agents in Overcooked. It is the first cooperative multi-agent environment specially designed for DCD methods and, consequently, the first benchmarked with state-of-the-art methods. It is fully GPU-accelerated, built on the DCD benchmark suite minimax, and freely available under an open-source license: https://git.hcics.simtech.uni-stuttgart.de/public-projects/OGC. We show that current DCD algorithms struggle to produce useful policies in this novel challenge, even if combined with recent network architectures that were designed for scalability and generalisability. The OGC pushes the boundaries of real-world human-AI cooperation by enabling the research community to study the impact of generalisation on cooperating agents."
"VSA4VQA: Scaling a Vector Symbolic Architecture to Visual Question Answering on Natural Images.","2024","CoRR","Anna Penzkofer, Lei Shi, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2405-03852","While Vector Symbolic Architectures (VSAs) are promising for modelling spatial cognition, their application is currently limited to artificially generated images and simple spatial queries. We propose VSA4VQA - a novel 4D implementation of VSAs that implements a mental representation of natural images for the challenging task of Visual Question Answering (VQA). VSA4VQA is the first model to scale a VSA to complex spatial queries. Our method is based on the Semantic Pointer Architecture (SPA) to encode objects in a hyperdimensional vector space. To encode natural images, we extend the SPA to include dimensions for object's width and height in addition to their spatial location. To perform spatial queries we further introduce learned spatial query masks and integrate a pre-trained vision-language model for answering attribute-related questions. We evaluate our method on the GQA benchmark dataset and show that it can effectively encode natural images, achieving competitive performance to state-of-the-art deep learning methods for zero-shot VQA."
"Bullingers Briefwechsel zugänglich machen: Stand der Handschriftenerkennung.","2023","DHd","Phillip Ströbel, Tobias Hodel, Andreas Fischer 0012, Anna Scius, Beat Wolf, Anna Janka, Jonas Widmer, Patricia Scheurer, Martin Volk 0001","Conference and Workshop Papers","https://dblp.org/rec/conf/dhd/StrobelHFSWJWSV23",""
"EHTask: Recognizing User Tasks From Eye and Head Movements in Immersive Virtual Reality.","2023","IEEE Trans. Vis. Comput. Graph.","Zhiming Hu, Andreas Bulling, Sheng Li 0008, Guoping Wang","Journal Articles","https://dblp.org/rec/journals/tvcg/HuBLW23","Understanding human visual attention in immersive virtual reality (VR) is crucial for many important applications, including gaze prediction, gaze guidance, and gaze-contingent rendering. However, previous works on visual attention analysis typically only explored one specific VR task and paid less attention to the differences between different tasks. Moreover, existing task recognition methods typically focused on 2D viewing conditions and only explored the effectiveness of human eye movements. We first collect eye and head movements of 30 participants performing four tasks, i.e., <italic>Free viewing</italic>, <italic>Visual search</italic>, <italic>Saliency</italic>, and <italic>Track</italic>, in 15 360-degree VR videos. Using this dataset, we analyze the patterns of human eye and head movements and reveal significant differences across different tasks in terms of fixation duration, saccade amplitude, head rotation velocity, and eye-head coordination. We then propose <italic>EHTask</italic> – a novel learning-based method that employs eye and head movements to recognize user tasks in VR. We show that our method significantly outperforms the state-of-the-art methods derived from 2D viewing conditions both on our dataset (accuracy of <inline-formula><tex-math notation=""LaTeX"">$84.4\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>84</mml:mn><mml:mo>.</mml:mo><mml:mn>4</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""li-ieq1-3138902.gif""/></alternatives></inline-formula> <italic>versus</italic> <inline-formula><tex-math notation=""LaTeX"">$62.8\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>62</mml:mn><mml:mo>.</mml:mo><mml:mn>8</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""li-ieq2-3138902.gif""/></alternatives></inline-formula>) and on a real-world dataset (<inline-formula><tex-math notation=""LaTeX"">$61.9\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>61</mml:mn><mml:mo>.</mml:mo><mml:mn>9</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""li-ieq3-3138902.gif""/></alternatives></inline-formula> <italic>versus</italic> <inline-formula><tex-math notation=""LaTeX"">$44.1\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>44</mml:mn><mml:mo>.</mml:mo><mml:mn>1</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""li-ieq4-3138902.gif""/></alternatives></inline-formula>). As such, our work provides meaningful insights into human visual attention under different VR tasks and guides future work on recognizing user tasks in VR."
"Exploring Natural Language Processing Methods for Interactive Behaviour Modelling.","2023","INTERACT","Guanhua Zhang, Matteo Bortoletto, Zhiming Hu, Lei Shi, Mihai Bâce, Andreas Bulling","Conference and Workshop Papers","https://dblp.org/rec/conf/interact/ZhangBHSBB23","Analysing and modelling interactive behaviour is an important topic in human-computer interaction (HCI) and a key requirement for the development of intelligent interactive systems. Interactive behaviour has a sequential (actions happen one after another) and hierarchical (a sequence of actions forms an activity driven by interaction goals) structure, which may be similar to the structure of natural language. Designed based on such a structure, natural language processing (NLP) methods have achieved groundbreaking success in various downstream tasks. However, few works linked interactive behaviour with natural language. In this paper, we explore the similarity between interactive behaviour and natural language by applying an NLP method, byte pair encoding (BPE), to encode mouse and keyboard behaviour. We then analyse the vocabulary, i.e., the set of action sequences, learnt by BPE, as well as use the vocabulary to encode the input behaviour for interactive task recognition. An existing dataset collected in constrained lab settings and our novel out-of-the-lab dataset were used for evaluation. Results show that this natural language-inspired approach not only learns action sequences that reflect specific interaction goals, but also achieves higher F1 scores on task recognition than other methods. Our work reveals the similarity between interactive behaviour and natural language, and presents the potential of applying the new pack of methods that leverage insights from NLP to model interactive behaviour in HCI."
"GazeCast: Using Mobile Devices to Allow Gaze-based Interaction on Public Displays.","2023","ETRA","Omar Namnakani, Penpicha Sinrattanavong, Yasmeen Abdrabou, Andreas Bulling, Florian Alt, Mohamed Khamis","Conference and Workshop Papers","https://dblp.org/rec/conf/etra/NamnakaniSABAK23","Gaze is promising for natural and spontaneous interaction with public displays, but current gaze-enabled displays require movement-hindering stationary eye trackers or cumbersome head-mounted eye trackers. We propose and evaluate GazeCast – a novel system that leverages users’ handheld mobile devices to allow gaze-based interaction with surrounding displays. In a user study (N = 20), we compared GazeCast to a standard webcam for gaze-based interaction using Pursuits. We found that while selection using GazeCast requires more time and physical demand, participants value GazeCast’s high accuracy and flexible positioning. We conclude by discussing how mobile computing can facilitate the adoption of gaze interaction with pervasive displays."
"Impact of Privacy Protection Methods of Lifelogs on Remembered Memories.","2023","CHI","Passant Elagroudy, Mohamed Khamis, Florian Mathis, Diana Irmscher, Ekta Sood, Andreas Bulling, Albrecht Schmidt 0001","Conference and Workshop Papers","https://dblp.org/rec/conf/chi/ElagroudyKMISB023","Lifelogging is traditionally used for memory augmentation. However, recent research shows that users’ trust in the completeness and accuracy of lifelogs might skew their memories. Privacy-protection alterations such as body blurring and content deletion are commonly applied to photos to circumvent capturing sensitive information. However, their impact on how users remember memories remain unclear. To this end, we conduct a white-hat memory attack and report on an iterative experiment (N=21) to compare the impact of viewing 1) unaltered lifelogs, 2) blurred lifelogs, and 3) a subset of the lifelogs after deleting private ones, on confidently remembering memories. Findings indicate that all the privacy methods impact memories’ quality similarly and that users tend to change their answers in recognition more than recall scenarios. Results also show that users have high confidence in their remembered content across all privacy methods. Our work raises awareness about the mindful designing of technological interventions."
"Improving neural saliency prediction with a cognitive model of human visual attention.","2023","CogSci","Ekta Sood, Lei Shi, Matteo Bortoletto, Yao Wang, Philipp Müller 0001, Andreas Bulling","Conference and Workshop Papers","https://dblp.org/rec/conf/cogsci/SoodSBW0B23",""
"MultiMediate '23: Engagement Estimation and Bodily Behaviour Recognition in Social Interactions.","2023","ACM Multimedia","Philipp Müller 0001, Michal Balazia, Tobias Baur 0001, Michael Dietz, Alexander Heimerl, Dominik Schiller, Mohammed Guermal, Dominike Thomas, François Brémond, Jan Alexandersson, Elisabeth André, Andreas Bulling","Conference and Workshop Papers","https://dblp.org/rec/conf/mm/0001B0DHSGTBAAB23",""
"Multimodal Integration of Human-Like Attention in Visual Question Answering.","2023","CVPR Workshops","Ekta Sood, Fabian Kögel, Philipp Müller 0001, Dominike Thomas, Mihai Bâce, Andreas Bulling","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/SoodKMTBB23","Human-like attention as a supervisory signal to guide neural attention has shown significant promise but is currently limited to unimodal integration – even for inherently multimodal tasks such as visual question answering (VQA). We present the Multimodal Human-like Attention Network (MULAN) – the first method for multimodal integration of human-like attention on image and text during training of VQA models. MULAN integrates attention predictions from two state-of-the-art text and image saliency models into neural self-attention layers of a recent transformer-based VQA model. Through evaluations on the challenging VQAv2 dataset, we show that MULAN is competitive to state of the art in its model class – achieving 73.98% accuracy on test-std and 73.72% on test-dev with approximately 80% fewer trainable parameters than prior work. Overall, our work underlines the potential of integrating multimodal human-like attention into neural attention mechanisms for VQA."
"Privacy-Aware Eye Tracking: Challenges and Future Directions.","2023","IEEE Pervasive Comput.","Céline Gressel, Rebekah Overdorf, Inken Hagestedt, Murat Karaboga, Helmut Lurtz, Michael Raschke, Andreas Bulling","Journal Articles","https://dblp.org/rec/journals/pervasive/GresselOHKLRB23","What do you have to keep in mind when developing or using eye-tracking technologies regarding privacy? In this article we discuss the main ethical, technical, and legal categories of privacy, which is much more than just data protection. We additionally provide recommendations about how such technologies might mitigate privacy risks and in which cases the risks are higher than the benefits of the technology."
"SUPREYES: SUPer Resolutin for EYES Using Implicit Neural Representation Learning.","2023","UIST","Chuhan Jiao, Zhiming Hu, Mihai Bâce, Andreas Bulling","Conference and Workshop Papers","https://dblp.org/rec/conf/uist/JiaoHBB23","We introduce SUPREYES – a novel self-supervised method to increase the spatio-temporal resolution of gaze data recorded using low(er)-resolution eye trackers. Despite continuing advances in eye tracking technology, the vast majority of current eye trackers – particularly mobile ones and those integrated into mobile devices – suffer from low-resolution gaze data, thus fundamentally limiting their practical usefulness. SUPREYES learns a continuous implicit neural representation from low-resolution gaze data to up-sample the gaze data to arbitrary resolutions. We compare our method with commonly used interpolation methods on arbitrary scale super-resolution and demonstrate that SUPREYES outperforms these baselines by a significant margin. We also test on the sample downstream task of gaze-based user identification and show that our method improves the performance of original low-resolution gaze data and outperforms other baselines. These results are promising as they open up a new direction for increasing eye tracking fidelity as well as enabling new gaze-based applications without the need for new eye tracking equipment."
"The Bullinger Dataset: A Writer Adaptation Challenge.","2023","ICDAR","Anna Scius-Bertrand, Phillip Ströbel, Martin Volk 0001, Tobias Hodel, Andreas Fischer 0012","Conference and Workshop Papers","https://dblp.org/rec/conf/icdar/SciusBertrandSVHF23",""
"GazeMoDiff: Gaze-guided Diffusion Model for Stochastic Human Motion Prediction.","2023","CoRR","Haodong Yan, Zhiming Hu, Syn Schmitt, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2312-12090","Human motion prediction is important for many virtual and augmented reality (VR/AR) applications such as collision avoidance and realistic avatar generation. Existing methods have synthesised body motion only from observed past motion, despite the fact that human eye gaze is known to correlate strongly with body movements and is readily available in recent VR/AR headsets. We present GazeMoDiff - a novel gaze-guided denoising diffusion model to generate stochastic human motions. Our method first uses a gaze encoder and a motion encoder to extract the gaze and motion features respectively, then employs a graph attention network to fuse these features, and finally injects the gaze-motion features into a noise prediction network via a cross-attention mechanism to progressively generate multiple reasonable human motions in the future. Extensive experiments on the MoGaze and GIMO datasets demonstrate that our method outperforms the state-of-the-art methods by a large margin in terms of multi-modal final displacement error (17.3% on MoGaze and 13.3% on GIMO). We further conducted a human study (N=21) and validated that the motions generated by our method were perceived as both more precise and more realistic than those of prior methods. Taken together, these results reveal the significant information content available in eye gaze for stochastic human motion prediction as well as the effectiveness of our method in exploiting this information."
"Inferring Human Intentions from Predicted Action Probabilities.","2023","CoRR","Lei Shi, Paul-Christian Bürkner, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2308-12194","Predicting the next action that a human is most likely to perform is key to human-AI collaboration and has consequently attracted increasing research interests in recent years. An important factor for next action prediction are human intentions: If the AI agent knows the intention it can predict future actions and plan collaboration more effectively. Existing Bayesian methods for this task struggle with complex visual input while deep neural network (DNN) based methods do not provide uncertainty quantifications. In this work we combine both approaches for the first time and show that the predicted next action probabilities contain information that can be used to infer the underlying intention. We propose a two-step approach to human intention prediction: While a DNN predicts the probabilities of the next action, MCMC-based Bayesian inference is used to infer the underlying intention from these predictions. This approach not only allows for independent design of the DNN architecture but also the subsequently fast, design-independent inference of human intentions. We evaluate our method using a series of experiments on the Watch-And-Help (WAH) and a keyboard and mouse interaction dataset. Our results show that our approach can accurately predict human intentions from observed actions and the implicit information contained in next action probabilities. Furthermore, we show that our approach can predict the correct intention even if only few actions have been observed."
"Int-HRL: Towards Intention-based Hierarchical Reinforcement Learning.","2023","CoRR","Anna Penzkofer, Simon Schaefer, Florian Strohm, Mihai Bâce, Stefan Leutenegger, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2306-11483","While deep reinforcement learning (RL) agents outperform humans on an increasing number of tasks, training them requires data equivalent to decades of human gameplay. Recent hierarchical RL methods have increased sample efficiency by incorporating information inherent to the structure of the decision problem but at the cost of having to discover or use human-annotated sub-goals that guide the learning process. We show that intentions of human players, i.e. the precursor of goal-oriented decisions, can be robustly predicted from eye gaze even for the long-horizon sparse rewards task of Montezuma's Revenge - one of the most challenging RL tasks in the Atari2600 game suite. We propose Int-HRL: Hierarchical RL with intention-based sub-goals that are inferred from human eye gaze. Our novel sub-goal extraction pipeline is fully automatic and replaces the need for manual sub-goal annotation by human experts. Our evaluations show that replacing hand-crafted sub-goals with automatically extracted intentions leads to a HRL agent that is significantly more sample efficient than previous methods."
"Labeled pupils in the wild (LPW).","2023","DaRUS","Andreas Bulling","Data and Artifacts","https://dblp.org/rec/data/10/Bulling23a",""
"MPIIEmo.","2023","DaRUS","Andreas Bulling","Data and Artifacts","https://dblp.org/rec/data/10/Bulling23","Automatic analysis of human behaviour is a fundamental prerequisite for the creation of machines that can effectively interact with- and support humans in social interactions. In MultiMediate'23, we address two key human social behaviour analysis tasks for the first time in a controlled challenge: engagement estimation and bodily behaviour recognition in social interactions. This paper describes the MultiMediate'23 challenge and presents novel sets of annotations for both tasks. For engagement estimation we collected novel annotations on the NOvice eXpert Interaction (NOXI) database. For bodily behaviour recognition, we annotated test recordings of the MPIIGroupInteraction corpus with the BBSI annotation scheme. In addition, we present baseline results for both challenge tasks."
"MPIIFaceGaze.","2023","DaRUS","Andreas Bulling","Data and Artifacts","https://dblp.org/rec/data/10/Bulling23b","Latest gaze estimation methods require large-scale training data but their collection and exchange pose significant privacy risks. We propose PrivatEyes - the first privacy-enhancing training approach for appearance-based gaze estimation based on federated learning (FL) and secure multi-party computation (MPC). PrivatEyes enables training gaze estimators on multiple local datasets across different users and server-based secure aggregation of the individual estimators' updates. PrivatEyes guarantees that individual gaze data remains private even if a majority of the aggregating servers is malicious. We also introduce a new data leakage attack DualView that shows that PrivatEyes limits the leakage of private training data more effectively than previous approaches. Evaluations on the MPIIGaze, MPIIFaceGaze, GazeCapture, and NVGaze datasets further show that the improved privacy does not lead to a lower gaze estimation accuracy or substantially higher computational costs - both of which are on par with its non-secure counterparts."
"Mindful Explanations: Prevalence and Impact of Mind Attribution in XAI Research.","2023","CoRR","Susanne Hindennach, Lei Shi, Filip Miletic 0002, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2312-12119","When users perceive AI systems as mindful, independent agents, they hold them responsible instead of the AI experts who created and designed these systems. So far, it has not been studied whether explanations support this shift in responsibility through the use of mind-attributing verbs like ""to think"". To better understand the prevalence of mind-attributing explanations we analyse AI explanations in 3,533 explainable AI (XAI) research articles from the Semantic Scholar Open Research Corpus (S2ORC). Using methods from semantic shift detection, we identify three dominant types of mind attribution: (1) metaphorical (e.g. ""to learn"" or ""to predict""), (2) awareness (e.g. ""to consider""), and (3) agency (e.g. ""to make decisions""). We then analyse the impact of mind-attributing explanations on awareness and responsibility in a vignette-based experiment with 199 participants. We find that participants who were given a mind-attributing explanation were more likely to rate the AI system as aware of the harm it caused. Moreover, the mind-attributing explanation had a responsibility-concealing effect: Considering the AI experts' involvement lead to reduced ratings of AI responsibility for participants who were given a non-mind-attributing or no explanation. In contrast, participants who read the mind-attributing explanation still held the AI system responsible despite considering the AI experts' involvement. Taken together, our work underlines the need to carefully phrase explanations about AI systems in scientific writing to reduce mind attribution and clearly communicate human responsibility."
"MultiMediate'23: Engagement Estimation and Bodily Behaviour Recognition in Social Interactions.","2023","CoRR","Philipp Müller 0001, Michal Balazia, Tobias Baur 0001, Michael Dietz, Alexander Heimerl, Dominik Schiller, Mohammed Guermal, Dominike Thomas, François Brémond, Jan Alexandersson, Elisabeth André, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2308-08256",""
"Neural Reasoning About Agents' Goals, Preferences, and Actions.","2023","CoRR","Matteo Bortoletto, Lei Shi, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2312-07122","We propose the Intuitive Reasoning Network (IRENE) - a novel neural model for intuitive psychological reasoning about agents' goals, preferences, and actions that can generalise previous experiences to new situations. IRENE combines a graph neural network for learning agent and world state representations with a transformer to encode the task context. When evaluated on the challenging Baby Intuitions Benchmark, IRENE achieves new state-of-the-art performance on three out of its five tasks - with up to 48.9% improvement. In contrast to existing methods, IRENE is able to bind preferences to specific agents, to better distinguish between rational and irrational agents, and to better understand the role of blocking obstacles. We also investigate, for the first time, the influence of the training tasks on test performance. Our analyses demonstrate the effectiveness of IRENE in combining prior knowledge gained during training for unseen evaluation tasks."
"Pose2Gaze: Generating Realistic Human Gaze Behaviour from Full-body Poses using an Eye-body Coordination Model.","2023","CoRR","Zhiming Hu, Jiahui Xu, Syn Schmitt, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2312-12042","."
"Usable and Fast Interactive Mental Face Reconstruction.","2023","UIST","Florian Strohm, Mihai Bâce, Andreas Bulling","Conference and Workshop Papers","https://dblp.org/rec/conf/uist/StrohmBB23","We introduce an end-to-end interactive system for mental face reconstruction – the challenging task of visually reconstructing a face image a person only has in their mind. In contrast to existing methods that suffer from low usability and high mental load, our approach only requires the user to rank images over multiple iterations according to the perceived similarity with their mental image. Based on these rankings, our mental face reconstruction system extracts image features in each iteration, combines them into a joint feature vector, and then uses a generative model to visually reconstruct the mental image. To avoid the need for collecting large amounts of human training data, we further propose a computational user model that can simulate human ranking behaviour using data from an online crowd-sourcing study (N=215). Results from a 12-participant user study show that our method can reconstruct mental images that are visually similar to existing approaches but has significantly higher usability, lower perceived workload, and is faster. In addition, results from a third 22-participant lineup study in which we validated our reconstructions on a face ranking task show a identification rate of , which is in line with prior work. These results represent an important step towards new interactive intelligent systems that can robustly and effortlessly reconstruct a user’s mental image."
"VD-GR: Boosting Visual Dialog with Cascaded Spatial-Temporal Multi-Modal GRaphs.","2023","CoRR","Adnen Abdessaied, Lei Shi, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2310-16590","We propose $\mathbb{V}\mathbb{D}{\text{ - }}\mathbb{G}\mathbb{R}$ – a novel visual dialog model that combines pre-trained language models (LMs) with graph neural networks (GNNs). Prior works mainly focused on one class of models at the expense of the other, thus missing out on the opportunity of combining their respective benefits. At the core of $\mathbb{V}\mathbb{D}{\text{ - }}\mathbb{G}\mathbb{R}$ is a novel integration mechanism that alternates between spatial-temporal multi-modal GNNs and BERT layers, and that covers three distinct contributions: First, we use multi-modal GNNs to process the features of each modality (image, question, and dialog history) and exploit their local structures before performing BERT global attention. Second, we propose hub-nodes that link to all other nodes within one modality graph, allowing the model to propagate information from one GNN (modality) to the other in a cascaded manner. Third, we augment the BERT hidden states with fine-grained multi-modal GNN features before passing them to the next $\mathbb{V}\mathbb{D}{\text{ - }}\mathbb{G}\mathbb{R}$ layer. Evaluations on VisDial v1.0, VisDial v0.9, VisDialConv, and VisPro show that $\mathbb{V}\mathbb{D}{\text{ - }}\mathbb{G}\mathbb{R}$ achieves new state-of-the-art results across all four datasets."
