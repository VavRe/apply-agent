"title","year","venue","authors","type","url","abstract"
"A Pontryagin Perspective on Reinforcement Learning.","2024","CoRR","Onno Eberhard, Claire Vernade, Michael Muehlebach","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2405-18100","Reinforcement learning has traditionally focused on learning state-dependent policies to solve optimal control problems in a closed-loop fashion. In this work, we introduce the paradigm of open-loop reinforcement learning where a fixed action sequence is learned instead. We present three new algorithms: one robust model-based method and two sample-efficient model-free methods. Rather than basing our algorithms on Bellman's equation from dynamic programming, our work builds on Pontryagin's principle from the theory of open-loop optimal control. We provide convergence guarantees and evaluate all methods empirically on a pendulum swing-up task, as well as on two high-dimensional MuJoCo tasks, demonstrating remarkable performance compared to existing baselines."
"International Conference on Algorithmic Learning Theory, 25-28 February 2024, La Jolla, California, USA.","2024","ALT","Claire Vernade, Daniel Hsu 0001","Editorship","https://dblp.org/rec/conf/alt/2024",""
"Online Decision Deferral under Budget Constraints.","2024","CoRR","Mirabel Reid, Tom Sühr, Claire Vernade, Samira Samadi","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2409-20489","Machine Learning (ML) models are increasingly used to support or substitute decision making. In applications where skilled experts are a limited resource, it is crucial to reduce their burden and automate decisions when the performance of an ML model is at least of equal quality. However, models are often pre-trained and fixed, while tasks arrive sequentially and their distribution may shift. In that case, the respective performance of the decision makers may change, and the deferral algorithm must remain adaptive. We propose a contextual bandit model of this online decision making problem. Our framework includes budget constraints and different types of partial feedback models. Beyond the theoretical guarantees of our algorithm, we propose efficient extensions that achieve remarkable performance on real-world datasets."
"Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits.","2024","CoRR","Nicolas Nguyen, Imad Aouali, András György 0001, Claire Vernade","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2402-05878","We study the problem of Bayesian fixed-budget best-arm identification (BAI) in structured bandits. We propose an algorithm that uses fixed allocations based on the prior information and the structure of the environment. We provide theoretical bounds on its performance across diverse models, including the first prior-dependent upper bounds for linear and hierarchical BAI. Our key contribution is introducing new proof methods that result in tighter bounds for multi-armed BAI compared to existing methods. We extensively compare our approach to other fixed-budget BAI methods, demonstrating its consistent and robust performance in various settings. Our work improves our understanding of Bayesian fixed-budget BAI in structured bandits and highlights the effectiveness of our approach in practical scenarios."
"Beyond Average Return in Markov Decision Processes.","2023","NeurIPS","Alexandre Marthe, Aurélien Garivier, Claire Vernade","Conference and Workshop Papers","https://dblp.org/rec/conf/nips/MartheGV23",""
"POMRL: No-Regret Learning-to-Plan with Increasing Horizons.","2023","Trans. Mach. Learn. Res.","Khimya Khetarpal, Claire Vernade, Brendan O'Donoghue, Satinder Singh 0001, Tom Zahavy","Journal Articles","https://dblp.org/rec/journals/tmlr/KhetarpalVO0Z23","We study the problem of planning under model uncertainty in an online meta-reinforcement learning (RL) setting where an agent is presented with a sequence of related tasks with limited interactions per task. The agent can use its experience in each task and across tasks to estimate both the transition model and the distribution over tasks. We propose an algorithm to meta-learn the underlying structure across tasks, utilize it to plan in each task, and upper-bound the regret of the planning loss. Our bound suggests that the average regret over tasks decreases as the number of tasks increases and as the tasks are more similar. In the classical single-task setting, it is known that the planning horizon should depend on the estimated model's accuracy, that is, on the number of samples within task. We generalize this finding to meta-RL and study this dependence of planning horizons on the number of tasks. Based on our theoretical findings, we derive heuristics for selecting slowly increasing discount factors, and we validate its significance empirically."
