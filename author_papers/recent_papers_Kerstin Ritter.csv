"title","year","venue","authors","type","url","abstract"
"Benchmarking the influence of pre-training on explanation performance in MR image classification.","2024","Frontiers Artif. Intell.","Marta Oliveira, Rick Wilming, Benedict Clark, Céline Budding, Fabian Eitel, Kerstin Ritter, Stefan Haufe","Journal Articles","https://dblp.org/rec/journals/frai/OliveiraWCBERH24","Convolutional Neural Networks (CNNs) are frequently and successfully used in medical prediction tasks. They are often used in combination with transfer learning, leading to improved performance when training data for the task are scarce. The resulting models are highly complex and typically do not provide any insight into their predictive mechanisms, motivating the field of “explainable” artificial intelligence (XAI). However, previous studies have rarely quantitatively evaluated the “explanation performance” of XAI methods against ground-truth data, and transfer learning and its influence on objective measures of explanation performance has not been investigated. Here, we propose a benchmark dataset that allows for quantifying explanation performance in a realistic magnetic resonance imaging (MRI) classification task. We employ this benchmark to understand the influence of transfer learning on the quality of explanations. Experimental results show that popular XAI methods applied to the same underlying model differ vastly in performance, even when considering only correctly classified examples. We further observe that explanation performance strongly depends on the task used for pre-training and the number of CNN layers pre-trained. These results hold after correcting for a substantial correlation between explanation and classification performance."
"DeepRepViz: Identifying Potential Confounders in Deep Learning Model Predictions.","2024","MICCAI","Roshan Prakash Rane, JiHoon Kim, Arjun Umesha, Didem Stark, Marc-André Schulz, Kerstin Ritter","Conference and Workshop Papers","https://dblp.org/rec/conf/miccai/RaneKUSSR24",""
"EEG-Language Modeling for Pathology Detection.","2024","CoRR","Sam Gijsen, Kerstin Ritter","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2409-07480","AbstractThe human brain is constantly subjected to a multi-modal stream of probabilistic sensory inputs. EEG signatures, such as the mismatch negativity (MMN) and the P3, can give valuable insight into neuronal probabilistic inference. Although reported for different modalities, mismatch responses have largely been studied in isolation, with a strong focus on the auditory MMN. To investigate the extent to which early and late mismatch responses across modalities represent comparable signatures of uni- and cross-modal probabilistic inference in the hierarchically structured cortex, we recorded EEG from 32 participants undergoing a novel tri-modal roving stimulus paradigm. The employed sequences consisted of high and low intensity stimuli in the auditory, somatosensory and visual modalities and were governed by uni-modal transition probabilities and cross-modal conditional dependencies. We found modality specific signatures of MMN (∼100-200ms) in all three modalities, which were source localized to the respective sensory cortices and shared right lateralized pre-frontal sources. Additionally, we identified a cross-modal signature of mismatch processing in the P3a time range (∼300-350ms), for which a common network with frontal dominance was found. Across modalities, the mismatch responses showed highly comparable parametric effects of stimulus train length, which were driven by standard and deviant response modulations in opposite directions. Strikingly, the P3a responses across modalities were increased for mispredicted compared to predicted and unpredictable stimuli, suggesting sensitivity to cross-modal predictive information. Finally, model comparisons indicated that the observed single trial dynamics were best captured by Bayesian learning models tracking uni-modal stimulus transitions as well as cross-modal conditional dependencies."
"TLIMB - A Transfer Learning Framework for IMage Analysis of the Brain.","2024","EDBT/ICDT Workshops","Marc-Andre Schulz, Jan Philipp Albrecht, Alpay Yilmaz, Alexander Koch 0007, Dagmar Kainmüller, Ulf Leser, Kerstin Ritter","Conference and Workshop Papers","https://dblp.org/rec/conf/edbt/SchulzAY0KLR24","AbstractThis study critically reevaluates the utility of brain-age models within the context of detecting neurological and psychiatric disorders, challenging the conventional emphasis on maximizing chronological age prediction accuracy. Our analysis of T1 MRI data from 46,381 UK Biobank participants reveals a paradox: simpler machine learning models, and notably those with excessive regularization, demonstrate superior sensitivity to disease-relevant changes compared to their more complex counterparts. This counterintuitive discovery suggests that models traditionally deemed less accurate in predicting chronological age might, in fact, offer a more meaningful biomarker for brain health by capturing variations pertinent to disease states. Our findings challenge the traditional understanding of brain-age prediction as normative modeling, emphasizing the inadvertent identification of non-normative pathological markers over precise age prediction."
"Benchmark data to study the influence of pre-training on explanation performance in MR image classification.","2023","CoRR","Marta Oliveira, Rick Wilming, Benedict Clark, Céline Budding, Fabian Eitel, Kerstin Ritter, Stefan Haufe","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2306-12150","Convolutional Neural Networks (CNNs) are frequently and successfully used in medical prediction tasks. They are often used in combination with transfer learning, leading to improved performance when training data for the task are scarce. The resulting models are highly complex and typically do not provide any insight into their predictive mechanisms, motivating the field of 'explainable' artificial intelligence (XAI). However, previous studies have rarely quantitatively evaluated the 'explanation performance' of XAI methods against ground-truth data, and transfer learning and its influence on objective measures of explanation performance has not been investigated. Here, we propose a benchmark dataset that allows for quantifying explanation performance in a realistic magnetic resonance imaging (MRI) classification task. We employ this benchmark to understand the influence of transfer learning on the quality of explanations. Experimental results show that popular XAI methods applied to the same underlying model differ vastly in performance, even when considering only correctly classified examples. We further observe that explanation performance strongly depends on the task used for pre-training and the number of CNN layers pre-trained. These results hold after correcting for a substantial correlation between explanation and classification performance."
"Deep neural network heatmaps capture Alzheimer's disease patterns reported in a large meta-analysis of neuroimaging studies.","2023","NeuroImage","Di Wang, Nicolas Honnorat, Peter T. Fox, Kerstin Ritter, Simon B. Eickhoff, Sudha Seshadri, Mohamad Habes","Journal Articles","https://dblp.org/rec/journals/neuroimage/WangHFRESH23",""
"Identifying confounders in deep-learning-based model predictions using DeepRepViz.","2023","CoRR","Roshan Prakash Rane, JiHoon Kim, Arjun Umesha, Didem Stark, Marc-André Schulz, Kerstin Ritter","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2309-15551",""
"Promises and pitfalls of deep neural networks in neuroimaging-based psychiatric research.","2023","CoRR","Fabian Eitel, Marc-André Schulz, Moritz Seiler, Henrik Walter, Kerstin Ritter","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2301-08525",""
