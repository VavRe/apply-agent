"title","year","venue","authors","type","url","abstract"
"Ethical considerations and statistical analysis of industry involvement in machine learning research.","2023","AI Soc.","Thilo Hagendorff, Kristof Meding","Journal Articles","https://dblp.org/rec/journals/ais/HagendorffM23","AbstractIndustry involvement in the machine learning (ML) community seems to be increasing. However, the quantitative scale and ethical implications of this influence are rather unknown. For this purpose, we have not only carried out an informed ethical analysis of the field, but have inspected all papers of the main ML conferences NeurIPS, CVPR, and ICML of the last 5 years—almost 11,000 papers in total. Our statistical approach focuses on conflicts of interest, innovation, and gender equality. We have obtained four main findings. (1) Academic–corporate collaborations are growing in numbers. At the same time, we found that conflicts of interest are rarely disclosed. (2) Industry papers amply mention terms that relate to particular trending machine learning topics earlier than academia does. (3) Industry papers are not lagging behind academic papers with regard to how often they mention keywords that are proxies for social impact considerations. (4) Finally, we demonstrate that industry papers fall short of their academic counterparts with respect to the ratio of gender diversity. We believe that this work is a starting point for an informed debate within and outside of the ML community."
"Fairness Hacking: The Malicious Practice of Shrouding Unfairness in Algorithms.","2023","CoRR","Kristof Meding, Thilo Hagendorff","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2311-06826","AbstractFairness in machine learning (ML) is an ever-growing field of research due to the manifold potential for harm from algorithmic discrimination. To prevent such harm, a large body of literature develops new approaches to quantify fairness. Here, we investigate how one can divert the quantification of fairness by describing a practice we call “fairness hacking” for the purpose of shrouding unfairness in algorithms. This impacts end-users who rely on learning algorithms, as well as the broader community interested in fair AI practices. We introduce two different categories of fairness hacking in reference to the established concept of p-hacking. The first category, intra-metric fairness hacking, describes the misuse of a particular metric by adding or removing sensitive attributes from the analysis. In this context, countermeasures that have been developed to prevent or reduce p-hacking can be applied to similarly prevent or reduce fairness hacking. The second category of fairness hacking is inter-metric fairness hacking. Inter-metric fairness hacking is the search for a specific fair metric with given attributes. We argue that countermeasures to prevent or reduce inter-metric fairness hacking are still in their infancy. Finally, we demonstrate both types of fairness hacking using real datasets. Our paper intends to serve as a guidance for discussions within the fair ML community to prevent or reduce the misuse of fairness metrics, and thus reduce overall harm from ML applications."
