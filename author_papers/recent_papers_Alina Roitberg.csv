"title","year","venue","authors","type","url","abstract"
"Chart4Blind: An Intelligent Interface for Chart Accessibility Conversion.","2024","IUI","Omar Moured, Morris Baumgarten-Egemole, Karin Müller 0001, Alina Roitberg, Thorsten Schwarz, Rainer Stiefelhagen","Conference and Workshop Papers","https://dblp.org/rec/conf/iui/MouredB0RSS24","In a world driven by data visualization, ensuring the inclusive accessibility of charts for Blind and Visually Impaired (BVI) individuals remains a significant challenge. Charts are usually presented as raster graphics without textual and visual metadata needed for an equivalent exploration experience for BVI people. Additionally, converting these charts into accessible formats requires considerable effort from sighted individuals. Digitizing charts with metadata extraction is just one aspect of the issue; transforming it into accessible modalities, such as tactile graphics, presents another difficulty. To address these disparities, we propose Chart4Blind, an intelligent user interface that converts bitmap image representations of line charts into universally accessible formats. Chart4Blind achieves this transformation by generating Scalable Vector Graphics (SVG), Comma-Separated Values (CSV), and alternative text exports, all comply with established accessibility standards. Through interviews and a formal user study, we demonstrate that even inexperienced sighted users can make charts accessible in an average of 4 minutes using Chart4Blind, achieving a System Usability Scale rating of 90%. In comparison to existing approaches, Chart4Blind provides a comprehensive solution, generating end-to-end accessible SVGs suitable for assistive technologies such as embossed prints (papers and laser cut), 2D tactile displays, and screen readers. For additional information, including open-source codes and demos, please visit our project page https://moured.github.io/chart4blind/."
"Elevating Skeleton-Based Action Recognition with Efficient Multi-Modality Self-Supervision.","2024","ICASSP","Yiping Wei, Kunyu Peng, Alina Roitberg, Jiaming Zhang 0001, Junwei Zheng, Ruiping Liu, Yufan Chen, Kailun Yang 0001, Rainer Stiefelhagen","Conference and Workshop Papers","https://dblp.org/rec/conf/icassp/WeiPRZZLC0S24","Self-supervised representation learning for human action recognition has developed rapidly in recent years. Most of the existing works are based on skeleton data while using a multi-modality setup. These works overlooked the differences in performance among modalities, which led to the propagation of erroneous knowledge between modalities while only three fundamental modalities, i.e., joints, bones, and motions are used, hence no additional modalities are explored.In this work, we first propose an Implicit Knowledge Exchange Module (IKEM) which alleviates the propagation of erroneous knowledge between low-performance modalities. Then, we further propose three new modalities to enrich the complementary information between modalities. Finally, to maintain efficiency when introducing new modalities, we propose a novel teacher-student framework to distill the knowledge from the secondary modalities into the mandatory modalities considering the relationship constrained by anchors, positives, and negatives, named relational cross-modality knowledge distillation. The experimental results demonstrate the effectiveness of our approach, unlocking the efficient use of skeleton-based multi-modality data. Source code will be made publicly available at https://github.com/desehuileng0o0/IKEM."
"Learning human actions from complex manipulation tasks and their transfer to robots in the circular factory.","2024","Autom.","Manuel Zaremski, Blanca Handwerker, Christian R. G. Dreher, Fabian Leven, David Schneider, Alina Roitberg, Rainer Stiefelhagen, Gerhard Neumann, Michael Heizmann, Tamim Asfour, Barbara Deml","Journal Articles","https://dblp.org/rec/journals/at/ZaremskiHDLSRSNHAD24","
 Process automation is essential to establish an economically viable circular factory in high-wage locations. This involves using autonomous production technologies, such as robots, to disassemble, reprocess, and reassemble used products with unknown conditions into the original or a new generation of products. This is a complex and highly dynamic issue that involves a high degree of uncertainty. To adapt robots to these conditions, learning from humans is necessary. Humans are the most flexible resource in the circular factory and they can adapt their knowledge and skills to new tasks and changing conditions. This paper presents an interdisciplinary research framework for learning human action knowledge from complex manipulation tasks through human observation and demonstration. The acquired knowledge will be described in a machine-executable form and will be transferred to industrial automation execution by robots in a circular factory. There are two primary research objectives. First, we investigate the multi-modal capture of human behavior and the description of human action knowledge. Second, the reproduction and generalization of learned actions, such as disassembly and assembly actions on robots is studied."
"Managing uncertainty in product and process design for the circular factory.","2024","Autom.","Michael Heizmann, Jürgen Beyerer, Stefan Dietrich, Luisa Hoffmann, Jan-Philipp Kaiser, Gisela Lanza, Alina Roitberg, Rainer Stiefelhagen, Nicole Stricker, Helena Wexel, Frederik Zanger","Journal Articles","https://dblp.org/rec/journals/at/HeizmannBDHKLRSSWZ24","
 In the circular factory, uncertain attributes of object instances and process steps are found at diverse occasions. Even if uncertainty can also be found to some extent in linear production, the high variation of product attributes of used objects causes the process steps in the circular factory to generate a much higher variability of the properties of the objects handled in circular processes. In consequence, a methodology is needed to model, handle and manage uncertainties at all relevant situations within the circular factory. In contrast to linear production, the uncertainty of attributes cannot be extended to an object class (with the same production history), but must be assigned to each object instance (with its own history) individually. In this contribution, the basic concepts for managing uncertainty in the circular factory are introduced. As a common basis, probabilities are used to express uncertainty, thus being compatible with the traditional and proven concepts of measurement science and stochastics. To describe the individual information state of object instances, it is complemented with a joint probability distribution describing all relevant object attributes. Some examples for processes within the circular factory demonstrate how uncertainty is considered to manage the uncertainty related challenges of used objects."
"Navigating Open Set Scenarios for Skeleton-Based Action Recognition.","2024","AAAI","Kunyu Peng, Cheng Yin, Junwei Zheng, Ruiping Liu, David Schneider, Jiaming Zhang 0001, Kailun Yang 0001, M. Saquib Sarfraz, Rainer Stiefelhagen, Alina Roitberg","Conference and Workshop Papers","https://dblp.org/rec/conf/aaai/PengYZLS00SSR24","In real-world scenarios, human actions often fall outside the distribution of training data, making it crucial for models to recognize known actions and reject unknown ones. However, using pure skeleton data in such open-set conditions poses challenges due to the lack of visual background cues and the distinct sparse structure of body pose sequences. In this paper, we tackle the unexplored Open-Set Skeleton-based Action Recognition (OS-SAR) task and formalize the benchmark on three skeleton-based datasets. We assess the performance of seven established open-set approaches on our task and identify their limits and critical generalization issues when dealing with skeleton information.To address these challenges, we propose a distance-based cross-modality ensemble method that leverages the cross-modal alignment of skeleton joints, bones, and velocities to achieve superior open-set recognition performance. We refer to the key idea as CrossMax - an approach that utilizes a novel cross-modality mean max discrepancy suppression mechanism to align latent spaces during training and a cross-modality distance-based logits refinement method during testing. CrossMax outperforms existing approaches and consistently yields state-of-the-art results across all datasets and backbones. We will release the benchmark, code, and models to the community."
"Probing Fine-Grained Action Understanding and Cross-View Generalization of Foundation Models.","2024","CoRR","Thinesh Thiyakesan Ponbagavathi, Kunyu Peng, Alina Roitberg","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2407-15605","Foundation models (FMs) are large neural networks trained on broad datasets, excelling in downstream tasks with minimal fine-tuning. Human activity recognition in video has advanced with FMs, driven by competition among different architectures. However, high accuracies on standard benchmarks can draw an artificially rosy picture, as they often overlook real-world factors like changing camera perspectives. Popular benchmarks, mostly from YouTube or movies, offer diverse views but only coarse actions, which are insufficient for use-cases needing fine-grained, domain-specific actions. Domain-specific datasets (e.g., for industrial assembly) typically use data from limited static perspectives. This paper empirically evaluates how perspective changes affect different FMs in fine-grained human activity recognition. We compare multiple backbone architectures and design choices, including image- and video- based models, and various strategies for temporal information fusion, including commonly used score averaging and more novel attention-based temporal aggregation mechanisms. This is the first systematic study of different foundation models and specific design choices for human activity recognition from unknown views, conducted with the goal to provide guidance for backbone- and temporal- fusion scheme selection. Code and models will be made publicly available to the community."
"Referring Atomic Video Action Recognition.","2024","CoRR","Kunyu Peng, Jia Fu 0001, Kailun Yang 0001, Di Wen 0006, Yufan Chen, Ruiping Liu, Junwei Zheng, Jiaming Zhang 0001, M. Saquib Sarfraz, Rainer Stiefelhagen, Alina Roitberg","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2407-01872","We introduce a new task called Referring Atomic Video Action Recognition (RAVAR), aimed at identifying atomic actions of a particular person based on a textual description and the video data of this person. This task differs from traditional action recognition and localization, where predictions are delivered for all present individuals. In contrast, we focus on recognizing the correct atomic action of a specific individual, guided by text. To explore this task, we present the RefAVA dataset, containing 36,630 instances with manually annotated textual descriptions of the individuals. To establish a strong initial benchmark, we implement and validate baselines from various domains, e.g., atomic action localization, video question answering, and text-video retrieval. Since these existing methods underperform on RAVAR, we introduce RefAtomNet -- a novel cross-stream attention-driven method specialized for the unique challenges of RAVAR: the need to interpret a textual referring expression for the targeted individual, utilize this reference to guide the spatial localization and harvest the prediction of the atomic actions for the referring person. The key ingredients are: (1) a multi-stream architecture that connects video, text, and a new location-semantic stream, and (2) cross-stream agent attention fusion and agent token fusion which amplify the most relevant information across these streams and consistently surpasses standard attention-based fusion on RAVAR. Extensive experiments demonstrate the effectiveness of RefAtomNet and its building blocks for recognizing the action of the described individual. The dataset and code will be made publicly available at https://github.com/KPeng9510/RAVAR."
"Skeleton-Based Human Action Recognition with Noisy Labels.","2024","CoRR","Yi Xu, Kunyu Peng, Di Wen 0006, Ruiping Liu, Junwei Zheng, Yufan Chen, Jiaming Zhang 0001, Alina Roitberg, Kailun Yang 0001, Rainer Stiefelhagen","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2403-09975","Understanding human actions from body poses is critical for assistive robots sharing space with humans in order to make informed and safe decisions about the next interaction. However, precise temporal localization and annotation of activity sequences is time-consuming and the resulting labels are often noisy. If not effectively addressed, label noise negatively affects the model's training, resulting in lower recognition quality. Despite its importance, addressing label noise for skeleton-based action recognition has been overlooked so far. In this study, we bridge this gap by implementing a framework that augments well-established skeleton-based human action recognition methods with label-denoising strategies from various research areas to serve as the initial benchmark. Observations reveal that these baselines yield only marginal performance when dealing with sparse skeleton data. Consequently, we introduce a novel methodology, NoiseEraSAR, which integrates global sample selection, co-teaching, and Cross-Modal Mixture-of-Experts (CM-MOE) strategies, aimed at mitigating the adverse impacts of label noise. Our proposed approach demonstrates better performance on the established benchmark, setting new state-of-the-art standards. The source code for this study is accessible at https://github.com/xuyizdby/NoiseEraSAR."
"SynthAct: Towards Generalizable Human Action Recognition based on Synthetic Data.","2024","ICRA","David Schneider, Marco Keller, Zeyun Zhong, Kunyu Peng, Alina Roitberg, Jürgen Beyerer, Rainer Stiefelhagen","Conference and Workshop Papers","https://dblp.org/rec/conf/icra/SchneiderKZPRBS24","Synthetic data generation is a proven method for augmenting training sets without the need for extensive setups, yet its application in human activity recognition is underexplored. This is particularly crucial for human-robot collaboration in household settings, where data collection is often privacy-sensitive. In this paper, we introduce SynthAct, a synthetic data generation pipeline designed to significantly minimize the reliance on real-world data. Leveraging modern 3D pose estimation techniques, SynthAct can be applied to arbitrary 2D or 3D video action recordings, making it applicable for uncontrolled in-the-field recordings by robotic agents or smarthome monitoring systems. We present two SynthAct datasets: AMARV, a large synthetic collection with over 800k multi-view action clips, and Synthetic Smarthome, mirroring the Toyota Smarthome dataset. SynthAct generates a rich set of data, including RGB videos and depth maps from four synchronized views, 3D body poses, normal maps, segmentation masks and bounding boxes. We validate the efficacy of our datasets through extensive synthetic-to-real experiments on NTU RGB+D and Toyota Smarthome. SynthAct is available on our project page4."
"Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints.","2024","CoRR","Jonas Nasimzada, Jens Kleesiek, Ken Herrmann, Alina Roitberg, Constantin Seibold","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2409-16382","Recognizing pain in video is crucial for improving patient-computer interaction systems, yet traditional data collection in this domain raises significant ethical and logistical challenges. This study introduces a novel approach that leverages synthetic data to enhance video-based pain recognition models, providing an ethical and scalable alternative. We present a pipeline that synthesizes realistic 3D facial models by capturing nuanced facial movements from a small participant pool, and mapping these onto diverse synthetic avatars. This process generates 8,600 synthetic faces, accurately reflecting genuine pain expressions from varied angles and perspectives. Utilizing advanced facial capture techniques, and leveraging public datasets like CelebV-HQ and FFHQ-UV for demographic diversity, our new synthetic dataset significantly enhances model training while ensuring privacy by anonymizing identities through facial replacements. Experimental results demonstrate that models trained on combinations of synthetic data paired with a small amount of real participants achieve superior performance in pain recognition, effectively bridging the gap between synthetic simulations and real-world applications. Our approach addresses data scarcity and ethical concerns, offering a new solution for pain detection and opening new avenues for research in privacy-preserving dataset generation. All resources are publicly available to encourage further innovation in this field."
"AdaptiveClick: Clicks-aware Transformer with Adaptive Focal Loss for Interactive Image Segmentation.","2023","CoRR","Jiacheng Lin, Jiajun Chen, Kailun Yang 0001, Alina Roitberg, Siyu Li, Zhiyong Li 0001, Shutao Li","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2305-04276","Interactive image segmentation (IIS) has emerged as a promising technique for decreasing annotation time. Substantial progress has been made in pre-and post-processing for IIS, but the critical issue of interaction ambiguity, notably hindering segmentation quality, has been under-researched. To address this, we introduce AdaptiveClick - a click-aware transformer incorporating an adaptive focal loss (AFL) that tackles annotation inconsistencies with tools for mask-and pixel-level ambiguity resolution. To the best of our knowledge, AdaptiveClick is the first transformer-based, mask-adaptive segmentation framework for IIS. The key ingredient of our method is the click-aware mask-adaptive transformer decoder (CAMD), which enhances the interaction between click and image features. Additionally, AdaptiveClick enables pixel-adaptive differentiation of hard and easy samples in the decision space, independent of their varying distributions. This is primarily achieved by optimizing a generalized AFL with a theoretical guarantee, where two adaptive coefficients control the ratio of gradient values for hard and easy pixels. Our analysis reveals that the commonly used Focal and BCE losses can be considered special cases of the proposed AFL. With a plain ViT backbone, extensive experimental results on nine datasets demonstrate the superiority of AdaptiveClick compared to state-of-the-art methods. The source code is publicly available at https://github.com/lab206/AdaptiveClick."
"Delving Deep Into One-Shot Skeleton-Based Action Recognition With Diverse Occlusions.","2023","IEEE Trans. Multim.","Kunyu Peng, Alina Roitberg, Kailun Yang 0001, Jiaming Zhang 0001, Rainer Stiefelhagen","Journal Articles","https://dblp.org/rec/journals/tmm/PengRYZS23","Occlusions areuniversal disruptions constantly present in the real world. Especially for sparse representations, such as human skeletons, a few occluded points might destroy the geometrical and temporal continuity critically affecting the results. Yet, the research of data-scarce recognition from skeleton sequences, such as one-shot action recognition, does not explicitly consider occlusions despite their everyday pervasiveness. In this work, we explicitly tackle body occlusions for Skeleton-based One-shot Action Recognition (SOAR). We mainly consider two occlusion variants: 1) random occlusions and 2) more realistic occlusions caused by diverse everyday objects, which we generate by projecting the existing IKEA 3D furniture models into the camera coordinate system of the 3D skeletons with different geometric parameters, (e.g., rotation and displacement). We leverage the proposed pipeline to blend out portions of skeleton sequences of the three popular action recognition datasets (NTU-120, NTU-60 and Toyota Smart Home) and formalize the first benchmark for SOAR from partially occluded body poses. This is the first benchmark which considers occlusions for data-scarce action recognition. Another key property of our benchmark are the more realistic occlusions generated by everyday objects, as even in standard recognition from 3D skeletons, only randomly missing joints were considered. We re-evaluate existing state-of-the-art frameworks for SOAR in the light of this new task and further introduce Trans4SOAR – a new transformer-based model which leverages three data streams and mixed attention fusion mechanism to alleviate the adverse effects caused by occlusions. While our experiments demonstrate a clear decline in accuracy with missing skeleton portions, this effect is smaller with Trans4SOAR, which outperforms other architectures on all datasets. Although we specifically focus on occlusions, Trans4SOAR additionally yields state-of-the-art in the standard SOAR without occlusion, surpassing the best published approach by 2.85% on NTU-120."
"Elevating Skeleton-Based Action Recognition with Efficient Multi-Modality Self-Supervision.","2023","CoRR","Yiping Wei, Kunyu Peng, Alina Roitberg, Jiaming Zhang 0001, Junwei Zheng, Ruiping Liu, Yufan Chen, Kailun Yang 0001, Rainer Stiefelhagen","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2309-12009","Self-supervised representation learning for human action recognition has developed rapidly in recent years. Most of the existing works are based on skeleton data while using a multi-modality setup. These works overlooked the differences in performance among modalities, which led to the propagation of erroneous knowledge between modalities while only three fundamental modalities, i.e., joints, bones, and motions are used, hence no additional modalities are explored.In this work, we first propose an Implicit Knowledge Exchange Module (IKEM) which alleviates the propagation of erroneous knowledge between low-performance modalities. Then, we further propose three new modalities to enrich the complementary information between modalities. Finally, to maintain efficiency when introducing new modalities, we propose a novel teacher-student framework to distill the knowledge from the secondary modalities into the mandatory modalities considering the relationship constrained by anchors, positives, and negatives, named relational cross-modality knowledge distillation. The experimental results demonstrate the effectiveness of our approach, unlocking the efficient use of skeleton-based multi-modality data. Source code will be made publicly available at https://github.com/desehuileng0o0/IKEM."
"FeatFSDA: Towards Few-shot Domain Adaptation for Video-based Activity Recognition.","2023","CoRR","Kunyu Peng, Di Wen 0006, David Schneider, Jiaming Zhang 0001, Kailun Yang 0001, M. Saquib Sarfraz, Rainer Stiefelhagen, Alina Roitberg","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2305-08420",""
"FishDreamer: Towards Fisheye Semantic Completion via Unified Image Outpainting and Segmentation.","2023","CVPR Workshops","Hao Shi, Yu Li, Kailun Yang 0001, Jiaming Zhang 0001, Kunyu Peng, Alina Roitberg, Yaozu Ye, Huajian Ni, Kaiwei Wang, Rainer Stiefelhagen","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/ShiLYZPRYNWS23","This paper raises the new task of Fisheye Semantic Completion (FSC), where dense texture, structure, and semantics of a fisheye image are inferred even beyond the sensor field-of-view (FoV). Fisheye cameras have larger FoV than ordinary pinhole cameras, yet its unique special imaging model naturally leads to a blind area at the edge of the image plane. This is suboptimal for safety-critical applications since important perception tasks, such as semantic segmentation, become very challenging within the blind zone. Previous works considered the out-FoV outpainting and in-FoV segmentation separately. However, we observe that these two tasks are actually closely coupled. To jointly estimate the tightly intertwined complete fisheye image and scene semantics, we introduce the new FishDreamer which relies on successful ViTs enhanced with a novel Polar-aware Cross Attention module (PCA) to leverage dense context and guide semantically-consistent content generation while considering different polar distributions. In addition to the contribution of the novel task and architecture, we also derive Cityscapes-BF and KITTI360-BF datasets to facilitate training and evaluation of this new track. Our experiments demonstrate that the proposed FishDreamer outperforms methods solving each task in isolation and surpasses alternative approaches on the Fisheye Semantic Completion. Code and datasets are publicly available at FishDreamer."
"Line Graphics Digitization: A Step Towards Full Automation.","2023","ICDAR","Omar Moured, Jiaming Zhang 0001, Alina Roitberg, Thorsten Schwarz, Rainer Stiefelhagen","Conference and Workshop Papers","https://dblp.org/rec/conf/icdar/MouredZRSS23","The digitization of documents allows for wider accessibility and reproducibility. While automatic digitization of document layout and text content has been a long-standing focus of research, this problem in regard to graphical elements, such as statistical plots, has been under-explored. In this paper, we introduce the task of fine-grained visual understanding of mathematical graphics and present the Line Graphics (LG) dataset, which includes pixel-wise annotations of 5 coarse and 10 fine-grained categories. Our dataset covers 520 images of mathematical graphics collected from 450 documents from different disciplines. Our proposed dataset can support two different computer vision tasks, i.e., semantic segmentation and object detection. To benchmark our LG dataset, we explore 7 state-of-the-art models. To foster further research on the digitization of statistical graphs, we will make the dataset, code, and models publicly available to the community."
"MuscleMap: Towards Video-based Activated Muscle Group Estimation.","2023","CoRR","Kunyu Peng, David Schneider, Alina Roitberg, Kailun Yang 0001, Jiaming Zhang 0001, M. Saquib Sarfraz, Rainer Stiefelhagen","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2303-00952",""
"On Transferability of Driver Observation Models from Simulated to Real Environments in Autonomous Cars.","2023","ITSC","Walter Morales-Alvarez, Novel Certad, Alina Roitberg, Rainer Stiefelhagen, Cristina Olaverri-Monreal","Conference and Workshop Papers","https://dblp.org/rec/conf/itsc/Morales-Alvarez23",""
"Quantized Distillation: Optimizing Driver Activity Recognition Models for Resource-Constrained Environments.","2023","IROS","Calvin Tanama, Kunyu Peng, Zdravko Marinov, Rainer Stiefelhagen, Alina Roitberg","Conference and Workshop Papers","https://dblp.org/rec/conf/iros/TanamaPMSR23","Deep learning-based models are at the top of most driver observation benchmarks due to their remarkable accuracies but come with a high computational cost, while the resources are often limited in real-world driving scenarios. This paper presents a lightweight framework for resource- efficient driver activity recognition. We enhance 3D MobileNet, a speed-optimized neural architecture for video classification, with two paradigms for improving the trade-off between model accuracy and computational efficiency: knowledge distillation and model quantization. Knowledge distillation prevents large drops in accuracy when reducing the model size by harvesting knowledge from a large teacher model (I3D) via soft labels instead of using the original ground truth. Quantization further drastically reduces the memory and computation requirements by representing the model weights and activations using lower precision integers. Extensive experiments on a public dataset for in-vehicle monitoring during autonomous driving show that our proposed framework leads to an 3- fold reduction in model size and 1.4-fold improvement in inference time compared to an already speed-optimized architecture. Our code is available at https://github.com/calvintanama/qd-driver-activity-reco."
"Towards Privacy-Supporting Fall Detection via Deep Unsupervised RGB2Depth Adaptation.","2023","CoRR","Hejun Xiao, Kunyu Peng, Xiangsheng Huang, Alina Roitberg, Hao Li, Zhaohui Wang, Rainer Stiefelhagen","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2308-12049","Fall detection is a vital task in health monitoring, as it allows the system to trigger an alert and therefore enable faster interventions when a person experiences a fall. Although most previous approaches rely on standard RGB video data, such detailed appearance-aware monitoring poses significant privacy concerns. Depth sensors, on the other hand, are better at preserving privacy as they merely capture the distance of objects from the sensor or camera, omitting color and texture information. In this article, we introduce a privacy-supporting solution that makes the RGB-trained model applicable in the depth domain and utilizes depth data at test time for fall detection. To achieve cross-modal fall detection, we present an unsupervised RGB domain to the depth domain (RGB2Depth) cross-modal domain adaptation approach that leverages labeled RGB data and unlabeled depth data during training. Our proposed pipeline incorporates an intermediate domain module (IDM) for feature bridging, modality adversarial loss for modality discrimination, classification loss for pseudo-labeled depth data and labeled source data, triplet loss that considers both the source and target domains, and a novel adaptive loss weight adjustment method for improved coordination among various losses. Our approach achieves state-of-the-art results in the unsupervised RGB2Depth domain adaptation task for fall detection. Code is available at https://github.com/1015206533/privacy_supporting_fall_detection."
"Navigating Open Set Scenarios for Skeleton-based Action Recognition.","2023","CoRR","Kunyu Peng, Cheng Yin, Junwei Zheng, Ruiping Liu, David Schneider, Jiaming Zhang 0001, Kailun Yang 0001, M. Saquib Sarfraz, Rainer Stiefelhagen, Alina Roitberg","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2312-06330","In real-world scenarios, human actions often fall outside the distribution of training data, making it crucial for models to recognize known actions and reject unknown ones. However, using pure skeleton data in such open-set conditions poses challenges due to the lack of visual background cues and the distinct sparse structure of body pose sequences. In this paper, we tackle the unexplored Open-Set Skeleton-based Action Recognition (OS-SAR) task and formalize the benchmark on three skeleton-based datasets. We assess the performance of seven established open-set approaches on our task and identify their limits and critical generalization issues when dealing with skeleton information.To address these challenges, we propose a distance-based cross-modality ensemble method that leverages the cross-modal alignment of skeleton joints, bones, and velocities to achieve superior open-set recognition performance. We refer to the key idea as CrossMax - an approach that utilizes a novel cross-modality mean max discrepancy suppression mechanism to align latent spaces during training and a cross-modality distance-based logits refinement method during testing. CrossMax outperforms existing approaches and consistently yields state-of-the-art results across all datasets and backbones. We will release the benchmark, code, and models to the community."
"Unveiling the Hidden Realm: Self-supervised Skeleton-based Action Recognition in Occluded Environments.","2023","CoRR","Yifei Chen, Kunyu Peng, Alina Roitberg, David Schneider, Jiaming Zhang 0001, Junwei Zheng, Ruiping Liu, Yufan Chen, Kailun Yang 0001, Rainer Stiefelhagen","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2309-12029",""
