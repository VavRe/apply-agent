"title","year","venue","authors","type","url","abstract"
"ChatPose: Chatting about 3D Human Pose.","2024","CVPR","Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun 0030, Priyanka Patel, Michael J. Black","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/FengLDSPB24","We introduce ChatPose, a framework employing Large Language Models (LLMs) to understand and reason about 3D human poses from images or textual descriptions. Our work is motivated by the human ability to intuitively under-stand postures from a single image or a brief description, a process that intertwines image interpretation, world knowl-edge, and an understanding of body language. Traditional human pose estimation and generation methods often op-erate in isolation, lacking semantic understanding and rea-soning abilities. ChatPose addresses these limitations by embedding SMPL poses as distinct signal tokens within a multimodal LLM, enabling the direct generation of 3D body poses from both textual and visual inputs. Leveraging the powerful capabilities of multimodal LLMs, ChatPose uni-fies classical 3D human pose and generation tasks while offering user interactions. Additionally, ChatPose empow-ers LLMs to apply their extensive world knowledge in rea-soning about human poses, leading to two advanced tasks: speculative pose generation and reasoning about pose esti-mation. These tasks involve reasoning about humans to generate 3D poses from subtle text queries, possibly ac-companied by images. We establish benchmarks for these tasks, moving beyond traditional 3D pose generation and estimation methods. Our results show that ChatPose out-performs existing multimodal LLMs and task-specific meth-ods on these newly proposed tasks. Furthermore, Chat-Pose's ability to understand and generate 3D human poses based on complex reasoning opens new directions in human pose analysis. Code and data are available for research at https://yfeng95.github.io/ChatPose."
"EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Expressive Masked Audio Gesture Modeling.","2024","CVPR","Haiyang Liu, Zihao Zhu, Giorgio Becherini, Yichen Peng, Mingyang Su, You Zhou, Xuefei Zhe, Naoya Iwamoto, Bo Zheng, Michael J. Black","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/LiuZBPSZZIZB24","We propose EMAGE, a framework to generate full-body human gestures from audio and masked gestures, encompassing facial, local body, hands, and global movements. To achieve this, we first introduce BEAT2 (BEAT-SMPLX-FLAME), a new mesh-level holistic co-speech dataset. BEAT2 combines a MoShed SMPL-X body with FLAME head parameters and further refines the modeling of head, neck, and finger movements, offering a community-standardized, high-quality 3D motion captured dataset. EMAGE leverages masked body gesture priors during training to boost inference performance. It involves a Masked Audio Gesture Transformer, facilitating joint training on audio-to-gesture generation and masked gesture reconstruction to effectively encode audio and body gesture hints. Encoded body hints from masked gestures are then separately employed to generate facial and body movements. Moreover, EMAGE adaptively merges speech features from the audio's rhythm and content and utilizes four compositional VQ-VAEs to enhance the results' fidelity and diversity. Experiments demonstrate that EMAGE generates holistic gestures with state-of-the-art performance and is flexible in accepting predefined spatial-temporal gesture inputs, generating complete, audio-synchronized results. Our code and dataset are available. 1"
"Emotional Speech-Driven 3D Body Animation via Disentangled Latent Diffusion.","2024","CVPR","Kiran Chhatre, Radek Danecek, Nikos Athanasiou, Giorgio Becherini, Christopher E. Peters, Michael J. Black, Timo Bolkart","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/ChhatreDABPBB24","Existing methods for synthesizing 3D human gestures from speech have shown promising results, but they do not explicitly model the impact of emotions on the generated gestures. Instead, these methods directly output animations from speech without control over the expressed emotion. To address this limitation, we present AMUSE, an emotional speech-driven body animation model based on latent diffusion. Our observation is that content (i.e., gestures related to speech rhythm and word utterances), emotion, and personal style are separable. To account for this, AMUSE maps the driving audio to three disentangled latent vectors: one for content, one for emotion, and one for personal style. A latent diffusion model, trained to generate gesture motion sequences, is then conditioned on these latent vectors. Once trained, AMUSE synthesizes 3D human gestures directly from speech with control over the expressed emotions and style by combining the content from the driving speech with the emotion and style of another speech sequence. Randomly sampling the noise of the diffusion model further generates variations of the gesture with the same emotional expressivity. Qualitative, quantitative, and perceptual evaluations demonstrate that AMUSE outputs realistic gesture sequences. Compared to the state of the art, the generated gestures are better synchronized with the speech content, and better represent the emotion expressed by the input speech. Our code is available at amuse.is.tue.mpg.de."
"GRIP: Generating Interaction Poses Using Spatial Cues and Latent Consistency.","2024","3DV","Omid Taheri, Yi Zhou 0023, Dimitrios Tzionas, Yang Zhou, Duygu Ceylan, Sören Pirk, Michael J. Black","Conference and Workshop Papers","https://dblp.org/rec/conf/3dim/TaheriZTZCPB24","Hands are dexterous and highly versatile manipulators that are central to how humans interact with objects and their environment. Consequently, modeling realistic hand-object interactions, including the subtle motion of individual fingers, is critical for applications in computer graphics, computer vision, and mixed reality. Prior work on capturing and modeling humans interacting with objects in 3D focuses on the body and object motion, often ignoring hand pose. In contrast, we introduce GRIP, a learning-based method that takes, as input, the 3D motion of the body and the object, and synthesizes realistic motion for both hands before, during, and after object interaction. As a preliminary step before synthesizing the hand motion, we first use a network, ANet, to denoise the arm motion. Then, we leverage the spatio-temporal relationship between the body and the object to extract novel temporal interaction cues, and use them in a two-stage inference pipeline to generate the hand motion. In the first stage, we introduce a new approach to encourage motion temporal consistency in the latent space (LTC) and generate consistent interaction motions. In the second stage, GRIP generates refined hand poses to avoid hand-object penetrations. Given sequences of noisy body and object motion, GRIP “upgrades” them to include hand-object interaction. Quantitative experiments and perceptual studies demonstrate that GRIP outperforms baseline methods and generalizes to unseen objects and motions from different motion-capture datasets. Our models and code are available for research purposes at https://grip.is.tue.mpg.de."
"Generative Proxemics: A Prior for 3D Social Interaction from Images.","2024","CVPR","Lea Müller, Vickie Ye, Georgios Pavlakos, Michael J. Black, Angjoo Kanazawa","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/MullerYPBK24","Social interaction is a fundamental aspect of human behavior and communication. The way individuals position themselves in relation to others, also known as proxemics, conveys social cues and affects the dynamics of social interaction. Reconstructing such interaction from images presents challenges because of mutual occlusion and the limited availability of large training datasets. To address this, we present a novel approach that learns a prior over the 3D proxemics two people in close social interaction and demonstrate its use for single-view 3D reconstruction. We start by creating 3D training data of interacting people using image datasets with contact annotations. We then model the proxemics using a novel denoising diffusion model called BUDDI that learns the joint distribution over the poses of two people in close social interaction. Sampling from our generative proxemics model produces realistic 3D human interactions, which we validate through a perceptual study. We use BUDDI in reconstructing two people in close proximity from an image without any contact annotation via an optimization approach that uses the diffusion model as a prior. Our approach recovers accurate 3D social interactions from noisy initial estimates, outperforming state-of-the-art methods. Our code, data, and model are available at: muelea. github.io/buddi."
"HIT: Estimating Internal Human Implicit Tissues from the Body Surface.","2024","CVPR","Marilyn Keller, Vaibhav Arora, Abdelmouttaleb Dakri, Shivam Chandhok, Jürgen Machann, Andreas Fritsche, Michael J. Black, Sergi Pujades","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/KellerADCMFBP24","The creation of personalized anatomical digital twins is important in the fields of medicine, computer graphics, sports science, and biomechanics. To observe a subject's anatomy, expensive medical devices (MRI or CT) are required and the creation of the digital model is often time-consuming and involves manual effort. Instead, we leverage the fact that the shape of the body surface is correlated with the internal anatomy; e.g. from surface observations alone, one can predict body composition and skeletal structure. In this work, we go further and learn to infer the 3D location of three important anatomic tissues: subcutaneous adipose tissue (fat), lean tissue (muscles and organs), and long bones. To learn to infer these tissues, we tackle several key challenges. We first create a dataset of human tissues by segmenting full-body MRI scans and registering the SMPL body mesh to the body surface. With this dataset, we train HIT (Human Implicit Tissues), an implicit function that, given a point inside a body, predicts its tissue class. HIT leverages the SMPL body model shape and pose parameters to canonicalize the medical data. Unlike SMPL, which is trained from upright 3D scans, MRI scans are acquired with subjects lying on a table, resulting in significant soft-tissue deformation. Consequently, HIT uses a learned volumetric deformation field that undoes these deformations. Since HIT is parameterized by SMPL, we can repose bodies or change the shape of subjects and the internal structures deform appropriately. We perform extensive experiments to validate HIT's ability to predict a plausible internal structure for novel subjects. The dataset and HIT model are available at https://hit.is.tue.mpg.de to foster future research in this direction."
"HOLD: Category-Agnostic 3D Reconstruction of Interacting Hands and Objects from Video.","2024","CVPR","Zicong Fan, Maria Parelli, Maria Eleni Kadoglou, Xu Chen 0025, Muhammed Kocabas, Michael J. Black, Otmar Hilliges","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/FanPK0KBH24","Since humans interact with diverse objects every day, the holistic 3D capture of these interactions is important to understand and model human behaviour. However, most ex-isting methods for hand-object reconstruction from RGB ei-ther assume pre-scanned object templates or heavily rely on limited 3D hand-object data, restricting their ability to scale and generalize to more unconstrained interaction settings. To address this, we introduce HOLD the first category-agnostic method that reconstructs an articulated hand and an object jointly from a monocular interaction video. We develop a compositional articulated implicit model that can reconstruct disentangled 3D hands and ob-jects from 2D images. We also further incorporate hand-object constraints to improve hand-object poses and con-sequently the reconstruction quality. Our method does not rely on any 3D hand-object annotations while significantly outperforming fully-supervised baselines in both in-the-lab and challenging in-the-wild settings. Moreover, we qualita-tively show its robustness in reconstructing from in-the-wild videos. See here for code, data, models, and updates."
"InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction from Multi-view RGB-D Images.","2024","Int. J. Comput. Vis.","Yinghao Huang, Omid Taheri, Michael J. Black, Dimitrios Tzionas","Journal Articles","https://dblp.org/rec/journals/ijcv/HuangTBT24","AbstractHumans constantly interact with objects to accomplish tasks. To understand such interactions, computers need to reconstruct these in 3D from images of whole bodies manipulating objects, e.g., for grasping, moving and using the latter. This involves key challenges, such as occlusion between the body and objects, motion blur, depth ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community has followed a divide-and-conquer approach, focusing either only on interacting hands, ignoring the body, or on interacting bodies, ignoring the hands. However, these are only parts of the problem. On the contrary, recent work focuses on the whole problem. The GRAB dataset addresses whole-body interaction with dexterous hands but captures motion via markers and lacks video, while the BEHAVE dataset captures video of body-object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body SMPL-X model and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the body and object can be used to improve the pose estimation of both. (ii) Consumer-level Azure Kinect cameras let us set up a simple and flexible multi-view RGB-D system for reducing occlusions, with spatially calibrated and temporally synchronized cameras. With our InterCap method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 daily objects of various sizes and affordances, including contact with the hands or feet. To this end, we introduce a new data-driven hand motion prior, as well as explore simple ways for automatic contact detection based on 2D and 3D cues. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6  RGB-D images, paired with pseudo ground-truth 3D body and object meshes. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Data and code are available at https://intercap.is.tue.mpg.de."
"Multi-Track Timeline Control for Text-Driven 3D Human Motion Generation.","2024","CVPR Workshops","Mathis Petrovich, Or Litany, Umar Iqbal 0001, Michael J. Black, Gül Varol, Xue Bin Peng, Davis Rempe","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/PetrovichL0BVPR22",""
"Open X-Embodiment: Robotic Learning Datasets and RT-X Models : Open X-Embodiment Collaboration.","2024","ICRA","Abby O'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta 0004, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alexander Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu 0003, Charlotte Le, Chelsea Finn, Chen Wang 0053, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Paul Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su 0001, Haoshu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I. Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim 0001, Jaimyn Drake, Jan Peters 0001, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu 0001, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang 0002, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei 0001, Liam Tan, Linxi Jim Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma 0001, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen 0001, Nicolas Heess, Nikhil J. Joshi, Niko Sünderhauf, Ning Liu, Norman Di Palo, Nur Muhammad (Mahi) Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R. Sanketi, Patrick Tree Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair 0003, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Liangwei Xu, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang 0001, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang 0016, Zipeng Lin","Conference and Workshop Papers","https://dblp.org/rec/conf/icra/ONeillRMGPLPGMJ24","Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train ""generalist"" X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. The project website is robotics-transformer-x.github.io."
"PACE: Human and Camera Motion Estimation from in-the-wild Videos.","2024","3DV","Muhammed Kocabas, Ye Yuan 0007, Pavlo Molchanov 0001, Yunrong Guo, Michael J. Black, Otmar Hilliges, Jan Kautz, Umar Iqbal 0001","Conference and Workshop Papers","https://dblp.org/rec/conf/3dim/KocabasYMGBHKI24","We present a method to estimate human motion in a global scene from moving cameras. This is a highly challenging task due to the coupling of human and camera motions in the video. To address this problem, we propose a joint optimization framework that disentangles human and camera motions using both foreground human motion priors and background scene features. Unlike existing methods that use SLAM as initialization, we propose to tightly integrate SLAM and human motion priors in an optimization that is inspired by bundle adjustment. Specifically, we optimize human and camera motions to match both the observed human pose and scene features. This design combines the strengths of SLAM and motion priors, which leads to significant improvements in human and camera motion estimation. We additionally introduce a motion prior that is suitable for batch optimization, making our approach significantly more efficient than existing approaches. Finally, we propose a novel synthetic dataset that enables evaluating camera motion in addition to human motion from dynamic videos. Experiments on the synthetic and real-world RICH datasets demonstrate that our approach substantially outperforms prior art in recovering both human and camera motions."
"POCO: 3D Pose and Shape Estimation with Confidence.","2024","3DV","Sai Kumar Dwivedi, Cordelia Schmid, Hongwei Yi, Michael J. Black, Dimitrios Tzionas","Conference and Workshop Papers","https://dblp.org/rec/conf/3dim/DwivediSYBT24","The regression of 3D Human Pose and Shape (HPS) from an image is becoming increasingly accurate. This makes the results useful for downstream tasks like human action recognition or 3D graphics. Yet, no regressor is perfect, and accuracy can be affected by ambiguous image evidence or by poses and appearance that are unseen during training. Most current HPS regressors, however, do not report the confidence of their outputs, meaning that downstream tasks cannot differentiate accurate estimates from inaccurate ones. To address this, we develop POCO, a novel framework for training HPS regressors to estimate not only a 3D human body, but also their confidence, in a single feed-forward pass. Specifically, POCO estimates both the 3D body pose and a per-sample variance. The key idea is to introduce a Dual Conditioning Strategy (DCS) for regressing uncertainty that is highly correlated to pose reconstruction quality. The POCO framework can be applied to any HPS regressor and here we evaluate it by modifying HMR, PARE, and CLIFF. In all cases, training the network to reason about uncertainty helps it learn to more accurately estimate 3D pose. While this was not our goal, the improvement is modest but consistent. Our main motivation is to provide uncertainty estimates for downstream tasks; we demonstrate this in two ways: (1) We use the confidence estimates to bootstrap HPS training. Given unlabeled image data, we take the confident estimates of a POCO-trained regressor as pseudo ground truth. Retraining with this automatically-curated data improves accuracy. (2) We exploit uncertainty in video pose estimation by automatically identifying uncertain frames (e.g. due to occlusion) and “inpainting” these from confident frames. Code and models are available for research at https://poco.is.tue.mpg.de."
"TADA! Text to Animatable Digital Avatars.","2024","3DV","Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxiang Tang, Yangyi Huang, Justus Thies, Michael J. Black","Conference and Workshop Papers","https://dblp.org/rec/conf/3dim/LiaoYXTHTB24",""
"TECA: Text-Guided Generation and Editing of Compositional 3D Avatars.","2024","3DV","Hao Zhang, Yao Feng, Peter Kulits, Yandong Wen, Justus Thies, Michael J. Black","Conference and Workshop Papers","https://dblp.org/rec/conf/3dim/ZhangFKWTB24","Our goal is to create a realistic 3D facial avatar with hair and accessories using only a text description. While this challenge has attracted significant recent interest, existing methods either lack realism, produce unrealistic shapes, or do not support editing, such as modifications to the hairstyle. We argue that existing methods are limited because they employ a monolithic modeling approach, using a single representation for the head, face, hair, and accessories. Our observation is that the hair and face, for example, have very different structural qualities that benefit from different representations. Building on this insight, we generate avatars with a compositional model, in which the head, face, and upper body are represented with traditional 3D meshes, and the hair, clothing, and accessories with neural radiance fields (NeRF). The model-based mesh representation provides a strong geometric prior for the face region, improving realism while enabling editing of the person’s appearance. By using NeRFs to represent the remaining components, our method is able to model and synthesize parts with complex geometry and appearance, such as curly hair and fluffy scarves. Our novel system synthesizes these high-quality compositional avatars from text descriptions. Specifically, we generate a face image using text, fit a parametric shape model to it, and inpaint texture using diffusion models. Conditioned on the generated face, we sequentially generate style components such as hair or clothing using Score Distillation Sampling (SDS) with guidance from CLIPSeg segmentations. However, this alone is not sufficient to produce avatars with a high degree of realism. Consequently, we introduce a hierarchical approach to refine the non-face regions using a BLIP-based loss combined with SDS. The experimental results demonstrate that our method, Text-guided generation and Editing of Compositional Avatars (TECA), produces avatars that are more realistic than those of recent methods while being editable because of their compositional nature. For example, our TECA enables the seamless transfer of compositional features like hairstyles, scarves, and other accessories between avatars. This capability supports applications such as virtual try-on. The code and generated avatars will be publicly available for research purposes at yfeng95.github.io/teca."
"TokenHMR: Advancing Human Mesh Recovery with a Tokenized Pose Representation.","2024","CVPR","Sai Kumar Dwivedi, Yu Sun 0030, Priyanka Patel, Yao Feng, Michael J. Black","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/Dwivedi0PFB24","We address the problem of regressing 3D human pose and shape from a single image, with a focus on 3D accuracy. The current best methods leverage large datasets of 3D pseudo-ground-truth (p-GT) and 2D keypoints, leading to robust performance. With such methods, however, we observe a paradoxical decline in 3D pose accuracy with increasing 2D accuracy. This is caused by biases in the p-GT and the use of an approximate camera projection model. We quantify the error induced by current camera models and show that fitting 2D keypoints and p-GT accurately causes incorrect 3D poses. Our analysis defines the invalid distances within which minimizing 2D and p-GT losses is detrimental. We use this to formulate a new loss, “Threshold-Adaptive Loss Scaling” (TALS), that penalizes gross 2D and p-GT errors but not smaller ones. With such a loss, there are many 3D poses that could equally explain the 2D evidence. To reduce this ambiguity we need a prior over valid human poses but such priors can introduce unwanted bias. To address this, we exploit a tokenized representation of human pose and reformulate the problem as token prediction. This restricts the estimated poses to the space of valid poses, effectively improving robustness to occlusion. Exten-sive experiments on the EMDB and 3DPW datasets show that our reformulated loss and tokenization allows us to train on in-the-wild data while improving 3D accuracy over the state-of-the-art. Our models and code are available for research at https://tokenhmr.is.tue.mpg.de."
"WANDR: Intention-guided Human Motion Generation.","2024","CVPR","Markos Diomataris, Nikos Athanasiou, Omid Taheri, Xi Wang, Otmar Hilliges, Michael J. Black","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/DiomatarisATWHB24","Synthesizing natural human motions that enable a 3D human avatar to walk and reach for arbitrary goals in 3D space remains an unsolved problem with many applications. Existing methods (data-driven or using reinforcement learning) are limited in terms of generalization and motion naturalness. A primary obstacle is the scarcity of training data that combines locomotion with goal reaching. To ad-dress this, we introduce WANDR, a data-driven model that takes an avatar's initial pose and a goal's 3D position and generates natural human motions that place the end effec-tor (wrist) on the goal location. To solve this, we intro-duce novel intention features that drive rich goal-oriented movement. Intention guides the agent to the goal, and in-teractively adapts the generation to novel situations without needing to define sub-goals or the entire motion path. Cru-cially, intention allows training on datasets that have goal-oriented motions as well as those that do not. WANDR is a conditional Variational Auto-Encoder (c- VAE), which we train using the AMASS and CIRCLE datasets. We evaluate our method extensively and demonstrate its ability to gener-ate natural and long-term motions that reach 3D goals and generalize to unseen goal locations. Our models and code are available for research purposes at wandr.is.tue.mpg.de."
"BARC: Breed-Augmented Regression Using Classification for 3D Dog Reconstruction from Images.","2023","Int. J. Comput. Vis.","Nadine Rueegg, Silvia Zuffi, Konrad Schindler, Michael J. Black","Journal Articles","https://dblp.org/rec/journals/ijcv/RueeggZSB23","AbstractThe goal of this work is to reconstruct 3D dogs from monocular images. We take a model-based approach, where we estimate the shape and pose parameters of a 3D articulated shape model for dogs. We consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. Recent work has considered a similar task using the multi-animal SMAL model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. Like previous work, we observe that the original SMAL model is not expressive enough to represent dogs of many different breeds. Moreover, we make the hypothesis that the supervision signal used to train the network, that is 2D keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. We therefore go beyond previous work in two important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. Second, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss, consisting of two parts: One term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one is a breed classification loss. With our approach we obtain 3D dogs that, compared to previous work, are quantitatively better in terms of 2D reconstruction, and significantly better according to subjective and quantitative 3D evaluations. Our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species. We call our method BARC (Breed-Augmented Regression using Classification). Our code is publicly available for research purposes at https://barc.is.tue.mpg.de/."
"BlackBIRDS: Black-Box Inference foR Differentiable Simulators.","2023","J. Open Source Softw.","Arnau Quera-Bofarull, Joel Dyer, Anisoara Calinescu, J. Doyne Farmer, Michael J. Wooldridge","Journal Articles","https://dblp.org/rec/journals/jossw/QueraBofarullDCFW23","BlackBIRDS is a Python package consisting of generically applicable, black-box inference methods for differentiable simulation models. It facilitates both (a) the differentiable implementation of simulation models by providing a common object-oriented framework for their implementation in PyTorch (Paszke et al., 2019), and (b) the use of a variety of gradient-assisted inference procedures for these simulation models, allowing researchers to easily exploit the differentiable nature of their simulator in parameter estimation tasks. The package consists of both Bayesian and non-Bayesian inference methods, and relies on well-supported software libraries (e.g., normflows, Stimper et al., 2023) to provide this broad functionality."
"Neuroimaging-based classification of PTSD using data-driven computational approaches: A multisite big data study from the ENIGMA-PGC PTSD consortium.","2023","NeuroImage","Xi Zhu, Yoojean Kim, Orren Ravid, Xiaofu He, Benjamin Suarez-Jimenez, Sigal Zilcha-Mano, Amit Lazarov, Seonjoo Lee, Chadi G. Abdallah, Michael Angstadt, Christopher L. Averill, C. Lexi Baird, Lee Baugh, Jennifer Urbano Blackford, Jessica Bomyea, Steven E. Bruce, Richard A. Bryant, Zhihong Cao, Kyle Choi, Josh M. Cisler, Andrew S. Cotton, Judith K. Daniels, Nicholas D. Davenport, Richard J. Davidson, Michael D. De Bellis, Emily L. Dennis, Maria Densmore, Terri A. deRoon-Cassini, Seth G. Disner, Wissam El-Hage, Amit Etkin, Negar Fani, Kelene A. Fercho, Jacklynn M. Fitzgerald, Gina L. Forster, Jessie L. Frijling, Elbert Geuze, Atilla Gonenc, Evan M. Gordon, Staci Gruber, Daniel W. Grupe, Jeffrey P. Guenette, Courtney C. Haswell, Ryan J. Herringa, Julia Herzog, David Bernd Hofmann, Bobak Hosseini, Anna R. Hudson, Ashley A. Huggins, Jonathan C. Ipser, Neda Jahanshad, Meilin Jia-Richards, Tanja Jovanovic, Milissa L. Kaufman, Mitzy Kennis, Anthony King, Philipp Kinzel, Saskia B. J. Koch, Inga Koerte, Sheri-Michelle Koopowitz, Mayuresh S. Korgaonkar, John H. Krystal, Ruth A. Lanius, Christine L. Larson, Lauren A. M. Lebois, Gen Li, Israel Liberzon, Guang Ming Lu, Yifeng Luo, Vincent A. Magnotta, Antje Manthey, Adi Maron-Katz, Geoffery May, Katie A. McLaughlin, Sven C. Mueller, Laura Nawijn, Steven M. Nelson, Richard W. J. Neufeld, Jack B. Nitschke, Erin O'Leary, Bunmi O. Olatunji, Miranda Olff, Matthew Peverill, K. Luan Phan, Rongfeng Qi, Yann Quidé, Ivan Rektor, Kerry J. Ressler, Pavel Riha, Marisa Ross, Isabelle M. Rosso, Lauren E. Salminen, Kelly A. Sambrook, Christian Schmahl, Martha Elizabeth Shenton, Margaret A. Sheridan, Chiahao Shih, Maurizio Sicorello, Anika Sierk, Alan N. Simmons, Raluca M. Simons, Jeffrey S. Simons, Scott R. Sponheim, Murray B. Stein, Dan J. Stein, Jennifer S. Stevens, Thomas Straube, Delin Sun, Jean Théberge, Paul M. Thompson, Sophia I. Thomopoulos, Nic J. A. van der Wee, Steven J. A. van der Werff, Theo G. M. van Erp, Sanne J. H. van Rooij, Mirjam van Zuiden, Tim Varkevisser, Dick J. Veltman, Robert R. J. M. Vermeiren, Henrik Walter, Li Wang 0033, Xin Wang 0090, Carissa N. Weis, Sherry Winternitz, Hong Xie, Ye Zhu, Melanie Wall, Yuval Neria, Rajendra A. Morey","Journal Articles","https://dblp.org/rec/journals/neuroimage/ZhuKRHSZLLAAABBBBBBCCCCDDDDD23",""
"Reconstructing Signing Avatars from Video Using Linguistic Priors.","2023","CVPR","Maria-Paola Forte, Peter Kulits, Chun-Hao Huang, Vasileios Choutas, Dimitrios Tzionas, Katherine J. Kuchenbecker, Michael J. Black","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/ForteKHCTKB23","Sign language (SL) is the primary method of communication for the 70 million Deaf people around the world. Video dictionaries of isolated signs are a core SL learning tool. Replacing these with 3D avatars can aid learning and enable AR/VR applications, improving access to technology and online media. However, little work has attempted to estimate expressive 3D avatars from SL video; occlusion, noise, and motion blur make this task difficult. We address this by introducing novel linguistic priors that are universally applicable to SL and provide constraints on 3D hand pose that help resolve ambiguities within isolated signs. Our method, SGNify, captures fine-grained hand pose, facial expression, and body movement fully automatically from in-the-wild monocular SL videos. We evaluate SGNify quantitatively by using a commercial motion-capture system to compute 3D avatars synchronized with monocular video. SGNify outperforms state-of-the-art 3D body-pose-and shape-estimation methods on SL videos. A perceptual study shows that SGNify's 3D reconstructions are significantly more comprehensible and natural than those of previous methods and are on par with the source videos. Code and data are available at sgnify.is.tue.mpg.de."
"AWOL: Analysis WithOut synthesis using Language.","2024","CoRR","Silvia Zuffi, Michael J. Black","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2404-03042","Many classical parametric 3D shape models exist, but creating novel shapes with such models requires expert knowledge of their parameters. For example, imagine creating a specific type of tree using procedural graphics or a new kind of animal from a statistical shape model. Our key idea is to leverage language to control such existing models to produce novel shapes. This involves learning a mapping between the latent space of a vision-language model and the parameter space of the 3D model, which we do using a small set of shape and text pairs. Our hypothesis is that mapping from language to parameters allows us to generate parameters for objects that were never seen during training. If the mapping between language and parameters is sufficiently smooth, then interpolation or generalization in language should translate appropriately into novel 3D shapes. We test our approach with two very different types of parametric shape models (quadrupeds and arboreal trees). We use a learned statistical shape model of quadrupeds and show that we can use text to generate new animals not present during training. In particular, we demonstrate state-of-the-art shape estimation of 3D dogs. This work also constitutes the first language-driven method for generating 3D trees. Finally, embedding images in the CLIP latent space enables us to generate animals and trees directly from images."
"Adversarial Likelihood Estimation With One-Way Flows.","2024","WACV","Omri Ben-Dov, Pravir Singh Gupta, Victoria Fernández Abrevaya, Michael J. Black, Partha Ghosh","Conference and Workshop Papers","https://dblp.org/rec/conf/wacv/Ben-DovGABG24",""
"Can Large Language Models Understand Symbolic Graphics Programs?","2024","CoRR","Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu 0019, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard Schölkopf","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2408-08313","Against the backdrop of enthusiasm for large language models (LLMs), there is an urgent need to scientifically assess their capabilities and shortcomings. This is nontrivial in part because it is difficult to find tasks which the models have not encountered during training. Utilizing symbolic graphics programs, we propose a domain well-suited to test multiple spatial-semantic reasoning skills of LLMs. Popular in computer graphics, these programs procedurally generate visual data. While LLMs exhibit impressive skills in general program synthesis and analysis, symbolic graphics programs offer a new layer of evaluation: they allow us to test an LLM's ability to answer different-grained semantic-level questions of the images or 3D geometries without a vision encoder. To semantically understand the symbolic programs, LLMs would need to possess the ability to""imagine""and reason how the corresponding graphics content would look with only the symbolic description. We use this task to evaluate LLMs by creating a large benchmark for the semantic visual understanding of symbolic graphics programs, built procedurally with minimal human effort. Particular emphasis is placed on transformations of images that leave the image level semantics invariant while introducing significant changes to the underlying program. We evaluate commercial and open-source LLMs on our benchmark to assess their ability to reason about visual output of programs, finding that LLMs considered stronger at reasoning generally perform better. Lastly, we introduce a novel method to improve this ability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned with pre-collected instruction data on symbolic graphics programs. Interestingly, we find that SIT not only improves LLM's understanding on symbolic programs, but it also improves general reasoning ability on various other benchmarks."
"ChatHuman: Language-driven 3D Human Understanding with Retrieval-Augmented Tool Reasoning.","2024","CoRR","Jing Lin, Yao Feng, Weiyang Liu, Michael J. Black","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2405-04533","Numerous methods have been proposed to detect, estimate, and analyze properties of people in images, including the estimation of 3D pose, shape, contact, human-object interaction, emotion, and more. Each of these methods works in isolation instead of synergistically. Here we address this problem and build a language-driven human understanding system -- ChatHuman, which combines and integrates the skills of many different methods. To do so, we finetune a Large Language Model (LLM) to select and use a wide variety of existing tools in response to user inputs. In doing so, ChatHuman is able to combine information from multiple tools to solve problems more accurately than the individual tools themselves and to leverage tool output to improve its ability to reason about humans. The novel features of ChatHuman include leveraging academic publications to guide the application of 3D human-related tools, employing a retrieval-augmented generation model to generate in-context-learning examples for handling new tools, and discriminating and integrating tool results to enhance 3D human understanding. Our experiments show that ChatHuman outperforms existing models in both tool selection accuracy and performance across multiple 3D human-related tasks. ChatHuman is a step towards consolidating diverse methods for human analysis into a single, powerful, system for 3D human reasoning."
"ContourCraft: Learning to Resolve Intersections in Neural Multi-Garment Simulations.","2024","SIGGRAPH","Artur Grigorev 0002, Giorgio Becherini, Michael J. Black, Otmar Hilliges, Bernhard Thomaszewski","Conference and Workshop Papers","https://dblp.org/rec/conf/siggraph/0002BBHT24","Learning-based approaches to cloth simulation have started to show their potential in recent years. However, handling collisions and intersections in neural simulations remains a largely unsolved problem. In this work, we present \moniker{}, a learning-based solution for handling intersections in neural cloth simulations. Unlike conventional approaches that critically rely on intersection-free inputs, \moniker{} robustly recovers from intersections introduced through missed collisions, self-penetrating bodies, or errors in manually designed multi-layer outfits. The technical core of \moniker{} is a novel intersection contour loss that penalizes interpenetrations and encourages rapid resolution thereof. We integrate our intersection loss with a collision-avoiding repulsion objective into a neural cloth simulation method based on graph neural networks (GNNs). We demonstrate our method's ability across a challenging set of diverse multi-layer outfits under dynamic human motions. Our extensive analysis indicates that \moniker{} significantly improves collision handling for learned simulation and produces visually compelling results."
"EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Masked Audio Gesture Modeling.","2024","CoRR","Haiyang Liu, Zihao Zhu, Giorgio Becherini, Yichen Peng, Mingyang Su, You Zhou, Naoya Iwamoto, Bo Zheng, Michael J. Black","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2401-00374","We propose EMAGE, a framework to generate full-body human gestures from audio and masked gestures, encompassing facial, local body, hands, and global movements. To achieve this, we first introduce BEAT2 (BEAT-SMPLX-FLAME), a new mesh-level holistic co-speech dataset. BEAT2 combines a MoShed SMPL-X body with FLAME head parameters and further refines the modeling of head, neck, and finger movements, offering a community-standardized, high-quality 3D motion captured dataset. EMAGE leverages masked body gesture priors during training to boost inference performance. It involves a Masked Audio Gesture Transformer, facilitating joint training on audio-to-gesture generation and masked gesture reconstruction to effectively encode audio and body gesture hints. Encoded body hints from masked gestures are then separately employed to generate facial and body movements. Moreover, EMAGE adaptively merges speech features from the audio's rhythm and content and utilizes four compositional VQ-VAEs to enhance the results' fidelity and diversity. Experiments demonstrate that EMAGE generates holistic gestures with state-of-the-art performance and is flexible in accepting predefined spatial-temporal gesture inputs, generating complete, audio-synchronized results. Our code and dataset are available. 1"
"Explorative Inbetweening of Time and Space.","2024","CoRR","Haiwen Feng, Zheng Ding, Zhihao Xia, Simon Niklaus, Victoria Fernández Abrevaya, Michael J. Black, Xuaner Zhang","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2403-14611","We introduce bounded generation as a generalized task to control video generation to synthesize arbitrary camera and subject motion based only on a given start and end frame. Our objective is to fully leverage the inherent generalization capability of an image-to-video model without additional training or fine-tuning of the original model. This is achieved through the proposed new sampling strategy, which we call Time Reversal Fusion, that fuses the temporally forward and backward denoising paths conditioned on the start and end frame, respectively. The fused path results in a video that smoothly connects the two frames, generating inbetweening of faithful subject motion, novel views of static scenes, and seamless video looping when the two bounding frames are identical. We curate a diverse evaluation dataset of image pairs and compare against the closest existing methods. We find that Time Reversal Fusion outperforms related work on all subtasks, exhibiting the ability to generate complex motions and 3D-consistent views guided by bounded frames. See project page at https://time-reversal.github.io."
"Generating Human Interaction Motions in Scenes with Text Control.","2024","ECCV","Hongwei Yi, Justus Thies, Michael J. Black, Xue Bin Peng, Davis Rempe","Conference and Workshop Papers","https://dblp.org/rec/conf/eccv/YiTBPR24","We present TeSMo, a method for text-controlled scene-aware motion generation based on denoising diffusion models. Previous text-to-motion methods focus on characters in isolation without considering scenes due to the limited availability of datasets that include motion, text descriptions, and interactive scenes. Our approach begins with pre-training a scene-agnostic text-to-motion diffusion model, emphasizing goal-reaching constraints on large-scale motion-capture datasets. We then enhance this model with a scene-aware component, fine-tuned using data augmented with detailed scene information, including ground plane and object shapes. To facilitate training, we embed annotated navigation and interaction motions within scenes. The proposed method produces realistic and diverse human-object interactions, such as navigation and sitting, in different scenes with various object shapes, orientations, initial body positions, and poses. Extensive experiments demonstrate that our approach surpasses prior techniques in terms of the plausibility of human-scene interactions, as well as the realism and variety of the generated motions. Code will be released upon publication of this work at https://research.nvidia.com/labs/toronto-ai/tesmo."
"Ghost on the Shell: An Expressive Representation of General 3D Shapes.","2024","ICLR","Zhen Liu 0019, Yao Feng, Yuliang Xiu, Weiyang Liu, Liam Paull, Michael J. Black, Bernhard Schölkopf","Conference and Workshop Papers","https://dblp.org/rec/conf/iclr/LiuFXLPBS24",""
"HMP: Hand Motion Priors for Pose and Shape Estimation from Video.","2024","WACV","Enes Duran, Muhammed Kocabas, Vasileios Choutas, Zicong Fan, Michael J. Black","Conference and Workshop Papers","https://dblp.org/rec/conf/wacv/DuranKCFB24","Understanding how humans interact with the world necessitates accurate 3D hand pose estimation, a task complicated by the hand’s high degree of articulation, frequent occlusions, self-occlusions, and rapid motions. While most existing methods rely on single-image inputs, videos have useful cues to address aforementioned issues. However, existing video-based 3D hand datasets are insufficient for training feedforward models to generalize to in-the-wild scenarios. On the other hand, we have access to large human motion capture datasets which also include hand motions, e.g. AMASS. Therefore, we develop a generative motion prior specific for hands, trained on the AMASS dataset which features diverse and high-quality hand motions. This motion prior is then employed for video-based 3D hand motion estimation following a latent optimization approach. Our integration of a robust motion prior significantly enhances performance, especially in occluded scenarios. It produces stable, temporally consistent results that surpass conventional single-frame methods. We demonstrate our method’s efficacy via qualitative and quantitative evaluations on the HO3D and DexYCB datasets, with special emphasis on an occlusion-focused subset of HO3D. Code is available at https://hmp.is.tue.mpg.de"
"MotionFix: Text-Driven 3D Human Motion Editing.","2024","CoRR","Nikos Athanasiou, Alpár Ceske, Markos Diomataris, Michael J. Black, Gül Varol","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2408-00712","The focus of this paper is on 3D motion editing. Given a 3D human motion and a textual description of the desired modification, our goal is to generate an edited motion as described by the text. The key challenges include the scarcity of training data and the need to design a model that accurately edits the source motion. In this paper, we address both challenges. We propose a methodology to semi-automatically collect a dataset of triplets comprising (i) a source motion, (ii) a target motion, and (iii) an edit text, introducing the new MotionFix dataset. Access to this data allows us to train a conditional diffusion model, TMED, that takes both the source motion and the edit text as input. We develop several baselines to evaluate our model, comparing it against models trained solely on text-motion pair datasets, and demonstrate the superior performance of our model trained on triplets. We also introduce new retrieval-based metrics for motion editing, establishing a benchmark on the evaluation set of MotionFix. Our results are promising, paving the way for further research in fine-grained motion generation. Code, models, and data are available at https://motionfix.is.tue.mpg.de/ ."
"On Predicting 3D Bone Locations Inside the Human Body.","2024","MICCAI","Abdelmouttaleb Dakri, Vaibhav Arora, Léo Challier, Marilyn Keller, Michael J. Black, Sergi Pujades","Conference and Workshop Papers","https://dblp.org/rec/conf/miccai/DakriACKBP24",""
"Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization.","2024","ICLR","Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu 0019, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, Bernhard Schölkopf","Conference and Workshop Papers","https://dblp.org/rec/conf/iclr/LiuQFXXYF0HPWBW24","Large foundation models are becoming ubiquitous, but training them from scratch is prohibitively expensive. Thus, efficiently adapting these powerful models to downstream tasks is increasingly important. In this paper, we study a principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream task adaptation. Despite demonstrating good generalizability, OFT still uses a fairly large number of trainable parameters due to the high dimensionality of orthogonal matrices. To address this, we start by examining OFT from an information transmission perspective, and then identify a few key desiderata that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast Fourier transform algorithm enables efficient information transmission, we propose an efficient orthogonal parameterization using butterfly structures. We apply this parameterization to OFT, creating a novel parameter-efficient finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a special case, BOFT introduces a generalized orthogonal finetuning framework. Finally, we conduct an extensive empirical study of adapting large vision transformers, large language models, and text-to-image diffusion models to various downstream tasks in vision and language."
"PuzzleAvatar: Assembling 3D Avatars from Personal Albums.","2024","CoRR","Yuliang Xiu, Yufei Ye, Zhen Liu 0019, Dimitrios Tzionas, Michael J. Black","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2405-14869","Generating personalized 3D avatars is crucial for AR/VR. However, recent text-to-3D methods that generate avatars for celebrities or fictional characters, struggle with everyday people. Methods for faithful reconstruction typically require full-body images in controlled settings. What if a user could just upload their personal""OOTD""(Outfit Of The Day) photo collection and get a faithful avatar in return? The challenge is that such casual photo collections contain diverse poses, challenging viewpoints, cropped views, and occlusion (albeit with a consistent outfit, accessories and hairstyle). We address this novel""Album2Human""task by developing PuzzleAvatar, a novel model that generates a faithful 3D avatar (in a canonical pose) from a personal OOTD album, while bypassing the challenging estimation of body and camera pose. To this end, we fine-tune a foundational vision-language model (VLM) on such photos, encoding the appearance, identity, garments, hairstyles, and accessories of a person into (separate) learned tokens and instilling these cues into the VLM. In effect, we exploit the learned tokens as""puzzle pieces""from which we assemble a faithful, personalized 3D avatar. Importantly, we can customize avatars by simply inter-changing tokens. As a benchmark for this new task, we collect a new dataset, called PuzzleIOI, with 41 subjects in a total of nearly 1K OOTD configurations, in challenging partial photos with paired ground-truth 3D bodies. Evaluation shows that PuzzleAvatar not only has high reconstruction accuracy, outperforming TeCH and MVDreamBooth, but also a unique scalability to album photos, and strong robustness. Our code and data are publicly available for research purpose at https://puzzleavatar.is.tue.mpg.de/"
"RILe: Reinforced Imitation Learning.","2024","CoRR","Mert Albaba, Sammy Joe Christen, Christoph Gebhardt, Thomas Langarek, Michael J. Black, Otmar Hilliges","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2406-08472","Reinforcement Learning has achieved significant success in generating complex behavior but often requires extensive reward function engineering. Adversarial variants of Imitation Learning and Inverse Reinforcement Learning offer an alternative by learning policies from expert demonstrations via a discriminator. However, these methods struggle in complex tasks where randomly sampling expert-like behaviors is challenging. This limitation stems from their reliance on policy-agnostic discriminators, which provide insufficient guidance for agent improvement, especially as task complexity increases and expert behavior becomes more distinct. We introduce RILe (Reinforced Imitation Learning environment), a novel trainer-student system that learns a dynamic reward function based on the student's performance and alignment with expert demonstrations. In RILe, the student learns an action policy while the trainer, using reinforcement learning, continuously updates itself via the discriminator's feedback to optimize the alignment between the student and the expert. The trainer optimizes for long-term cumulative rewards from the discriminator, enabling it to provide nuanced feedback that accounts for the complexity of the task and the student's current capabilities. This approach allows for greater exploration of agent actions by providing graduated feedback rather than binary expert/non-expert classifications. By reducing dependence on policy-agnostic discriminators, RILe enables better performance in complex settings where traditional methods falter, outperforming existing methods by 2x in complex simulated robot-locomotion tasks."
"Re-Thinking Inverse Graphics With Large Language Models.","2024","CoRR","Peter Kulits, Haiwen Feng, Weiyang Liu, Victoria Fernández Abrevaya, Michael J. Black","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2404-15228","Inverse graphics -- the task of inverting an image into physical variables that, when rendered, enable reproduction of the observed scene -- is a fundamental challenge in computer vision and graphics. Successfully disentangling an image into its constituent elements, such as the shape, color, and material properties of the objects of the 3D scene that produced it, requires a comprehensive understanding of the environment. This complexity limits the ability of existing carefully engineered approaches to generalize across domains. Inspired by the zero-shot ability of large language models (LLMs) to generalize to novel contexts, we investigate the possibility of leveraging the broad world knowledge encoded in such models to solve inverse-graphics problems. To this end, we propose the Inverse-Graphics Large Language Model (IG-LLM), an inverse-graphics framework centered around an LLM, that autoregressively decodes a visual embedding into a structured, compositional 3D-scene representation. We incorporate a frozen pre-trained visual encoder and a continuous numeric head to enable end-to-end training. Through our investigation, we demonstrate the potential of LLMs to facilitate inverse graphics through next-token prediction, without the application of image-space supervision. Our analysis enables new possibilities for precise spatial reasoning about images that exploit the visual knowledge of LLMs. We release our code and data at https://ig-llm.is.tue.mpg.de/ to ensure the reproducibility of our investigation and to facilitate future research."
"SCULPT: Shape-Conditioned Unpaired Learning of Pose-dependent Clothed and Textured Human Meshes.","2024","CVPR","Soubhik Sanyal, Partha Ghosh, Jinlong Yang, Michael J. Black, Justus Thies, Timo Bolkart","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/SanyalGYBTB24",""
"Text-Conditioned Generative Model of 3D Strand-Based Human Hairstyles.","2024","CVPR","Vanessa Sklyarova, Egor Zakharov, Otmar Hilliges, Michael J. Black, Justus Thies","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/SklyarovaZHBT24","We present HAAR, a new strand-based generative model for 3D human hairstyles. Specifically, based on textual inputs, HAAR produces 3D hairstyles that could be used as production-level assets in modern computer graphics engines. Current AI-based generative models take advantage of powerful 2D priors to reconstruct 3D content in the form of point clouds, meshes, or volumetric functions. However, by using the 2D priors, they are intrinsically limited to only recovering the visual parts. Highly occluded hair structures can not be reconstructed with those methods, and they only model the “outer shell”, which is not ready to be used in physics-based rendering or simulation pipelines. In contrast, we propose a first text-guided generative method that uses 3D hair strands as an underlying representation. Leveraging 2D visual question-answering (VQA) systems, we automatically annotate synthetic hair models that are generated from a small set of artist-created hairstyles. This allows us to train a latent diffusion model that operates in a common hairstyle UV space. In qualitative and quantitative studies, we demonstrate the capabilities of the proposed model and compare it to existing hairstyle generation approaches. For results, please refer to our project page."
"VAREN: Very Accurate and Realistic Equine Network.","2024","CVPR","Silvia Zuffi, Ylva Mellbin, Ci Li, Markus Höschle, Hedvig Kjellström, Senya Polikovsky, Elin Hernlund, Michael J. Black","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/ZuffiMLHKPHB24","Data-driven three-dimensional parametric shape mod-els of the human body have gained enormous popularity both for the analysis of visual data and for the generation of synthetic humans. Following a similar approach for animals does not scale to the multitude of existing ani-mal species, not to mention the difficulty of accessing sub-jects to scan in 3D. However, we argue that for domestic species of great importance, like the horse, it is a highly valuable investment to put effort into gathering a large dataset of real 3D scans, and learn a realistic 3D articu-lated shape model. We introduce VAREN, a novel 3D ar-ticulated parametric shape model learned from 3D scans of many real horses. VAREN bridges synthesis and analysis tasks, as the generated model instances have unprecedented realism, while being able to represent horses of different sizes and shapes. Differently from previous body models, VAREN has two resolutions, an anatomical skeleton, and interpretable, learned pose-dependent deformations, which are related to the body muscles. We show with experiments that this formulation has superior performance with respect to previous strategies for modeling pose-dependent deformations in the human body case, while also being more compact and allowing an analysis of the relationship be-tween articulation and muscle deformation during articu-lated motion. The VAREN model and data are available at https://varen.is.tue.mpg.de."
"WHAM: Reconstructing World-Grounded Humans with Accurate 3D Motion.","2024","CVPR","Soyong Shin, Juyong Kim 0002, Eni Halilaj, Michael J. Black","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/Shin0HB24","The estimation of 3D human motion from video has progressed rapidly but current methods still have several key limitations. First, most methods estimate the human in camera coordinates. Second, prior work on estimating humans in global coordinates often assumes a flat ground plane and produces foot sliding. Third, the most accurate methods rely on computationally expensive optimization pipelines, limiting their use to offline applications. Finally, existing video-based methods are surprisingly less accurate than singleframe methods. We address these limitations with WHAM (World-grounded Humans with Accurate Motion), which accurately and efficiently reconstructs 3D human motion in a global coordinate system from video. WHAM learns to lift 2D keypoint sequences to 3D using motion capture data and fuses this with video features, integrating motion context and visual information. WHAM exploits camera angular velocity estimated from a SLAM method together with human motion to estimate the body's global trajectory. We combine this with a contact-aware trajectory refinement method that lets WHAM capture human motion in diverse conditions, such as climbing stairs. WHAM outperforms all existing 3D human motion recovery methods across multiple in-the-wild benchmarks. Code is available for research purposes at http://wham.is.tue.mpg.del."
"Gaussian Garments: Reconstructing Simulation-Ready Clothing with Photorealistic Appearance from Multi-View Video.","2024","CoRR","Boxiang Rong, Artur Grigorev 0002, Wenbo Wang 0007, Michael J. Black, Bernhard Thomaszewski, Christina Tsalicoglou, Otmar Hilliges","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2409-08189",""
"HUMOS: Human Motion Model Conditioned on Body Shape.","2024","CoRR","Shashank Tripathi, Omid Taheri, Christoph Lassner, Michael J. Black, Daniel Holden, Carsten Stoll","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2409-03944","Generating realistic human motion is essential for many computer vision and graphics applications. The wide variety of human body shapes and sizes greatly impacts how people move. However, most existing motion models ignore these differences, relying on a standardized, average body. This leads to uniform motion across different body types, where movements don't match their physical characteristics, limiting diversity. To solve this, we introduce a new approach to develop a generative motion model based on body shape. We show that it's possible to train this model using unpaired data by applying cycle consistency, intuitive physics, and stability constraints, which capture the relationship between identity and movement. The resulting model generates diverse, physically plausible, and dynamically stable human motions that are both quantitatively and qualitatively more realistic than current state-of-the-art methods. More details are available on our project page https://CarstenEpic.github.io/humos/."
"Human Hair Reconstruction with Strand-Aligned 3D Gaussians.","2024","CoRR","Egor Zakharov, Vanessa Sklyarova, Michael Julian Black, Gi-Joon Nam, Justus Thies, Otmar Hilliges","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2409-14778","We introduce a new hair modeling method that uses a dual representation of classical hair strands and 3D Gaussians to produce accurate and realistic strand-based reconstructions from multi-view data. In contrast to recent approaches that leverage unstructured Gaussians to model human avatars, our method reconstructs the hair using 3D polylines, or strands. This fundamental difference allows the use of the resulting hairstyles out-of-the-box in modern computer graphics engines for editing, rendering, and simulation. Our 3D lifting method relies on unstructured Gaussians to generate multi-view ground truth data to supervise the fitting of hair strands. The hairstyle itself is represented in the form of the so-called strand-aligned 3D Gaussians. This representation allows us to combine strand-based hair priors, which are essential for realistic modeling of the inner structure of hairstyles, with the differentiable rendering capabilities of 3D Gaussian Splatting. Our method, named Gaussian Haircut, is evaluated on synthetic and real scenes and demonstrates state-of-the-art performance in the task of strand-based hair reconstruction."
"3D Human Pose Estimation via Intuitive Physics.","2023","CVPR","Shashank Tripathi, Lea Müller, Chun-Hao P. Huang, Omid Taheri, Michael J. Black, Dimitrios Tzionas","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/TripathiMHTBT23","Estimating 3D humans from images often produces implausible bodies that lean, float, or penetrate the floor. Such methods ignore the fact that bodies are typically supported by the scene. A physics engine can be used to enforce physical plausibility, but these are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. In contrast, we exploit novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Inspired by biomechanics, we infer the pressure heatmap on the body, the Center of Pressure (CoP) from the heatmap, and the SMPL body's Center of Mass (CoM). With these, we develop IPMAN, to estimate a 3D body from a color image in a “stable” configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, differentiable, and can be integrated into existing optimization and regression methods. We evaluate IPMAN on standard datasets and MoYo, a new dataset with synchronized multi-view images, ground-truth 3D bodies with complex poses, body-floor contact, CoM and pressure. IPMAN produces more plausible results than the state of the art, improving accuracy for static poses, while not hurting dynamic ones. Code and data are available for research at https://ipman.is.tue.mpg.de."
"ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation.","2023","CVPR","Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed Kocabas, Manuel Kaufmann, Michael J. Black, Otmar Hilliges","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/FanTTKKBH23",""
"BEDLAM: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion.","2023","CVPR","Michael J. Black, Priyanka Patel, Joachim Tesch, Jinlong Yang","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/BlackPTY23","We show, for the first time, that neural networks trained only on synthetic data achieve state-of-the-art accuracy on the problem of 3D human pose and shape (HPS) estimation from real images. Previous synthetic datasets have been small, unrealistic, or lacked realistic clothing. Achieving sufficient realism is non-trivial and we show how to do this for full bodies in motion. Specifically, our BEDLAM dataset contains monocular RGB videos with ground-truth 3D bodies in SMPL-X format. It includes a diversity of body shapes, motions, skin tones, hair, and clothing. The clothing is realistically simulated on the moving bodies using commercial clothing physics simulation. We render varying numbers of people in realistic scenes with varied lighting and camera motions. We then train various HPS regressors using BEDLAM and achieve state-of-the-art accuracy on real-image benchmarks despite training with synthetic data. We use BEDLAM to gain insights into what model design choices are important for accuracy. With good synthetic training data, we find that a basic method like HMR approaches the accuracy of the current SOTA method (CLIFF). BEDLAM is useful for a variety of tasks and all images, ground truth bodies, 3D clothing, support code, and more are available for research purposes. Additionally, we provide detailed information about our synthetic data generation pipeline, enabling others to generate their own datasets. See the project page: https://bedlam.is.tue.mpg.de/."
"BITE: Beyond Priors for Improved Three-D Dog Pose Estimation.","2023","CVPR","Nadine Rüegg, Shashank Tripathi, Konrad Schindler, Michael J. Black, Silvia Zuffi","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/RueggTSBZ23","We address the problem of inferring the 3D shape and pose of dogs from images. Given the lack of 3D training data, this problem is challenging, and the best methods lag behind those designed to estimate human shape and pose. To make progress, we attack the problem from multiple sides at once. First, we need a good 3D shape prior, like those available for humans. To that end, we learn a dog-specific 3D parametric model, called D-SMAL. Second, existing methods focus on dogs in standing poses because when they sit or lie down, their legs are self occluded and their bodies deform. Without access to a good pose prior or 3D data, we need an alternative approach. To that end, we exploit contact with the ground as a form of side information. We consider an existing large dataset of dog images and label any 3D contact of the dog with the ground. We exploit body-ground contact in estimating dog pose and find that it significantly improves results. Third, we develop a novel neural network architecture to infer and exploit this contact information. Fourth, to make progress, we have to be able to measure it. Current evaluation metrics are based on 2D features like keypoints and silhouettes, which do not directly correlate with 3D errors. To address this, we create a synthetic dataset containing rendered images of scanned 3D dogs. With these advances, our method recovers significantly better dog shape and pose than the state of the art, and we evaluate this improvement in 3D. Our code, model and test dataset are publicly available for research purposes at https://bite.is.tue.mpg.de/."
"Detecting Human-Object Contact in Images.","2023","CVPR","Yixin Chen, Sai Kumar Dwivedi, Michael J. Black, Dimitrios Tzionas","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/ChenDBT23","Humans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (“Human-Object conTact”), a new dataset of human-object contacts in images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons around the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task, that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability. Our HOT data and model are available for research at https://hot.is.tue.mpg.de."
"ECON: Explicit Clothed humans Optimized via Normal integration.","2023","CVPR","Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, Michael J. Black","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/XiuYCTB23","The combination of deep learning, artist-curated scans, and Implicit Functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry, but produce disembodied limbs or degenerate shapes for novel poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit representation and explicit body regularization. To this end, we make two key observations: (1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a “canvas” for stitching together detailed surface patches. Based on these, our method, ECON, has three main steps: (1) It infers detailed 2D normal maps for the front and back side of a clothed person. (2) From these, it recovers 2.5D front and back surfaces, called d-BiNI, that are equally detailed, yet incomplete, and registers these w.r.t. each other with the help of a SMPL-X body mesh recovered from the image. (3) It “inpaints” the missing geometry between d-BiNI surfaces. If the face and hands are noisy, they can optionally be replaced with the ones of SMPL-X. As a result, ECON infers high-fidelity 3D humans even in loose clothes and challenging poses. This goes beyond previous methods, according to the quantitative evaluation on the CAPE and Renderpeople datasets. Perceptual studies also show that ECON's perceived realism is better by a large margin. Code and models are available for research purposes at econ.is.tue.mpg.de"
"FLARE: Fast Learning of Animatable and Relightable Mesh Avatars.","2023","ACM Trans. Graph.","Shrisha Bharadwaj, Yufeng Zheng, Otmar Hilliges, Michael J. Black, Victoria Fernández Abrevaya","Journal Articles","https://dblp.org/rec/journals/tog/BharadwajZHBA23","Our goal is to efficiently learn personalized animatable 3D head avatars from videos that are geometrically accurate, realistic, relightable, and compatible with current rendering systems. While 3D meshes enable efficient processing and are highly portable, they lack realism in terms of shape and appearance. Neural representations, on the other hand, are realistic but lack compatibility and are slow to train and render. Our key insight is that it is possible to efficiently learn high-fidelity 3D mesh representations via differentiable rendering by exploiting highly-optimized methods from traditional computer graphics and approximating some of the components with neural networks. To that end, we introduce FLARE, a technique that enables the creation of animatable and relightable mesh avatars from a single monocular video. First, we learn a canonical geometry using a mesh representation, enabling efficient differentiable rasterization and straightforward animation via learned blendshapes and linear blend skinning weights. Second, we follow physically-based rendering and factor observed colors into intrinsic albedo, roughness, and a neural representation of the illumination, allowing the learned avatars to be relit in novel scenes. Since our input videos are captured on a single device with a narrow field of view, modeling the surrounding environment light is non-trivial. Based on the split-sum approximation for modeling specular reflections, we address this by approximating the prefiltered environment map with a multi-layer perceptron (MLP) modulated by the surface roughness, eliminating the need to explicitly model the light. We demonstrate that our mesh-based avatar formulation, combined with learned deformation, material, and lighting MLPs, produces avatars with high-quality geometry and appearance, while also being efficient to train and render compared to existing approaches."
"Fast-SNARF: A Fast Deformer for Articulated Neural Fields.","2023","IEEE Trans. Pattern Anal. Mach. Intell.","Xu Chen 0025, Tianjian Jiang, Jie Song 0006, Max Rietmann, Andreas Geiger 0001, Michael J. Black, Otmar Hilliges","Journal Articles","https://dblp.org/rec/journals/pami/ChenJSRGBH23","Neural fields have revolutionized the area of 3D reconstruction and novel view synthesis of <italic>rigid</italic> scenes. A key challenge in making such methods applicable to <italic>articulated</italic> objects, such as the human body, is to model the deformation of 3D locations between the rest pose (a canonical space) and the deformed space. We propose a new articulation module for neural fields, Fast-SNARF, which finds accurate correspondences between canonical space and posed space via iterative root finding. Fast-SNARF is a drop-in replacement in functionality to our previous work, SNARF, while significantly improving its computational efficiency. We contribute several algorithmic and implementation improvements over SNARF, yielding a speed-up of <inline-formula><tex-math notation=""LaTeX"">$150\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>150</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""chen-ieq1-3271569.gif""/></alternatives></inline-formula>. These improvements include voxel-based correspondence search, pre-computing the linear blend skinning function, and an efficient software implementation with CUDA kernels. Fast-SNARF enables efficient and simultaneous optimization of shape and skinning weights given deformed observations without correspondences (e.g. 3D meshes). Because learning of deformation maps is a crucial component in many 3D human avatar methods and since Fast-SNARF provides a computationally efficient solution, we believe that this work represents a significant step towards the practical creation of 3D virtual humans."
"From Skin to Skeleton: Towards Biomechanically Accurate 3D Digital Humans.","2023","ACM Trans. Graph.","Marilyn Keller, Keenon Werling, Soyong Shin, Scott L. Delp, Sergi Pujades, C. Karen Liu, Michael J. Black","Journal Articles","https://dblp.org/rec/journals/tog/KellerWSDPLB23","Great progress has been made in estimating 3D human pose and shape from images and video by training neural networks to directly regress the parameters of parametric human models like SMPL. However, existing body models have simplified kinematic structures that do not correspond to the true joint locations and articulations in the human skeletal system, limiting their potential use in biomechanics. On the other hand, methods for estimating biomechanically accurate skeletal motion typically rely on complex motion capture systems and expensive optimization methods. What is needed is a parametric 3D human model with a biomechanically accurate skeletal structure that can be easily posed. To that end, we develop SKEL, which re-rigs the SMPL body model with a biomechanics skeleton. To enable this, we need training data of skeletons inside SMPL meshes in diverse poses. We build such a dataset by optimizing biomechanically accurate skeletons inside SMPL meshes from AMASS sequences. We then learn a regressor from SMPL mesh vertices to the optimized joint locations and bone rotations. Finally, we re-parametrize the SMPL mesh with the new kinematic parameters. The resulting SKEL model is animatable like SMPL but with fewer, and biomechanically-realistic, degrees of freedom. We show that SKEL has more biomechanically accurate joint locations than SMPL, and the bones fit inside the body surface better than previous methods. By fitting SKEL to SMPL meshes we are able to ""upgrade"" existing human pose and shape datasets to include biomechanical parameters. SKEL provides a new tool to enable biomechanics in the wild, while also providing vision and graphics researchers with a better constrained and more realistic model of human articulation. The model, code, and data are available for research at https://skel.is.tue.mpg.de."
"Generating Holistic 3D Human Motion from Speech.","2023","CVPR","Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, Michael J. Black","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/YiLLCWBTB23","This work addresses the problem of generating 3D holistic body motions from human speech. Given a speech recording, we synthesize sequences of 3D body poses, hand gestures, and facial expressions that are realistic and diverse. To achieve this, we first build a high-quality dataset of 3D holistic body meshes with synchronous speech. We then define a novel speech-to-motion generation framework in which the face, body, and hands are modeled separately. The separated modeling stems from the fact that face articulation strongly correlates with human speech, while body poses and hand gestures are less correlated. Specifically, we employ an autoencoder for face motions, and a compositional vector-quantized variational autoencoder (VQ- VAE) for the body and hand motions. The compositional VQ-VAE is key to generating diverse results. Additionally, we propose a cross-conditional autoregressive model that generates body poses and hand gestures, leading to coherent and realistic motions. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively. Our dataset and code are released for research purposes at https://talkshow.is.tue.mpg.de/."
"HOOD: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics.","2023","CVPR","Artur Grigorev 0002, Michael J. Black, Otmar Hilliges","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/GrigorevBH23","We propose a method that leverages graph neural networks, multi-level message passing, and unsupervised training to enable efficient prediction of realistic clothing dynamics. Whereas existing methods based on linear blend skinning must be trained for specific garments, our method, called HOOD, is agnostic to body shape and applies to tight-fitting garments as well as loose, free-flowing clothing. Furthermore, HOOD handles changes in topology (e.g., garments with buttons or zippers) and material properties at inference time. As one key contribution, we propose a hierarchical message-passing scheme that efficiently propagates stiff stretching modes while preserving local detail. We empirically show that HOOD outperforms strong baselines quantitatively and that its results are perceived as more realistic than state-of-the-art methods."
"Instant Multi-View Head Capture through Learnable Registration.","2023","CVPR","Timo Bolkart, Tianye Li, Michael J. Black","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/BolkartLB23","Existing methods for capturing datasets of 3D heads in dense semantic correspondence are slow and commonly address the problem in two separate steps; multi-view stereo (MVS) reconstruction followed by non-rigid registration. To simplify this process, we introduce TEMPEH (Towards Estimation of 3D Meshes from Performances of Expressive Heads) to directly infer 3D heads in dense correspondence from calibrated multi-view images. Registering datasets of 3D scans typically requires manual parameter tuning to find the right balance between accurately fitting the scans' surfaces and being robust to scanning noise and outliers. Instead, we propose to jointly register a 3D head dataset while training TEMPEH. Specifically, during training, we minimize a geometric loss commonly used for surface registration, effectively leveraging TEMPEH as a regularizer. Our multi-view head inference builds on a volumetric feature representation that samples and fuses features from each view using camera calibration information. To account for partial occlusions and a large capture volume that enables head movements, we use view-and surface-aware feature fusion, and a spatial transformer-based head localization module, respectively. We use raw MVS scans as supervision during training, but, once trained, TEMPEH directly predicts 3D heads in dense correspondence without requiring scans. Predicting one head takes about 0.3 seconds with a median reconstruction error of 0.26 mm, 64% lower than the current state-of-the-art. This enables the efficient capture of large datasets containing multiple people and diverse facial motions. Code, model, and data are publicly available at https://tempeh.is.tue.mpg.de."
"MIME: Human-Aware 3D Scene Generation.","2023","CVPR","Hongwei Yi, Chun-Hao P. Huang, Shashank Tripathi, Lea Hering, Justus Thies, Michael J. Black","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/YiHTHTB23",""
"PointAvatar: Deformable Point-Based Head Avatars from Videos.","2023","CVPR","Yufeng Zheng, Wang Yifan 0011, Gordon Wetzstein, Michael J. Black, Otmar Hilliges","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/ZhengYWBH23","The ability to create realistic animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting and albedo, limiting the ability to re-render the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-the-art quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods."
"SmartMocap: Joint Estimation of Human and Camera Motion Using Uncalibrated RGB Cameras.","2023","IEEE Robotics Autom. Lett.","Nitin Saini, Chun-Hao P. Huang, Michael J. Black, Aamir Ahmad","Journal Articles","https://dblp.org/rec/journals/ral/SainiHBA23","Markerless human motion capture (mocap) from multiple RGB cameras is a widely studied problem. Existing methods either need calibrated cameras or calibrate them relative to a static camera, which acts as the reference frame for the mocap system. The calibration step has to be done a priori for every capture session, which is a tedious process, and re-calibration is required whenever cameras are intentionally or accidentally moved. In this letter, we propose a mocap method which uses multiple static and moving extrinsically uncalibrated RGB cameras. The key components of our method are as follows. First, since the cameras and the subject can move freely, we select the ground plane as a common reference to represent both the body and the camera motions unlike existing methods which represent bodies in the camera coordinate system. Second, we learn a probability distribution of short human motion sequences ($\sim$1 sec) relative to the ground plane and leverage it to disambiguate between the camera and human motion. Third, we use this distribution as a motion prior in a novel multi-stage optimization approach to fit the SMPL human body model and the camera poses to the human body keypoints on the images. Finally, we show that our method can work on a variety of datasets ranging from aerial cameras to smartphones. It also gives more accurate results compared to the state-of-the-art on the task of monocular human mocap with a static camera."
"TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D Environments.","2023","CVPR","Yu Sun 0030, Qian Bao, Wu Liu, Tao Mei 0001, Michael J. Black","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/SunBLMB23",""
"Viewpoint-Driven Formation Control of Airships for Cooperative Target Tracking.","2023","IEEE Robotics Autom. Lett.","Eric Price 0002, Michael J. Black, Aamir Ahmad","Journal Articles","https://dblp.org/rec/journals/ral/PriceBA23",""
"AG3D: Learning to Generate 3D Avatars from 2D Image Collections.","2023","ICCV","Zijian Dong, Xu Chen 0025, Jinlong Yang, Michael J. Black, Otmar Hilliges, Andreas Geiger 0001","Conference and Workshop Papers","https://dblp.org/rec/conf/iccv/Dong0YBH023",""
"Adversarial Likelihood Estimation with One-way Flows.","2023","CoRR","Omri Ben-Dov, Pravir Singh Gupta, Victoria Fernández Abrevaya, Michael J. Black, Partha Ghosh","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2307-09882","Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; and 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way flow network, that is less constrained in terms of architecture, as it does not require a tractable inverse function. Our experimental results show that our method converges faster, produces comparable sample quality to GANs with similar architecture, successfully avoids over-fitting to commonly used datasets and produces smooth low-dimensional latent representations of the training data."
"DECO: Dense Estimation of 3D Human-Scene Contact In The Wild.","2023","ICCV","Shashank Tripathi, Agniv Chatterjee, Jean-Claude Passy, Hongwei Yi, Dimitrios Tzionas, Michael J. Black","Conference and Workshop Papers","https://dblp.org/rec/conf/iccv/TripathiCPYTB23","Understanding how humans use physical contact to interact with the world is key to enabling human-centric artificial intelligence. While inferring 3D contact is crucial for modeling realistic and physically-plausible human-object interactions, existing methods either focus on 2D, consider body joints rather than the surface, use coarse 3D body regions, or do not generalize to in-the-wild images. In contrast, we focus on inferring dense, 3D contact between the full body surface and objects in arbitrary images. To achieve this, we first collect DAMON, a new dataset containing dense vertex-level contact annotations paired with RGB images containing complex human-object and human-scene contact. Second, we train DECO, a novel 3D contact detector that uses both body-part-driven and scene-context-driven attention to estimate vertex-level contact on the SMPL body. DECO builds on the insight that human observers recognize contact by reasoning about the contacting body parts, their proximity to scene objects, and the surrounding scene context. We perform extensive evaluations of our detector on DAMON as well as on the RICH and BEHAVE datasets. We significantly outperform existing SOTA methods across all benchmarks. We also show qualitatively that DECO generalizes well to diverse and challenging real-world human interactions in natural images. The code, data, and models are available at https://deco.is.tue.mpg.de."
"Emotional Speech-Driven Animation with Content-Emotion Disentanglement.","2023","SIGGRAPH Asia","Radek Danecek, Kiran Chhatre, Shashank Tripathi, Yandong Wen, Michael J. Black, Timo Bolkart","Conference and Workshop Papers","https://dblp.org/rec/conf/siggrapha/DanecekCTWBB23",""
"GRIP: Generating Interaction Poses Using Latent Consistency and Spatial Cues.","2023","CoRR","Omid Taheri, Yi Zhou 0023, Dimitrios Tzionas, Yang Zhou, Duygu Ceylan, Sören Pirk, Michael J. Black","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2308-11617","Hands are dexterous and highly versatile manipulators that are central to how humans interact with objects and their environment. Consequently, modeling realistic hand-object interactions, including the subtle motion of individual fingers, is critical for applications in computer graphics, computer vision, and mixed reality. Prior work on capturing and modeling humans interacting with objects in 3D focuses on the body and object motion, often ignoring hand pose. In contrast, we introduce GRIP, a learning-based method that takes, as input, the 3D motion of the body and the object, and synthesizes realistic motion for both hands before, during, and after object interaction. As a preliminary step before synthesizing the hand motion, we first use a network, ANet, to denoise the arm motion. Then, we leverage the spatio-temporal relationship between the body and the object to extract novel temporal interaction cues, and use them in a two-stage inference pipeline to generate the hand motion. In the first stage, we introduce a new approach to encourage motion temporal consistency in the latent space (LTC) and generate consistent interaction motions. In the second stage, GRIP generates refined hand poses to avoid hand-object penetrations. Given sequences of noisy body and object motion, GRIP “upgrades” them to include hand-object interaction. Quantitative experiments and perceptual studies demonstrate that GRIP outperforms baseline methods and generalizes to unseen objects and motions from different motion-capture datasets. Our models and code are available for research purposes at https://grip.is.tue.mpg.de."
"Generalizing Neural Human Fitting to Unseen Poses With Articulated SE(3) Equivariance.","2023","ICCV","Haiwen Feng, Peter Kulits, Shichen Liu, Michael J. Black, Victoria Fernández Abrevaya","Conference and Workshop Papers","https://dblp.org/rec/conf/iccv/FengKLBA23",""
"Generative Proxemics: A Prior for 3D Social Interaction from Images.","2023","CoRR","Lea Müller, Vickie Ye, Georgios Pavlakos, Michael J. Black, Angjoo Kanazawa","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2306-09337","Social interaction is a fundamental aspect of human behavior and communication. The way individuals position themselves in relation to others, also known as proxemics, conveys social cues and affects the dynamics of social interaction. Reconstructing such interaction from images presents challenges because of mutual occlusion and the limited availability of large training datasets. To address this, we present a novel approach that learns a prior over the 3D proxemics two people in close social interaction and demonstrate its use for single-view 3D reconstruction. We start by creating 3D training data of interacting people using image datasets with contact annotations. We then model the proxemics using a novel denoising diffusion model called BUDDI that learns the joint distribution over the poses of two people in close social interaction. Sampling from our generative proxemics model produces realistic 3D human interactions, which we validate through a perceptual study. We use BUDDI in reconstructing two people in close proximity from an image without any contact annotation via an optimization approach that uses the diffusion model as a prior. Our approach recovers accurate 3D social interactions from noisy initial estimates, outperforming state-of-the-art methods. Our code, data, and model are available at: muelea. github.io/buddi."
"Ghost on the Shell: An Expressive Representation of General 3D Shapes.","2023","CoRR","Zhen Liu 0019, Yao Feng, Yuliang Xiu, Weiyang Liu, Liam Paull, Michael J. Black, Bernhard Schölkopf","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2310-15168",""
"Learning Disentangled Avatars with Hybrid 3D Representations.","2023","CoRR","Yao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc Pollefeys, Michael J. Black","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2309-06441","Tremendous efforts have been made to learn animatable and photorealistic human avatars. Towards this end, both explicit and implicit 3D representations are heavily studied for a holistic modeling and capture of the whole human (e.g., body, clothing, face and hair), but neither representation is an optimal choice in terms of representation efficacy since different parts of the human avatar have different modeling desiderata. For example, meshes are generally not suitable for modeling clothing and hair. Motivated by this, we present Disentangled Avatars~(DELTA), which models humans with hybrid explicit-implicit 3D representations. DELTA takes a monocular RGB video as input, and produces a human avatar with separate body and clothing/hair layers. Specifically, we demonstrate two important applications for DELTA. For the first one, we consider the disentanglement of the human body and clothing and in the second, we disentangle the face and hair. To do so, DELTA represents the body or face with an explicit mesh-based parametric 3D model and the clothing or hair with an implicit neural radiance field. To make this possible, we design an end-to-end differentiable renderer that integrates meshes into volumetric rendering, enabling DELTA to learn directly from monocular videos without any 3D supervision. Finally, we show that how these two applications can be easily combined to model full-body avatars, such that the hair, face, body and clothing can be fully disentangled yet jointly rendered. Such a disentanglement enables hair and clothing transfer to arbitrary body shapes. We empirically validate the effectiveness of DELTA's disentanglement by demonstrating its promising performance on disentangled reconstruction, virtual clothing try-on and hairstyle transfer. To facilitate future research, we also release an open-sourced pipeline for the study of hybrid human avatar modeling."
"MeshDiffusion: Score-based Generative 3D Mesh Modeling.","2023","ICLR","Zhen Liu 0019, Yao Feng, Michael J. Black, Derek Nowrouzezahrai, Liam Paull, Weiyang Liu","Conference and Workshop Papers","https://dblp.org/rec/conf/iclr/LiuFBNPL23","We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectiveness of our model on multiple generative tasks."
"Optimizing the 3D Plate Shape for Proximal Humerus Fractures.","2023","MICCAI","Marilyn Keller, Marcell Krall, James Smith, Hans Clement, Alexander M. Kerner, Andreas Gradischar, Ute Schäfer, Michael J. Black, Annelie Weinberg, Sergi Pujades","Conference and Workshop Papers","https://dblp.org/rec/conf/miccai/KellerKSCKGSBWP23",""
"PACE: Human and Camera Motion Estimation from in-the-wild Videos.","2023","CoRR","Muhammed Kocabas, Ye Yuan 0007, Pavlo Molchanov 0001, Yunrong Guo, Michael J. Black, Otmar Hilliges, Jan Kautz, Umar Iqbal 0001","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2310-13768","We present a method to estimate human motion in a global scene from moving cameras. This is a highly challenging task due to the coupling of human and camera motions in the video. To address this problem, we propose a joint optimization framework that disentangles human and camera motions using both foreground human motion priors and background scene features. Unlike existing methods that use SLAM as initialization, we propose to tightly integrate SLAM and human motion priors in an optimization that is inspired by bundle adjustment. Specifically, we optimize human and camera motions to match both the observed human pose and scene features. This design combines the strengths of SLAM and motion priors, which leads to significant improvements in human and camera motion estimation. We additionally introduce a motion prior that is suitable for batch optimization, making our approach significantly more efficient than existing approaches. Finally, we propose a novel synthetic dataset that enables evaluating camera motion in addition to human motion from dynamic videos. Experiments on the synthetic and real-world RICH datasets demonstrate that our approach substantially outperforms prior art in recovering both human and camera motions."
"POCO: 3D Pose and Shape Estimation with Confidence.","2023","CoRR","Sai Kumar Dwivedi, Cordelia Schmid, Hongwei Yi, Michael J. Black, Dimitrios Tzionas","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2308-12965","The regression of 3D Human Pose and Shape (HPS) from an image is becoming increasingly accurate. This makes the results useful for downstream tasks like human action recognition or 3D graphics. Yet, no regressor is perfect, and accuracy can be affected by ambiguous image evidence or by poses and appearance that are unseen during training. Most current HPS regressors, however, do not report the confidence of their outputs, meaning that downstream tasks cannot differentiate accurate estimates from inaccurate ones. To address this, we develop POCO, a novel framework for training HPS regressors to estimate not only a 3D human body, but also their confidence, in a single feed-forward pass. Specifically, POCO estimates both the 3D body pose and a per-sample variance. The key idea is to introduce a Dual Conditioning Strategy (DCS) for regressing uncertainty that is highly correlated to pose reconstruction quality. The POCO framework can be applied to any HPS regressor and here we evaluate it by modifying HMR, PARE, and CLIFF. In all cases, training the network to reason about uncertainty helps it learn to more accurately estimate 3D pose. While this was not our goal, the improvement is modest but consistent. Our main motivation is to provide uncertainty estimates for downstream tasks; we demonstrate this in two ways: (1) We use the confidence estimates to bootstrap HPS training. Given unlabeled image data, we take the confident estimates of a POCO-trained regressor as pseudo ground truth. Retraining with this automatically-curated data improves accuracy. (2) We exploit uncertainty in video pose estimation by automatically identifying uncertain frames (e.g. due to occlusion) and “inpainting” these from confident frames. Code and models are available for research at https://poco.is.tue.mpg.de."
"Pairwise Similarity Learning is SimPLE.","2023","ICCV","Yandong Wen, Weiyang Liu, Yao Feng, Bhiksha Raj, Rita Singh, Adrian Weller, Michael J. Black, Bernhard Schölkopf","Conference and Workshop Papers","https://dblp.org/rec/conf/iccv/WenLFRSWBS23","In this paper, we focus on a general yet important learning problem, pairwise similarity learning (PSL). PSL subsumes a wide range of important applications, such as open-set face recognition, speaker verification, image retrieval and person re-identification. The goal of PSL is to learn a pairwise similarity function assigning a higher similarity score to positive pairs (i.e., a pair of samples with the same label) than to negative pairs (i.e., a pair of samples with different label). We start by identifying a key desideratum for PSL, and then discuss how existing methods can achieve this desideratum. We then propose a surprisingly simple proxy-free method, called SimPLE, which requires neither feature/proxy normalization nor angular margin and yet is able to generalize well in open-set recognition. We apply the proposed method to three challenging PSL tasks: open-set face recognition, image retrieval and speaker verification. Comprehensive experimental results on large-scale benchmarks show that our method performs significantly better than current state-of-the-art methods. Our project page is available at simple.is.tue.mpg.de."
"SCULPT: Shape-Conditioned Unpaired Learning of Pose-dependent Clothed and Textured Human Meshes.","2023","CoRR","Soubhik Sanyal, Partha Ghosh, Jinlong Yang, Michael J. Black, Justus Thies, Timo Bolkart","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2308-10638","We present SCULPT, a novel 3D generative model for clothed and textured 3D meshes of humans. Specifically, we devise a deep neural network that learns to represent the geometry and appearance distribution of clothed human bodies. Training such a model is challenging, as datasets of textured 3D meshes for humans are limited in size and accessibility. Our key observation is that there exist medium-sized 3D scan datasets like CAPE, as well as large-scale 2D image datasets of clothed humans and multiple appearances can be mapped to a single geometry. To effectively learn from the two data modalities, we propose an unpaired learning procedure for pose-dependent clothed and textured human meshes. Specifically, we learn a pose-dependent geometry space from 3D scan data. We represent this as per vertex displacements w.r.t. the SMPL model. Next, we train a geometry conditioned texture generator in an unsupervised way using the 2D image data. We use intermediate activations of the learned geometry model to condition our texture generator. To alleviate entanglement between pose and clothing type, and pose and clothing appearance, we condition both the texture and geometry generators with attribute labels such as clothing types for the geometry, and clothing colors for the texture generator. We automatically generated these conditioning labels for the 2D images based on the visual question answering model BLIP and CLIP. We validate our method on the SCULPT dataset, and compare to state-of-the-art 3D generative models for clothed human bodies. Our code and data can be found at https://sculpt.is.tue.mpg.de."
"SINC: Spatial Composition of 3D Human Motions for Simultaneous Action Generation.","2023","ICCV","Nikos Athanasiou, Mathis Petrovich, Michael J. Black, Gül Varol","Conference and Workshop Papers","https://dblp.org/rec/conf/iccv/AthanasiouPBV23","Our goal is to synthesize 3D human motions given textual inputs describing simultaneous actions, for example ‘waving hand’ while ‘walking’ at the same time. We refer to generating such simultaneous movements as performing spatial compositions. In contrast to temporal compositions that seek to transition from one action to another, spatial compositing requires understanding which body parts are involved in which action, to be able to move them simultaneously. Motivated by the observation that the correspondence between actions and body parts is encoded in powerful language models, we extract this knowledge by prompting GPT-3 with text such as ""what are the body parts involved in the action ?"", while also providing the parts list and few-shot examples. Given this action-part mapping, we combine body parts from two motions together and establish the first automated method to spatially compose two actions. However, training data with compositional actions is always limited by the combinatorics. Hence, we further create synthetic data with this approach, and use it to train a new state-of-the-art text-to-motion generation model, called SINC (""Simultaneous actioN Compositions for 3D human motions""). In our experiments, we find that training with such GPT-guided synthetic data improves spatial composition generation over baselines. Our code is publicly available at sinc.is.tue.mpg.de."
"Synthesizing Physical Character-Scene Interactions.","2023","SIGGRAPH","Mohamed Hassan 0003, Yunrong Guo, Tingwu Wang, Michael J. Black, Sanja Fidler, Xue Bin Peng","Conference and Workshop Papers","https://dblp.org/rec/conf/siggraph/HassanGWBFP23","Movement is how people interact with and affect their environment. For realistic character animation, it is necessary to synthesize such interactions between virtual characters and their surroundings. Despite recent progress in character animation using machine learning, most systems focus on controlling an agent’s movements in fairly simple and homogeneous environments, with limited interactions with other objects. Furthermore, many previous approaches that synthesize human-scene interactions require significant manual labeling of the training data. In contrast, we present a system that uses adversarial imitation learning and reinforcement learning to train physically-simulated characters that perform scene interaction tasks in a natural and life-like manner. Our method learns scene interaction behaviors from large unstructured motion datasets, without manual annotation of the motion data. These scene interactions are learned using an adversarial discriminator that evaluates the realism of a motion within the context of a scene. The key novelty involves conditioning both the discriminator and the policy networks on scene context. We demonstrate the effectiveness of our approach through three challenging scene interaction tasks: carrying, sitting, and lying down, which require coordination of a character’s movements in relation to objects in the environment. Our policies learn to seamlessly transition between different behaviors like idling, walking, and sitting. By randomizing the properties of the objects and their placements during training, our method is able to generalize beyond the objects and scenarios depicted in the training dataset, producing natural character-scene interactions for a wide variety of object shapes and placements. The approach takes physics-based character motion generation a step closer to broad applicability. Please see our supplementary video for more results."
"TADA! Text to Animatable Digital Avatars.","2023","CoRR","Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxiang Tang, Yangyi Huang, Justus Thies, Michael J. Black","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2308-10899","We introduce TADA, a simple-yet-effective approach that takes textual descriptions and produces expressive 3D avatars with high-quality geometry and lifelike textures, that can be animated and rendered with traditional graphics pipelines. Existing text-based character generation methods are limited in terms of geometry and texture quality, and cannot be realistically animated due to the misalignment between the geometry and the texture, particularly in the face region. To address these limitations, TADA leverages the synergy of a 2D diffusion model and a parametric body model. Specifically, we derive a high-resolution upsampled version of SMPL-X with a displacement layer and a texture map, and use hierarchical rendering with score distillation sampling (SDS) to create high-quality, detailed, holistic 3D avatars from text. To ensure alignment between the geometry and texture, we render normals and RGB images of the generated character and exploit their latent embeddings during the SDS optimization process. We further drive the character’s face with multiple expressions during optimization, ensuring that its semantics remain consistent with the original SMPL-X model. Both qualitative and quantitative evaluations show that TADA significantly surpasses existing approaches. TADA enables large-scale creation of digital characters ready for animation and rendering, while also enabling text-guided editing. The code is public for research purposes at tada.is.tue.mpg.de"
"TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis.","2023","ICCV","Mathis Petrovich, Michael J. Black, Gül Varol","Conference and Workshop Papers","https://dblp.org/rec/conf/iccv/PetrovichBV23","In this paper, we present TMR, a simple yet effective approach for text to 3D human motion retrieval. While previous work has only treated retrieval as a proxy evaluation metric, we tackle it as a standalone task. Our method extends the state-of-the-art text-to-motion synthesis model TEMOS, and incorporates a contrastive loss to better structure the cross-modal latent space. We show that maintaining the motion generation loss, along with the contrastive training, is crucial to obtain good performance. We introduce a benchmark for evaluation and provide an in-depth analysis by reporting results on several protocols. Our extensive experiments on the KIT-ML and HumanML3D datasets show that TMR outperforms the prior work by a significant margin, for example reducing the median rank from 54 to 19. Finally, we showcase the potential of our approach on moment retrieval. Our code and models are publicly available at https://mathis.petrovich.fr/tmr."
"Text-Guided Generation and Editing of Compositional 3D Avatars.","2023","CoRR","Hao Zhang, Yao Feng, Peter Kulits, Yandong Wen, Justus Thies, Michael J. Black","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2309-07125",""
"A Framework for Improving the Reliability of Black-box Variational Inference.","2024","J. Mach. Learn. Res.","Manushi Welandawe, Michael Riis Andersen, Aki Vehtari, Jonathan H. Huggins","Journal Articles","https://dblp.org/rec/journals/jmlr/WelandaweAVH24","Black-box variational inference (BBVI) now sees widespread use in machine learning and statistics as a fast yet flexible alternative to Markov chain Monte Carlo methods for approximate Bayesian inference. However, stochastic optimization methods for BBVI remain unreliable and require substantial expertise and hand-tuning to apply effectively. In this paper, we propose Robust and Automated Black-box VI (RABVI), a framework for improving the reliability of BBVI optimization. RABVI is based on rigorously justified automation techniques, includes just a small number of intuitive tuning parameters, and detects inaccurate estimates of the optimal variational approximation. RABVI adaptively decreases the learning rate by detecting convergence of the fixed--learning-rate iterates, then estimates the symmetrized Kullback--Leibler (KL) divergence between the current variational approximation and the optimal one. It also employs a novel optimization termination criterion that enables the user to balance desired accuracy against computational cost by comparing (i) the predicted relative decrease in the symmetrized KL divergence if a smaller learning were used and (ii) the predicted computation required to converge with the smaller learning rate. We validate the robustness and accuracy of RABVI through carefully designed simulation studies and on a diverse set of real-world model and data examples."
"AJIS Thinking Differently in Service of The Community.","2024","Australas. J. Inf. Syst.","Michael J. Davern, Stuart Black","Journal Articles","https://dblp.org/rec/journals/ajis/DavernB24",""
"Generative Active Learning for the Search of Small-molecule Protein Binders.","2024","CoRR","Maksym Korablyov, Cheng-Hao Liu, Moksh Jain, Almer M. van der Sloot, Eric Jolicoeur, Edward Ruediger, Andrei Cristian Nica, Emmanuel Bengio, Kostiantyn Lapchevskyi, Daniel St-Cyr, Doris Alexandra Schuetz, Victor Ion Butoi, Jarrid Rector-Brooks, Simon Blackburn, Leo Feng, Hadi Nekoei, Sai Krishna Gottipati, Priyesh Vijayan, Prateek Gupta, Ladislav Rampásek, Sasikanth Avancha, Pierre-Luc Bacon, William L. Hamilton, Brooks Paige, Sanchit Misra, Stanislaw Kamil Jastrzebski, Bharat Kaul, Doina Precup, José Miguel Hernández-Lobato, Marwin H. S. Segler, Michael M. Bronstein, Anne Marinier, Mike Tyers, Yoshua Bengio","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2405-01616",""
"Results from the NASA Tropics Mission After One Year in Orbit.","2024","IGARSS","William J. Blackwell, Andrew Cunningham, Michael DiLiberto, Shawn Donnelly, Chris Kidd, Min-Jeong Kim, Robert Vincent Leslie, Adam B. Milstein, Glenn Perras, Michael L. Pieper, Joelle Prince, Nicholas Zorn","Conference and Workshop Papers","https://dblp.org/rec/conf/igarss/BlackwellCDDKKLMPPPZ24","The four NASA TROPICS Earth Venture (EVI-3) CubeSat constellation satellites were successfully launched into orbit on May 8 and May 26, 2023 (NZST) – two satellites were deployed in each launch. TROPICS is now providing nearly all-weather observations of 3-D temperature and humidity, as well as cloud ice and precipitation horizontal structure, at a median refresh rate of approximately 60 minutes to conduct high-value science investigations of tropical cyclones. TROPICS provides microwave measurements in twelve channels spanning 90-205 GHz over the tropics that can be used to observe the thermodynamics of the troposphere and precipitation structure for storm systems at the mesoscale and synoptic scale over the entire storm lifecycle. Hundreds of high-resolution images of tropical cyclones have been captured thus far by the TROPICS mission, revealing detailed structure of the eyewall and surrounding rain bands. The new 205-GHz channel in particular (together with a traditional channel near 91.65 GHz) is providing new information on the inner storm structure, and, coupled with the relatively frequent revisit and low downlink latency, is informing tropical cyclone analysis at operational centers. In this paper, the radiance and geophysical performance of Pathfinder and the constellation satellites is presented, showing that the mission is on track to meet its baseline requirements."
"A Practical Stereo Depth System for Smart Glasses.","2023","CVPR","Jialiang Wang, Daniel Scharstein, Akash Bapat, Kevin Blackburn-Matzen, Matthew Yu, Jonathan Lehman, Suhib Alsisan, Yanghan Wang, Sam S. Tsai, Jan-Michael Frahm, Zijian He, Peter Vajda, Michael F. Cohen, Matt Uyttendaele","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/WangSBBYLAWTFHV23","We present the design of a productionized end-to-end stereo depth sensing system that does pre-processing, online stereo rectification, and stereo depth estimation with a fallback to monocular depth estimation when rectification is unreliable. The output of our depth sensing system is then used in a novel view generation pipeline to create 3D computational photography effects using point-of-view images captured by smart glasses. All these steps are executed on-device on the stringent compute budget of a mobile phone, and because we expect the users can use a wide range of smartphones, our design needs to be general and cannot be dependent on a particular hardware or ML accelerator such as a smartphone GPU. Although each of these steps is well studied, a description of a practical system is still lacking. For such a system, all these steps need to work in tandem with one another and fallback gracefully on failures within the system or less than ideal input data. We show how we handle unforeseen changes to calibration, e.g., due to heat, robustly support depth estimation in the wild, and still abide by the memory and latency constraints required for a smooth user experience. We show that our trained models are fast, and run in less than 1s on a six-year-old Samsung Galaxy S8 phone's CPU. Our models generalize well to unseen data and achieve good results on Middlebury and in-the-wild images captured from the smart glasses."
"An Alternative Method for Analytical Solutions of Two-Dimensional Black-Scholes-Merton Equation.","2023","J. Appl. Math.","Jun Yu, Michael J. Tomas","Journal Articles","https://dblp.org/rec/journals/jam/YuT23","We present a method of deriving analytical solutions for a two-dimensional Black-Scholes-Merton equation. The method consists of three changes of variables in order to reduce the original partial differential equation (PDE) to a normal form and then solve it. Analytical solutions for two cases of option pricing on the minimum and maximum of two assets are derived using our method and are shown to agree with previously published results. The advantage of our solution procedure is the ability of splitting the original problem into several components in order to demonstrate some solution properties. The solutions of the two cases have a total of five components; each is a particular solution of the PDE itself. Due to the linearity of the two-dimensional Black-Scholes-Merton equation, any linear combination of these components constitutes another solution. Some other possible solutions as well as the solution properties are discussed."
"Black-Box vs. Gray-Box: A Case Study on Learning Table Tennis Ball Trajectory Prediction with Spin and Impacts.","2023","L4DC","Jan Achterhold, Philip Tobuschat, Hao Ma, Dieter Büchler, Michael Muehlebach, Joerg Stueckler","Conference and Workshop Papers","https://dblp.org/rec/conf/l4dc/AchterholdTMBMS23","In this paper, we present a method for table tennis ball trajectory filtering and prediction. Our gray-box approach builds on a physical model. At the same time, we use data to learn parameters of the dynamics model, of an extended Kalman filter, and of a neural model that infers the ball's initial condition. We demonstrate superior prediction performance of our approach over two black-box approaches, which are not supplied with physical prior knowledge. We demonstrate that initializing the spin from parameters of the ball launcher using a neural network drastically improves long-time prediction performance over estimating the spin purely from measured ball positions. An accurate prediction of the ball trajectory is crucial for successful returns. We therefore evaluate the return performance with a pneumatic artificial muscular robot and achieve a return rate of 29/30 (97.7%)."
"Data governance and the secondary use of data: The board influence.","2023","Inf. Organ.","Stuart Black, Michael J. Davern, Sean B. Maynard, Humza Naseer","Journal Articles","https://dblp.org/rec/journals/iando/BlackDMN23",""
"Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion.","2023","CoRR","Kiran Chhatre, Radek Danecek, Nikos Athanasiou, Giorgio Becherini, Christopher E. Peters, Michael J. Black, Timo Bolkart","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2312-04466",""
"Environment-Specific People.","2023","CoRR","Mirela Ostrek, Soubhik Sanyal, Carol O'Sullivan, Michael J. Black, Justus Thies","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2312-14579",""
"European Health Data & Evidence Network - learnings from building out a standardized international health data network.","2023","J. Am. Medical Informatics Assoc.","Erica A. Voss, Clair Blacketer, Sebastiaan van Sandijk, Maxim Moinat, Michael Kallfelz, Michel Van Speybroeck, Daniel Prieto-Alhambra, Martijn J. Schuemie, Peter R. Rijnbeek","Journal Articles","https://dblp.org/rec/journals/jamia/VossBSMKSPSR23","Abstract Objective Health data standardized to a common data model (CDM) simplifies and facilitates research. This study examines the factors that make standardizing observational health data to the Observational Medical Outcomes Partnership (OMOP) CDM successful. Materials and methods Twenty-five data partners (DPs) from 11 countries received funding from the European Health Data Evidence Network (EHDEN) to standardize their data. Three surveys, DataQualityDashboard results, and statistics from the conversion process were analyzed qualitatively and quantitatively. Our measures of success were the total number of days to transform source data into the OMOP CDM and participation in network research. Results The health data converted to CDM represented more than 133 million patients. 100%, 88%, and 84% of DPs took Surveys 1, 2, and 3. The median duration of the 6 key extract, transform, and load (ETL) processes ranged from 4 to 115 days. Of the 25 DPs, 21 DPs were considered applicable for analysis of which 52% standardized their data on time, and 48% participated in an international collaborative study. Discussion This study shows that the consistent workflow used by EHDEN proves appropriate to support the successful standardization of observational data across Europe. Over the 25 successful transformations, we confirmed that getting the right people for the ETL is critical and vocabulary mapping requires specific expertise and support of tools. Additionally, we learned that teams that proactively prepared for data governance issues were able to avoid considerable delays improving their ability to finish on time. Conclusion This study provides guidance for future DPs to standardize to the OMOP CDM and participate in distributed networks. We demonstrate that the Observational Health Data Sciences and Informatics community must continue to evaluate and provide guidance and support for what ultimately develops the backbone of how community members generate evidence."
"HAAR: Text-Conditioned Generative Model of 3D Strand-based Human Hairstyles.","2023","CoRR","Vanessa Sklyarova, Egor Zakharov, Otmar Hilliges, Michael J. Black, Justus Thies","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2312-11666","We present HAAR, a new strand-based generative model for 3D human hairstyles. Specifically, based on textual inputs, HAAR produces 3D hairstyles that could be used as production-level assets in modern computer graphics engines. Current AI-based generative models take advantage of powerful 2D priors to reconstruct 3D content in the form of point clouds, meshes, or volumetric functions. However, by using the 2D priors, they are intrinsically limited to only recovering the visual parts. Highly occluded hair structures can not be reconstructed with those methods, and they only model the “outer shell”, which is not ready to be used in physics-based rendering or simulation pipelines. In contrast, we propose a first text-guided generative method that uses 3D hair strands as an underlying representation. Leveraging 2D visual question-answering (VQA) systems, we automatically annotate synthetic hair models that are generated from a small set of artist-created hairstyles. This allows us to train a latent diffusion model that operates in a common hairstyle UV space. In qualitative and quantitative studies, we demonstrate the capabilities of the proposed model and compare it to existing hairstyle generation approaches. For results, please refer to our project page."
"HMP: Hand Motion Priors for Pose and Shape Estimation from Video.","2023","CoRR","Enes Duran, Muhammed Kocabas, Vasileios Choutas, Zicong Fan, Michael J. Black","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2312-16737","Understanding how humans interact with the world necessitates accurate 3D hand pose estimation, a task complicated by the hand’s high degree of articulation, frequent occlusions, self-occlusions, and rapid motions. While most existing methods rely on single-image inputs, videos have useful cues to address aforementioned issues. However, existing video-based 3D hand datasets are insufficient for training feedforward models to generalize to in-the-wild scenarios. On the other hand, we have access to large human motion capture datasets which also include hand motions, e.g. AMASS. Therefore, we develop a generative motion prior specific for hands, trained on the AMASS dataset which features diverse and high-quality hand motions. This motion prior is then employed for video-based 3D hand motion estimation following a latent optimization approach. Our integration of a robust motion prior significantly enhances performance, especially in occluded scenarios. It produces stable, temporally consistent results that surpass conventional single-frame methods. We demonstrate our method’s efficacy via qualitative and quantitative evaluations on the HO3D and DexYCB datasets, with special emphasis on an occlusion-focused subset of HO3D. Code is available at https://hmp.is.tue.mpg.de"
"HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and Objects from Video.","2023","CoRR","Zicong Fan, Maria Parelli, Maria Eleni Kadoglou, Muhammed Kocabas, Xu Chen 0025, Michael J. Black, Otmar Hilliges","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2311-18448","Since humans interact with diverse objects every day, the holistic 3D capture of these interactions is important to understand and model human behaviour. However, most ex-isting methods for hand-object reconstruction from RGB ei-ther assume pre-scanned object templates or heavily rely on limited 3D hand-object data, restricting their ability to scale and generalize to more unconstrained interaction settings. To address this, we introduce HOLD the first category-agnostic method that reconstructs an articulated hand and an object jointly from a monocular interaction video. We develop a compositional articulated implicit model that can reconstruct disentangled 3D hands and ob-jects from 2D images. We also further incorporate hand-object constraints to improve hand-object poses and con-sequently the reconstruction quality. Our method does not rely on any 3D hand-object annotations while significantly outperforming fully-supervised baselines in both in-the-lab and challenging in-the-wild settings. Moreover, we qualita-tively show its robustness in reconstructing from in-the-wild videos. See here for code, data, models, and updates."
"Learning to Decode the Surface Code with a Recurrent, Transformer-Based Neural Network.","2023","CoRR","Johannes Bausch, Andrew W. Senior, Francisco J. H. Heras, Thomas Edlich, Alex Davies, Michael Newman, Cody Jones, Kevin J. Satzinger, Murphy Yuezhen Niu, Sam Blackwell, George Holland, Dvir Kafri, Juan Atalaya, Craig Gidney, Demis Hassabis, Sergio Boixo, Hartmut Neven, Pushmeet Kohli","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2310-05900","Quantum error-correction is a prerequisite for reliable quantum computation. Towards this goal, we present a recurrent, transformer-based neural network which learns to decode the surface code, the leading quantum error-correction code. Our decoder outperforms state-of-the-art algorithmic decoders on real-world data from Google's Sycamore quantum processor for distance 3 and 5 surface codes. On distances up to 11, the decoder maintains its advantage on simulated data with realistic noise including cross-talk, leakage, and analog readout signals, and sustains its accuracy far beyond the 25 cycles it was trained on. Our work illustrates the ability of machine learning to go beyond human-designed algorithms by learning from data directly, highlighting machine learning as a strong contender for decoding in quantum computers."
"Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization.","2023","CoRR","Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu 0019, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, Bernhard Schölkopf","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2311-06243","Large foundation models are becoming ubiquitous, but training them from scratch is prohibitively expensive. Thus, efficiently adapting these powerful models to downstream tasks is increasingly important. In this paper, we study a principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream task adaptation. Despite demonstrating good generalizability, OFT still uses a fairly large number of trainable parameters due to the high dimensionality of orthogonal matrices. To address this, we start by examining OFT from an information transmission perspective, and then identify a few key desiderata that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast Fourier transform algorithm enables efficient information transmission, we propose an efficient orthogonal parameterization using butterfly structures. We apply this parameterization to OFT, creating a novel parameter-efficient finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a special case, BOFT introduces a generalized orthogonal finetuning framework. Finally, we conduct an extensive empirical study of adapting large vision transformers, large language models, and text-to-image diffusion models to various downstream tasks in vision and language."
"PoseGPT: Chatting about 3D Human Pose.","2023","CoRR","Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun 0030, Priyanka Patel, Michael J. Black","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2311-18836","We introduce PoseGPT, a framework employing Large Language Models (LLMs) to understand and reason about 3D human poses from images or textual descriptions. Our work is motivated by the human ability to intuitively understand postures from a single image or a brief description, a process that intertwines image interpretation, world knowledge, and an understanding of body language. Traditional human pose estimation methods, whether image-based or text-based, often lack holistic scene comprehension and nuanced reasoning, leading to a disconnect between visual data and its real-world implications. PoseGPT addresses these limitations by embedding SMPL poses as a distinct signal token within a multi-modal LLM, enabling direct generation of 3D body poses from both textual and visual inputs. This approach not only simplifies pose prediction but also empowers LLMs to apply their world knowledge in reasoning about human poses, fostering two advanced tasks: speculative pose generation and reasoning about pose estimation. These tasks involve reasoning about humans to generate 3D poses from subtle text queries, possibly accompanied by images. We establish benchmarks for these tasks, moving beyond traditional 3D pose generation and"
"Representing and utilizing clinical textual data for real world studies: An OHDSI approach.","2023","J. Biomed. Informatics","Vipina Kuttichi Keloth, Juan M. Banda, Michael J. Gurley, Paul M. Heider, Georgina Kennedy, Hongfang Liu, Feifan Liu, Timothy A. Miller, Karthik Natarajan, Olga V. Patterson, Yifan Peng, Kalpana Raja, Ruth M. Reeves, Masoud Rouhizadeh, Jianlin Shi, Xiaoyan Wang, Yanshan Wang, Wei-Qi Wei, Andrew E. Williams, Rui Zhang 0028, Rimma Belenkaya, Christian G. Reich, Clair Blacketer, Patrick B. Ryan, George Hripcsak, Noémie Elhadad, Hua Xu 0001","Journal Articles","https://dblp.org/rec/journals/jbi/KelothBGHKLLMNPPRRRSWWWWZBRBRHEX23",""
"Tropics near Real Time Atmospheric Vertical Temperature and Water Vapor Profile Retrieval.","2023","IGARSS","Adam B. Milstein, Michael L. Pieper, Robert Vincent Leslie, William J. Blackwell","Conference and Workshop Papers","https://dblp.org/rec/conf/igarss/MilsteinPLB23","We have developed and implemented a near-real-time retrieval algorithm to estimate temperature and water vapor vertical profiles from TROPICS [1] L1b brightness temperature observations. A neural network approach, with heritage in our past work [2] in the operational Atmospheric Infrared Sounder science products [3], was selected due to fast execution time, overall accuracy, and robustness to a wide variety of meteorological conditions. Here, we describe our methodology, present initial performance results on the TROPICS Pathfinder mission on test data sets, and describe our ongoing efforts to validate the algorithm."
"WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion.","2023","CoRR","Soyong Shin, Juyong Kim 0002, Eni Halilaj, Michael J. Black","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2312-07531","The estimation of 3D human motion from video has progressed rapidly but current methods still have several key limitations. First, most methods estimate the human in camera coordinates. Second, prior work on estimating humans in global coordinates often assumes a flat ground plane and produces foot sliding. Third, the most accurate methods rely on computationally expensive optimization pipelines, limiting their use to offline applications. Finally, existing video-based methods are surprisingly less accurate than singleframe methods. We address these limitations with WHAM (World-grounded Humans with Accurate Motion), which accurately and efficiently reconstructs 3D human motion in a global coordinate system from video. WHAM learns to lift 2D keypoint sequences to 3D using motion capture data and fuses this with video features, integrating motion context and visual information. WHAM exploits camera angular velocity estimated from a SLAM method together with human motion to estimate the body's global trajectory. We combine this with a contact-aware trajectory refinement method that lets WHAM capture human motion in diverse conditions, such as climbing stairs. WHAM outperforms all existing 3D human motion recovery methods across multiple in-the-wild benchmarks. Code is available for research purposes at http://wham.is.tue.mpg.del."
"Black box no more: A cross-sectional multi-disciplinary survey for exploring governance and guiding adoption of AI in medical imaging and radiotherapy in the UK.","2024","Int. J. Medical Informatics","Nikolaos Stogiannos, Lia Litosseliti, Tracy O'Regan, Erica Scurr, Anna Barnes, Amrita Kumar, Rizwan Malik, Michael Pogose, Hugh Harvey, Mark F. McEntee, Christina Malamateniou","Journal Articles","https://dblp.org/rec/journals/ijmi/StogiannosLOSBKMPHMM24",""
"Black-Box Access is Insufficient for Rigorous AI Audits.","2024","FAccT","Stephen Casper, Carson Ezell, Charlotte Siegmann, Noam Kolt, Taylor Lynn Curtis, Benjamin Bucknall, Andreas A. Haupt, Kevin Wei, Jérémy Scheurer, Marius Hobbhahn, Lee Sharkey, Satyapriya Krishna, Marvin Von Hagen, Silas Alberti, Alan Chan, Qinyi Sun, Michael Gerovitch, David Bau, Max Tegmark, David Krueger 0001, Dylan Hadfield-Menell","Conference and Workshop Papers","https://dblp.org/rec/conf/fat/CasperESKCBHWSH24","External audits of AI systems are increasingly recognized as a key mechanism for AI governance. The effectiveness of an audit, however, depends on the degree of access granted to auditors. Recent audits of state-of-the-art AI systems have primarily relied on black-box access, in which auditors can only query the system and observe its outputs. However, white-box access to the system’s inner workings (e.g., weights, activations, gradients) allows an auditor to perform stronger attacks, more thoroughly interpret models, and conduct fine-tuning. Meanwhile, outside-the-box access to training and deployment information (e.g., methodology, code, documentation, data, deployment details, findings from internal evaluations) allows auditors to scrutinize the development process and design more targeted evaluations. In this paper, we examine the limitations of black-box audits and the advantages of white- and outside-the-box audits. We also discuss technical, physical, and legal safeguards for performing these audits with minimal security risks. Given that different forms of access can lead to very different levels of evaluation, we conclude that (1) transparency regarding the access and methods used by auditors is necessary to properly interpret audit results, and (2) white- and outside-the-box access allow for substantially more scrutiny than black-box access alone."
"Deep orientated distance-transform network for geometric-aware centerline detection.","2024","Pattern Recognit.","Zheheng Jiang, Hossein Rahmani 0001, Plamen Angelov 0001, Ritesh Vyas, Huiyu Zhou 0001, Sue Black 0002, Bryan M. Williams 0001","Journal Articles","https://dblp.org/rec/journals/pr/JiangRAVZBW24",""
"PMBO: Enhancing Black-Box Optimization through Multivariate Polynomial Surrogates.","2024","CoRR","Janina Schreiber, Pau Batlle, Damar Wicaksono, Michael Hecht","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2403-07485","We introduce a surrogate-based black-box optimization method, termed Polynomial-model-based optimization (PMBO). The algorithm alternates polynomial approximation with Bayesian optimization steps, using Gaussian processes to model the error between the objective and its polynomial fit. We describe the algorithmic design of PMBO and compare the results of the performance of PMBO with several optimization methods for a set of analytic test functions. The results show that PMBO outperforms the classic Bayesian optimization and is robust with respect to the choice of its correlation function family and its hyper-parameter setting, which, on the contrary, need to be carefully tuned in classic Bayesian optimization. Remarkably, PMBO performs comparably with state-of-the-art evolutionary algorithms such as the Covariance Matrix Adaptation -- Evolution Strategy (CMA-ES). This finding suggests that PMBO emerges as the pivotal choice among surrogate-based optimization methods when addressing low-dimensional optimization problems. Hereby, the simple nature of polynomials opens the opportunity for interpretation and analysis of the inferred surrogate model, providing a macroscopic perspective on the landscape of the objective function."
"Stability of Black Solitons in Optical Systems with Intensity-Dependent Dispersion.","2024","SIAM J. Math. Anal.","Dmitry E. Pelinovsky, Michael Plum","Journal Articles","https://dblp.org/rec/journals/siamma/PelinovskyP24",""
"The black box problem revisited. Real and imaginary challenges for automated legal decision making.","2024","Artif. Intell. Law","Bartosz Brozek, Michal Furman, Marek Jakubiec, Bartlomiej Kucharzyk","Journal Articles","https://dblp.org/rec/journals/ail/BrozekFJK24","AbstractThis paper addresses the black-box problem in artificial intelligence (AI), and the related problem of explainability of AI in the legal context. We argue, first, that the black box problem is, in fact, a superficial one as it results from an overlap of four different – albeit interconnected – issues: the opacity problem, the strangeness problem, the unpredictability problem, and the justification problem. Thus, we propose a framework for discussing both the black box problem and the explainability of AI. We argue further that contrary to often defended claims the opacity issue is not a genuine problem. We also dismiss the justification problem. Further, we describe the tensions involved in the strangeness and unpredictability problems and suggest some ways to alleviate them."
"Training Diffusion Models with Reinforcement Learning.","2024","ICLR","Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Sergey Levine","Conference and Workshop Papers","https://dblp.org/rec/conf/iclr/BlackJDKL24","Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation. The project's website can be found at http://rl-diffusion.github.io ."
"A Data-driven Understanding of Left-Wing Extremists on Social Media.","2023","CoRR","Utkucan Balci, Michael Sirivianos, Jeremy Blackburn","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2307-06981","Social media's role in the spread and evolution of extremism is a focus of intense study. Online extremists have been involved in the spread of online hate, mis/disinformation, and real-world violence. However, the overwhelming majority of existing work has focused on right-wing extremism. In this paper, we perform a first of its kind large-scale, data-driven study exploring left-wing extremism. We focus on""tankies,""a left-wing community that first arose in the 1950s in support of hardline actions of the USSR and has evolved to support what they call""actually existing socialist countries,""e.g., CCP run China, the USSR, former soviet countries, and North Korea. We collect 1.3M posts from 53K authors from tankies subreddits, and explore the position of tankies within the broader far-left community on Reddit. Among other things, we find that tankies are clearly on the periphery of the larger far-left community. When examining the contents of posts, we find misalignments and conceptual homomorphisms that confirm the description of tankies in the theoretical work. We also discover that tankies focus more on state-level political events rather than social issues in comparison to other far-left communities. Finally, we show that tankies exhibit some of the same worrying behaviors as right-wing extremists, e.g., relatively high toxicity and an organized response to deplatforming events."
"A Probabilistic Attention Model with Occlusion-aware Texture Regression for 3D Hand Reconstruction from a Single RGB Image.","2023","CVPR","Zheheng Jiang, Hossein Rahmani 0001, Sue Black 0002, Bryan M. Williams 0001","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/JiangR0023",""
"Analysis of Risk Factors and Diagnosis for Anxiety Disorder in Older People with the Aid of Artificial Intelligence: Observational Study.","2023","AICS","Jinling Wang 0003, Michaela M. Black, Debbie Rankin, Jonathan G. Wallace, Catherine F. Hughes, Leane Hoey, Adrian Moore 0001, Joshua Tobin, Mimi Zhang, James Ng, Geraldine Horigan, Paul Carlin, Kevin McCarroll, Conal Cunningham, Helene McNulty, Anne M. Molloy","Conference and Workshop Papers","https://dblp.org/rec/conf/aics/WangBRWHHMTZNHCMCMM23","Anxiety disorders are the most common mental health problems particularly in older people who suffer from loneliness and social isolation, chronic health conditions, financial insecurity and other factors that can lead to anxiety disorders. The high prevalence and health risks of anxiety disorders, and the requirement for effective mental care, coupled with recent advances in artificial intelligence, has resulted in an increase exploration of how machine learning can aid the diagnosis and prediction of mental health problems. Data from the Trinity-Ulster-Department of Agriculture (TUDA) study will be utilized to identify risk factors for anxiety in community dwelling older adults using machine learning techniques. The TUDA study includes detailed information on biochemical, clinical, nutritional, lifestyle, and sociodemographic factors in 5186 older people recruited from the Republic of Ireland and Northern Ireland. These characteristics could foster the prediction of anxiety disorders using supervised machine learning methods. Biomarker risk factor analysis was conducted to facilitate feature engineering. In this observational study, several classical machine learning models have been trained to predict anxiety disorders. Comparing the accuracy results and determining the impact of features on the predictions of each method. The models' performance was assessed on a held-out test set and achieved an accuracy of 85.4% (sensitivity: 67.0%, specificity: 90.3%) and 83.4% (sensitivity: 81.5%, specificity: 83.9%) for two best performing methods i.e., random forest and support vector machine respectively, using the standard Synthetic Minority Oversampling Technique. Risk factors such as female sex, loneliness, separated/divorced conditions, lifestyle-related, socio-economic low status, chronic diseases, and family related diseases were identified. These results will aid in the early detection of anxiety disorder in future studies."
"Black-box Attacks Against Neural Binary Function Detection.","2023","RAID","Joshua Bundt, Michael Davinroy, Ioannis Agadakos, Alina Oprea, William K. Robertson","Conference and Workshop Papers","https://dblp.org/rec/conf/raid/BundtDAOR23","Binary analyses based on deep neural networks (DNNs), or neural binary analyses (NBAs), have become a hotly researched topic in recent years. DNNs have been wildly successful at pushing the performance and accuracy envelopes in the natural language and image processing domains. Thus, DNNs are highly promising for solving binary analysis problems that are hard due to a lack of complete information resulting from the lossy compilation process. Despite this promise, it is unclear that the prevailing strategy of repurposing embeddings and model architectures originally developed for other problem domains is sound given the adversarial contexts under which binary analysis often operates. In this paper, we empirically demonstrate that the current state of the art in neural function boundary detection is vulnerable to both inadvertent and deliberate adversarial attacks. We proceed from the insight that current generation NBAs are built upon embeddings and model architectures intended to solve syntactic problems. We devise a simple, reproducible, and scalable black-box methodology for exploring the space of inadvertent attacks – instruction sequences that could be emitted by common compiler toolchains and configurations – that exploits this syntactic design focus. We then show that these inadvertent misclassifications can be exploited by an attacker, serving as the basis for a highly effective black-box adversarial example generation process. We evaluate this methodology against two state-of-the-art neural function boundary detectors: XDA and DeepDi. We conclude with an analysis of the evaluation data and recommendations for how future research might avoid succumbing to similar attacks."
"Computing Education Research in the UK & Ireland.","2023","Past, Present and Future of Computing Education Research","Brett A. Becker, Steven Bradley, Joseph Maguire 0001, Michaela Black, Tom Crick, Mohammed Saqr, Sue Sentance, Keith Quille","Parts in Books or Collections","https://dblp.org/rec/books/sp/23/BeckerB0BCSSQ23",""
"Constrained stochastic blackbox optimization using a progressive barrier and probabilistic estimates.","2023","Math. Program.","Kwassi Joseph Dzahini, Michael Kokkolaras, Sébastien Le Digabel","Journal Articles","https://dblp.org/rec/journals/mp/DzahiniKD23",""
"Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation.","2023","CoRR","Danqing Luo, Chen Zhang 0020, Jiahui Xu, Bin Wang 0040, Yiming Chen, Yan Zhang 0004, Haizhou Li 0001","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2305-13785","Training or finetuning large-scale language models (LLMs) such as GPT-3 requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks. One practical area of research is to treat these models as black boxes and interact with them through their inference APIs. In this paper, we investigate how to optimize few-shot text classification without accessing the gradients of the LLMs. To achieve this, we treat the black-box model as a feature extractor and train a classifier with the augmented text data. Data augmentation is performed using prompt-based finetuning on an auxiliary language model with a much smaller parameter size than the black-box model. Through extensive experiments on eight text classification datasets, we show that our approach, dubbed BT-Classifier, significantly outperforms state-of-the-art black-box few-shot learners and performs on par with methods that rely on full-model tuning."
"Minimizing Black Boxes due to Polynomial-Model-Based Optimization.","2023","GECCO Companion","Janina Schreiber, Damar Wicaksono, Michael Hecht","Conference and Workshop Papers","https://dblp.org/rec/conf/gecco/SchreiberWH23",""
"Opening the black box of knowledge management mechanisms: exploring knowledge flows at a consultancy.","2023","Kybernetes","Janek Richter, Dirk Basten, Bjoern Michalik, Christoph Rosenkranz, Stefan Smolnik","Journal Articles","https://dblp.org/rec/journals/kybernetes/RichterBMRS23","PurposeBased on an exploratory case-based approach, the purpose of this paper is to open the KM black box and examine the relationships that link knowledge management (KM) inputs (i.e. knowledge resources and KM practices) via knowledge processes to KM performance. This paper aims to identify the underlying mechanisms and explain how KM performance is enabled.Design/methodology/approachThis in-depth case study conducted at a medium-sized consultancy in the supply chain management industry empirically examines knowledge flows to uncover the relationships between KM inputs, knowledge processes and KM performance. We adopt the viable system model (VSM) as a theoretical lens to identify KM mechanisms.FindingsBy identifying six KM mechanisms, we contribute to the theoretical understanding of how KM inputs are interconnected and lead to KM performance via knowledge processes.Originality/valueBased on the insights gained, we provide propositions that organizations should consider in designing viable KM. Our findings help organizations in understanding their KM with the help of knowledge flow analysis and identifying how critical KM elements are interconnected."
"Project Silica: Towards Sustainable Cloud Archival Storage in Glass.","2023","SOSP","Patrick Anderson 0001, Erika Blancada Aranas, Youssef Assaf, Raphael Behrendt, Richard Black, Marco Caballero, Pashmina Cameron, Burcu Canakci, Thales De Carvalho, Andromachi Chatzieleftheriou, Rebekah Storan Clarke, James Clegg, Daniel Cletheroe, Bridgette Cooper, Tim Deegan, Austin Donnelly, Rokas Drevinskas, Alexander L. Gaunt, Christos Gkantsidis, Ariel Gomez Diaz, István Haller, Freddie Hong, Teodora Ilieva, Shashidhar Joshi, Russell Joyce, Mint Kunkel, David Lara, Sergey Legtchenko, Fanglin Linda Liu, Bruno Magalhães, Alana Marzoev, Marvin McNett, Jayashree Mohan, Michael Myrah, Trong Nguyen, Sebastian Nowozin, Aaron Ogus, Hiske Overweg, Antony I. T. Rowstron, Maneesh Sah, Masaaki Sakakura, Peter Scholtz, Nina Schreiner, Omer Sella, Adam Smith, Ioan A. Stefanovici, David Sweeney, Benn Thomsen, Govert Verkes, Phil Wainman, Jonathan Westcott, Luke Weston, Charles Whittaker, Pablo Wilke Berenguer, Hugh Williams, Thomas Winkler, Stefan Winzeck","Conference and Workshop Papers","https://dblp.org/rec/conf/sosp/0001AABBCCCCCCC23","Sustainable and cost-effective long-term storage remains an unsolved problem. The most widely used storage technologies today are magnetic (hard disk drives and tape). They use media that degrades over time and has a limited lifetime, which leads to inefficient, wasteful, and costly solutions for long-lived data. This paper presents Silica: the first cloud storage system for archival data underpinned by quartz glass, an extremely resilient media that allows data to be left in situ indefinitely. The hardware and software of Silica have been co-designed and co-optimized from the media up to the service level with sustainability as a primary objective. The design follows a cloud-first, data-driven methodology underpinned by principles derived from analyzing the archival workload of a large public cloud service. Silica can support a wide range of archival storage workloads and ushers in a new era of sustainable, cost-effective storage."
"Retiree Volunteerism: Automating ""Word of Mouth"" Communication 216-223.","2023","IUI Workshops","Jeffrey Black, Ishitha Michael, Dan Roberts, Brodrick Stigall, Bart P. Knijnenburg","Conference and Workshop Papers","https://dblp.org/rec/conf/iui/BlackMRSK23","Many retired people engage in volunteer opportunities as a means to give back to their communities, stay physically and intellectually active, and build and expand their social networks. However, our semi-structured interviews of six retirees found that they typically avoid searching for volunteer opportunities through websites and social media due to a lack of trust in those tools and a concern for privacy. Instead, they rely on word-of-mouth communication facilitated through emails with individuals and organizations they trust. To support this type of communication, we designed an adaptive interaction mechanism in the form of a newsletter with volunteer opportunities that are personalized using recommender system technology. The newsletter mechanism leverages personal connections through user-defined preference-based communities that allow users to share volunteer opportunities with their peers."
"Training Diffusion Models with Reinforcement Learning.","2023","CoRR","Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Sergey Levine","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2305-13301","Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation. The project's website can be found at http://rl-diffusion.github.io ."
"Who needs XAI in the Energy Sector? A Framework to Upgrade Black Box Explainability.","2023","ICIS","Sarah Kristin Lier, Jana Gerlach, Michael H. Breitner","Conference and Workshop Papers","https://dblp.org/rec/conf/icis/LierGB23",""
"3D Points Splatting for Real-Time Dynamic Hand Reconstruction.","2023","CoRR","Zheheng Jiang, Hossein Rahmani 0001, Sue Black 0002, Bryan M. Williams 0001","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2312-13770","We present 3D Points Splatting Hand Reconstruction (3D-PSHR), a real-time and photo-realistic hand reconstruction approach. We propose a self-adaptive canonical points upsampling strategy to achieve high-resolution hand geometry representation. This is followed by a self-adaptive deformation that deforms the hand from the canonical space to the target pose, adapting to the dynamic changing of canonical points which, in contrast to the common practice of subdividing the MANO model, offers greater flexibility and results in improved geometry fitting. To model texture, we disentangle the appearance color into the intrinsic albedo and pose-aware shading, which are learned through a Context-Attention module. Moreover, our approach allows the geometric and the appearance models to be trained simultaneously in an end-to-end manner. We demonstrate that our method is capable of producing animatable, photorealistic and relightable hand reconstructions using multiple datasets, including monocular videos captured with handheld smartphones and large-scale multi-view videos featuring various hand poses. We also demonstrate that our approach achieves real-time rendering speeds while simultaneously maintaining superior performance compared to existing state-of-the-art methods."
"BlackJack: Secure machine learning on IoT devices through hardware-based shuffling.","2023","CoRR","Karthik Ganesan 0002, Michal Fishkin, Ourong Lin, Natalie Enright Jerger","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2310-17804","Neural networks are seeing increased use in diverse Internet of Things (IoT) applications such as healthcare, smart homes and industrial monitoring. Their widespread use makes neural networks a lucrative target for theft. An attacker can obtain a model without having access to the training data or incurring the cost of training. Also, networks trained using private data (e.g., medical records) can reveal information about this data. Networks can be stolen by leveraging side channels such as power traces of the IoT device when it is running the network. Existing attacks require operations to occur in the same order each time; an attacker must collect and analyze several traces of the device to steal the network. Therefore, to prevent this type of attack, we randomly shuffle the order of operations each time. With shuffling, each operation can now happen at many different points in each execution, making the attack intractable. However, we show that shuffling in software can leak information which can be used to subvert this solution. Therefore, to perform secure shuffling and reduce latency, we present BlackJack, hardware added as a functional unit within the CPU. BlackJack secures neural networks on IoT devices by increasing the time needed for an attack to centuries, while adding just 2.46% area, 3.28% power and 0.56% latency overhead on an ARM M0+ SoC."
"Polynomial-Model-Based Optimization for Blackbox Objectives.","2023","CoRR","Janina Schreiber, Damar Wicaksono, Michael Hecht","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2309-00663","For a wide range of applications the structure of systems like Neural Networks or complex simulations, is unknown and approximation is costly or even impossible. Black-box optimization seeks to find optimal (hyper-) parameters for these systems such that a pre-defined objective function is minimized. Polynomial-Model-Based Optimization (PMBO) is a novel blackbox optimizer that finds the minimum by fitting a polynomial surrogate to the objective function. Motivated by Bayesian optimization the model is iteratively updated according to the acquisition function Expected Improvement, thus balancing the exploitation and exploration rate and providing an uncertainty estimate of the model. PMBO is benchmarked against other state-of-the-art algorithms for a given set of artificial, analytical functions. PMBO competes successfully with those algorithms and even outperforms all of them in some cases. As the results suggest, we believe PMBO is the pivotal choice for solving blackbox optimization tasks occurring in a wide range of disciplines."
