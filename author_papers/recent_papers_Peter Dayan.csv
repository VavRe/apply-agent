"title","year","venue","authors","type","url","abstract"
"Characterising the Creative Process in Humans and Large Language Models.","2024","CoRR","Surabhi S. Nath, Peter Dayan, Claire Stevenson","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2405-00899","Large language models appear quite creative, often performing on par with the average human on creative tasks. However, research on LLM creativity has focused solely on \textit{products}, with little attention on the creative \textit{process}. Process analyses of human creativity often require hand-coded categories or exploit response times, which do not apply to LLMs. We provide an automated method to characterise how humans and LLMs explore semantic spaces on the Alternate Uses Task, and contrast with behaviour in a Verbal Fluency Task. We use sentence embeddings to identify response categories and compute semantic similarities, which we use to generate jump profiles. Our results corroborate earlier work in humans reporting both persistent (deep search in few semantic spaces) and flexible (broad search across multiple semantic spaces) pathways to creativity, where both pathways lead to similar creativity scores. LLMs were found to be biased towards either persistent or flexible paths, that varied across tasks. Though LLMs as a population match human profiles, their relationship with creativity is different, where the more flexible models score higher on creativity. Our dataset and scripts are available on \href{https://github.com/surabhisnath/Creative_Process}{GitHub}."
"Detecting and Deterring Manipulation in a Cognitive Hierarchy.","2024","CoRR","Nitay Alon, Lion Schulz, Joseph M. Barnby, Jeffrey S. Rosenschein, Peter Dayan","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2405-01870","Humans need to be on their toes when interacting with competitive others to avoid being duped. Too much caution out of context can, however, be detrimental and produce false beliefs of intended harm. Here, we offer a formal account of this phenomenon through the lens of Theory of Mind. We simulate agents of different depths of mentalization within a simple game theoretic paradigm and show how, if aligned well, deep recursive mentalization gives rise to both successful deception as well as reasonable skepticism. However, we also show that if a self is mentalising too deeply -hyper-mentalising - false beliefs arise that a partner is trying to trick them maliciously, resulting ina material loss to the self. This theory offers a potential cognitive mechanism for suspiciousness, paranoia, and conspiratorial ideation. Rather than a deficit in Theory of Mind, paranoia may arise from the application of overly strategic thinking to ingenuous behaviour."
"Predicting the Future with Simple World Models.","2024","CoRR","Tankred Saanum, Peter Dayan, Eric Schulz","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2401-17835","World models can represent potentially high-dimensional pixel observations in compact latent spaces, making it tractable to model the dynamics of the environment. However, the latent dynamics inferred by these models may still be highly complex. Abstracting the dynamics of the environment with simple models can have several benefits. If the latent dynamics are simple, the model may generalize better to novel transitions, and discover useful latent representations of environment states. We propose a regularization scheme that simplifies the world model's latent dynamics. Our model, the Parsimonious Latent Space Model (PLSM), minimizes the mutual information between latent states and the dynamics that arise between them. This makes the dynamics softly state-invariant, and the effects of the agent's actions more predictable. We combine the PLSM with three different model classes used for i) future latent state prediction, ii) video prediction, and iii) planning. We find that our regularization improves accuracy, generalization, and performance in downstream tasks."
"Simplicity in Complexity.","2024","CoRR","Kevin Shen, Surabhi S. Nath, Aenne Brielmann, Peter Dayan","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2403-03134","The complexity of visual stimuli plays an important role in many cognitive phenomena, including attention, engagement, memorability, time perception and aesthetic evaluation. Despite its importance, complexity is poorly understood and ironically, previous models of image complexity have been quite complex. There have been many attempts to find handcrafted features that explain complexity, but these features are usually dataset specific, and hence fail to generalise. On the other hand, more recent work has employed deep neural networks to predict complexity, but these models remain difficult to interpret, and do not guide a theoretical understanding of the problem. Here we propose to model complexity using segment-based representations of images. We use state-of-the-art segmentation models, SAM and FC-CLIP, to quantify the number of segments at multiple granularities, and the number of classes in an image respectively. We find that complexity is well-explained by a simple linear model with these two features across six diverse image-sets of naturalistic scene and art images. This suggests that the complexity of images can be surprisingly simple."
"Compositionality under time pressure.","2023","CogSci","Valerio Rubino, Mani Hamidi, Peter Dayan, Charley M. Wu","Conference and Workshop Papers","https://dblp.org/rec/conf/cogsci/RubinoHDW23","Compositionality is a central component of the human faculty for generalization and flexibility. However, the computations involved are poorly understood, especially in terms of their cognitive costs. On one hand, compositionality requires searching combinatorially large hypothesis spaces, raising issues of tractability. On the other hand, compositional representations afford efficient and compact compression. To shed light on the cognitive resource required for compositionality, we used a within-subject time pressure manipulation to study how participants navigated a series of mazes, generated using recursive operations over spatial primitives. We find evidence that behavior is guided by the use of primitives and abstract operations over them, where the degree of compositional structure increases performance and speeds up decisions. And while time pressure led to more random errors, it did not impair the capacity for compositionality. Rather, participants increased their reliance on reusing and recombining previous computations, suggesting a remarkable robustness of human compositional reasoning."
"Habits of Mind: Reusing Action Sequences for Efficient Planning.","2023","CogSci","Noémi Élteto, Peter Dayan","Conference and Workshop Papers","https://dblp.org/rec/conf/cogsci/EltetoD23","When we exercise sequences of actions, their execution becomes more fluent and precise. Here, we consider the possibility that exercised action sequences can also be used to make planning faster and more accurate by focusing expansion of the search tree on paths that have been frequently used in the past, and by reducing deep planning problems to shallow ones via multi-step jumps in the tree. To capture such sequences, we use a flexible Bayesian action chunking mechanism which finds and exploits statistically reliable structure at different scales. This gives rise to shorter or longer routines that can be embedded into a Monte-Carlo tree search planner. We show the benefits of this scheme using a physical construction task patterned after tangrams."
"Reframing dopamine: A controlled controller at the limbic-motor interface.","2023","PLoS Comput. Biol.","Kevin Lloyd, Peter Dayan","Journal Articles","https://dblp.org/rec/journals/ploscb/LloydD23","Pavlovian influences notoriously interfere with operant behaviour. Evidence suggests this interference sometimes coincides with the release of the neuromodulator dopamine in the nucleus accumbens. Suppressing such interference is one of the targets of cognitive control. Here, using the examples of active avoidance and omission behaviour, we examine the possibility that direct manipulation of the dopamine signal is an instrument of control itself. In particular, when instrumental and Pavlovian influences come into conflict, dopamine levels might be affected by the controlled deployment of a reframing mechanism that recasts the prospect of possible punishment as an opportunity to approach safety, and the prospect of future reward in terms of a possible loss of that reward. We operationalize this reframing mechanism and fit the resulting model to rodent behaviour from two paradigmatic experiments in which accumbens dopamine release was also measured. We show that in addition to matching animals’ behaviour, the model predicts dopamine transients that capture some key features of observed dopamine release at the time of discriminative cues, supporting the idea that modulation of this neuromodulator is amongst the repertoire of cognitive control strategies."
"Reinforcement Learning with Simple Sequence Priors.","2023","NeurIPS","Tankred Saanum, Noémi Élteto, Peter Dayan, Marcel Binz, Eric Schulz","Conference and Workshop Papers","https://dblp.org/rec/conf/nips/SaanumEDBS23","Everything else being equal, simpler models should be preferred over more complex ones. In reinforcement learning (RL), simplicity is typically quantified on an action-by-action basis -- but this timescale ignores temporal regularities, like repetitions, often present in sequential strategies. We therefore propose an RL algorithm that learns to solve tasks with sequences of actions that are compressible. We explore two possible sources of simple action sequences: Sequences that can be learned by autoregressive models, and sequences that are compressible with off-the-shelf data compression algorithms. Distilling these preferences into sequence priors, we derive a novel information-theoretic objective that incentivizes agents to learn policies that maximize rewards while conforming to these priors. We show that the resulting RL algorithm leads to faster learning, and attains higher returns than state-of-the-art model-free approaches in a series of continuous control tasks from the DeepMind Control Suite. These priors also produce a powerful information-regularized agent that is robust to noisy observations and can perform open-loop control."
"The Inner Sentiments of a Thought.","2023","CoRR","Chris Gagne 0001, Peter Dayan","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2307-01784","This chapter approaches the question of “becoming just” from the perspective of theory and research on the psychology of the development of morality from childhood to adulthood. A perspective on the topic of what it means to be just is presented, based on both philosophical and psychological considerations. It is maintained that psychological-developmental research needs to be grounded in substantive definitions of the moral domain involving considerations of welfare, justice, and rights. Research has shown that there is a correspondence between philosophical analyses of welfare, justice, and rights and the types of judgments individuals begin to develop at early ages. Research findings are discussed regarding the types of social interactions children experience across cultures that contribute to their moral development, as well as the nature of moral judgments formed at different ages."
