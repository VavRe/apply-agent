"title","year","venue","authors","type","url","abstract"
"GazeMotion: Gaze-guided Human Motion Forecasting.","2024","CoRR","Zhiming Hu, Syn Schmitt, Daniel F. B. Haeufle, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2403-09885","We present GazeMotion, a novel method for human motion forecasting that combines information on past human poses with human eye gaze. Inspired by evidence from behavioural sciences showing that human eye and body movements are closely coordinated, GazeMotion first predicts future eye gaze from past gaze, then fuses predicted future gaze and past poses into a gaze-pose graph, and finally uses a residual graph convolutional network to forecast body motion. We extensively evaluate our method on the MoGaze, ADT, and GIMO benchmark datasets and show that it outperforms state-of-the-art methods by up to 7.4% improvement in mean per joint position error. Using head direction as a proxy to gaze, our method still achieves an average improvement of 5.5%. We finally report an online user study showing that our method also outperforms prior methods in terms of perceived realism. These results show the significant information content available in eye gaze for human motion forecasting as well as the effectiveness of our method in exploiting this information."
"Generating Realistic Arm Movements in Reinforcement Learning: A Quantitative Comparison of Reward Terms and Task Requirements.","2024","CoRR","Jhon Charaja, Isabell Wochner, Pierre Schumacher, Winfried Ilg, Martin A. Giese, Christophe Maufroy, Andreas Bulling, Syn Schmitt, Daniel F. B. Haeufle","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2402-13949",""
"HOIMotion: Forecasting Human Motion During Human-Object Interactions Using Egocentric 3D Object Bounding Boxes.","2024","CoRR","Zhiming Hu, Zheming Yin, Daniel F. B. Haeufle, Syn Schmitt, Andreas Bulling","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2407-02633","We present HOIMotion - a novel approach for human motion forecasting during human-object interactions that integrates information about past body poses and egocentric 3D object bounding boxes. Human motion forecasting is important in many augmented reality applications but most existing methods have only used past body poses to predict future motion. HOIMotion first uses an encoder-residual graph convolutional network (GCN) and multi-layer perceptrons to extract features from body poses and egocentric 3D object bounding boxes, respectively. Our method then fuses pose and object features into a novel pose-object graph and uses a residual-decoder GCN to forecast future body motion. We extensively evaluate our method on the Aria digital twin (ADT) and MoGaze datasets and show that HOIMotion consistently outperforms state-of-the-art methods by a large margin of up to 8.7% on ADT and 7.2% on MoGaze in terms of mean per joint position error. Complementing these evaluations, we report a human study (N=20) that shows that the improvements achieved by our method result in forecasted poses being perceived as both more precise and more realistic than those of existing methods. Taken together, these results reveal the significant information content available in egocentric 3D object bounding boxes for human motion forecasting and the effectiveness of our method in exploiting this information."
"Identifying Policy Gradient Subspaces.","2024","ICLR","Jan Schneider, Pierre Schumacher, Simon Guist, Le Chen, Daniel F. B. Haeufle, Bernhard Schölkopf, Dieter Büchler","Conference and Workshop Papers","https://dblp.org/rec/conf/iclr/SchneiderSGCHSB24","Policy gradient methods hold great potential for solving complex continuous control tasks. Still, their training efficiency can be improved by exploiting structure within the optimization problem. Recent work indicates that supervised learning can be accelerated by leveraging the fact that gradients lie in a low-dimensional and slowly-changing subspace. In this paper, we conduct a thorough evaluation of this phenomenon for two popular deep policy gradient methods on various simulated benchmark tasks. Our results demonstrate the existence of such gradient subspaces despite the continuously changing data distribution inherent to reinforcement learning. These findings reveal promising directions for future work on more efficient reinforcement learning, e.g., through improving parameter-space exploration or enabling second-order optimization."
"Learning to Control Emulated Muscles in Real Robots: Towards Exploiting Bio-Inspired Actuator Morphology.","2024","CoRR","Pierre Schumacher, Lorenz Krause, Jan Schneider, Dieter Büchler, Georg Martius, Daniel F. B. Haeufle","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2402-05371","Recent studies have demonstrated the immense potential of exploiting muscle actuator morphology for natural and robust movement -- in simulation. A validation on real robotic hardware is yet missing. In this study, we emulate muscle actuator properties on hardware in real-time, taking advantage of modern and affordable electric motors. We demonstrate that our setup can emulate a simplified muscle model on a real robot while being controlled by a learned policy. We improve upon an existing muscle model by deriving a damping rule that ensures that the model is not only performant and stable but also tuneable for the real hardware. Our policies are trained by reinforcement learning entirely in simulation, where we show that previously reported benefits of muscles extend to the case of quadruped locomotion and hopping: the learned policies are more robust and exhibit more regular gaits. Finally, we confirm that the learned policies can be executed on real hardware and show that sim-to-real transfer with real-time emulated muscles on a quadruped robot is possible. These results show that artificial muscles can be highly beneficial actuators for future generations of robust legged robots."
"A Commentary on Towards autonomous artificial agents with an active self: Modeling sense of control in situated action.","2023","Cogn. Syst. Res.","Chenxu Hao, Nele Russwinkel, Daniel F. B. Haeufle, Philipp Beckerle","Journal Articles","https://dblp.org/rec/journals/cogsr/HaoRHB23",""
"DEP-RL: Embodied Exploration for Reinforcement Learning in Overactuated and Musculoskeletal Systems.","2023","ICLR","Pierre Schumacher, Daniel F. B. Haeufle, Dieter Büchler, Syn Schmitt, Georg Martius","Conference and Workshop Papers","https://dblp.org/rec/conf/iclr/SchumacherHBSM23","Muscle-actuated organisms are capable of learning an unparalleled diversity of dexterous movements despite their vast amount of muscles. Reinforcement learning (RL) on large musculoskeletal models, however, has not been able to show similar performance. We conjecture that ineffective exploration in large overactuated action spaces is a key problem. This is supported by the finding that common exploration noise strategies are inadequate in synthetic examples of overactuated systems. We identify differential extrinsic plasticity (DEP), a method from the domain of self-organization, as being able to induce state-space covering exploration within seconds of interaction. By integrating DEP into RL, we achieve fast learning of reaching and locomotion in musculoskeletal systems, outperforming current approaches in all considered tasks in sample efficiency and robustness."
"Investigating the Impact of Action Representations in Policy Gradient Algorithms.","2023","CoRR","Jan Schneider, Pierre Schumacher, Daniel F. B. Häufle, Bernhard Schölkopf, Dieter Büchler","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2309-06921","Reinforcement learning~(RL) is a versatile framework for learning to solve complex real-world tasks. However, influences on the learning performance of RL algorithms are often poorly understood in practice. We discuss different analysis techniques and assess their effectiveness for investigating the impact of action representations in RL. Our experiments demonstrate that the action representation can significantly influence the learning performance on popular RL benchmark tasks. The analysis results indicate that some of the performance differences can be attributed to changes in the complexity of the optimization landscape. Finally, we discuss open challenges of analysis techniques for RL algorithms."
"Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models.","2023","CoRR","Pierre Schumacher, Thomas Geijtenbeek, Vittorio Caggiano, Vikash Kumar, Syn Schmitt, Georg Martius, Daniel F. B. Haeufle","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2309-02976",""
