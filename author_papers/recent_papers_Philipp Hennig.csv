"title","year","venue","authors","type","url","abstract"
"A Greedy Approximation for k-Determinantal Point Processes.","2024","AISTATS","Julia Grosse, Rahel Fischer, Roman Garnett, Philipp Hennig","Conference and Workshop Papers","https://dblp.org/rec/conf/aistats/GrosseFGH24","Determinantal point processes (DPPs) are an important concept in random matrix theory and combinatorics, and increasingly in machine learning. Samples from these processes exhibit a form of self-avoidance, so they are also helpful in guiding algorithms that explore to reduce uncertainty, such as in active learning, Bayesian optimization, reinforcement learning, and marginalization in graphical models. The best-known algorithms for sampling from DPPs exactly require significant computational expense, which can be unwelcome in machine learning applications when the cost of sampling is relatively low and capturing the precise repulsive nature of the DPP may not be critical. We suggest an inexpensive approximate strategy for sampling a fixed number of points (as would typically be desired in a machine learning setting) from a so-called k -DPP based on iterative inverse transform sampling. We prove that our algorithm satisfies a (1 − 1 /e ) approximation guarantee relative to exact sampling from the k -DPP, and provide an efficient implementation for many common kernels used in machine learning, including the Gaussian and Mat´ern class. Finally, we compare the empirical runtime of our method to exact and Markov-Chain-Monte-Carlo (MCMC) sam-plers and investigate the approximation quality in a Bayesian Quadrature (BQ) setting."
"A switching lung mechanics model for detection of expiratory flow limitation.","2024","Autom.","Carlotta Hennigs, Franziska Bilda, Jan Graßhoff, Stephan Walterspacher, Philipp Rostalski","Journal Articles","https://dblp.org/rec/journals/at/HennigsBGWR24","Abstract Expiratory flow limitation (EFL) is an often unrecognized clinical condition with a multitude of negative implications. A mathematical EFL model is proposed to detect flow limitations automatically. The EFL model is a switching one-compartment lung mechanics model with a volume-dependent airway resistance to simulate the dynamic behavior during expiration. The EFL detection is based on a breath-by-breath model parameter identification and validated on clinical data of mechanically ventilated patients. In the severe flow limitation group 93.9 % ± 5 % and in the no limitation group 10.2 % ± 13.7 % of the breaths are detected as EFL. Based on the high detection rate of EFL, these results support the usefulness of the EFL detection. It is a first step toward an automated detection of EFL in clinical applications and may help to reduce underdiagnosis of EFL."
"Computation-Aware Kalman Filtering and Smoothing.","2024","CoRR","Marvin Pförtner, Jonathan Wenger, Jon Cockayne, Philipp Hennig","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2405-08971",""
"Diffusion Tempering Improves Parameter Estimation with Probabilistic Integrators for Ordinary Differential Equations.","2024","ICML","Jonas Beck, Nathanael Bosch, Michael Deistler, Kyra L. Kadhim, Jakob H. Macke, Philipp Hennig, Philipp Berens","Conference and Workshop Papers","https://dblp.org/rec/conf/icml/BeckBDKMHB24",""
"FSP-Laplace: Function-Space Priors for the Laplace Approximation in Bayesian Deep Learning.","2024","CoRR","Tristan Cinquin, Marvin Pförtner, Vincent Fortuin, Philipp Hennig, Robert Bamler","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2407-13711",""
"Flexible inference in heterogeneous and attributed multilayer networks.","2024","CoRR","Martina Contisciani, Marius Hobbhahn, Eleanor A. Power, Philipp Hennig, Caterina De Bacco","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2405-20918","Networked datasets are often enriched by different types of information about individual nodes or edges. However, most existing methods for analyzing such datasets struggle to handle the complexity of heterogeneous data, often requiring substantial model-specific analysis. In this paper, we develop a probabilistic generative model to perform inference in multilayer networks with arbitrary types of information. Our approach employs a Bayesian framework combined with the Laplace matching technique to ease interpretation of inferred parameters. Furthermore, the algorithmic implementation relies on automatic differentiation, avoiding the need for explicit derivations. This makes our model scalable and flexible to adapt to any combination of input data. We demonstrate the effectiveness of our method in detecting overlapping community structures and performing various prediction tasks on heterogeneous multilayer data, where nodes and edges have different types of attributes. Additionally, we showcase its ability to unveil a variety of patterns in a social support network among villagers in rural India by effectively utilizing all input information in a meaningful way."
"Linearization Turns Neural Operators into Function-Valued Gaussian Processes.","2024","CoRR","Emilia Magnani, Marvin Pförtner, Tobias Weber, Philipp Hennig","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2406-05072","Modeling dynamical systems, e.g. in climate and engineering sciences, often necessitates solving partial differential equations. Neural operators are deep neural networks designed to learn nontrivial solution operators of such differential equations from data. As for all statistical models, the predictions of these models are imperfect and exhibit errors. Such errors are particularly difficult to spot in the complex nonlinear behaviour of dynamical systems. We introduce a new framework for approximate Bayesian uncertainty quantification in neural operators using function-valued Gaussian processes. Our approach can be interpreted as a probabilistic analogue of the concept of currying from functional programming and provides a practical yet theoretically sound way to apply the linearized Laplace approximation to neural operators. In a case study on Fourier neural operators, we show that, even for a discretized input, our method yields a Gaussian closure--a structured Gaussian process posterior capturing the uncertainty in the output function of the neural operator, which can be evaluated at an arbitrary set of points. The method adds minimal prediction overhead, can be applied post-hoc without retraining the neural operator, and scales to large models and datasets. We showcase the efficacy of our approach through applications to different types of partial differential equations."
"Parallel-in-Time Probabilistic Numerical ODE Solvers.","2024","J. Mach. Learn. Res.","Nathanael Bosch, Adrien Corenflos, Fatemeh Yaghoobi, Filip Tronarp, Philipp Hennig, Simo Särkkä","Journal Articles","https://dblp.org/rec/journals/jmlr/BoschCYTHS24","Probabilistic numerical solvers for ordinary differential equations (ODEs) treat the numerical simulation of dynamical systems as problems of Bayesian state estimation. Aside from producing posterior distributions over ODE solutions and thereby quantifying the numerical approximation error of the method itself, one less-often noted advantage of this formalism is the algorithmic flexibility gained by formulating numerical simulation in the framework of Bayesian filtering and smoothing. In this paper, we leverage this flexibility and build on the time-parallel formulation of iterated extended Kalman smoothers to formulate a parallel-in-time probabilistic numerical ODE solver. Instead of simulating the dynamical system sequentially in time, as done by current probabilistic solvers, the proposed method processes all time steps in parallel and thereby reduces the span cost from linear to logarithmic in the number of time steps. We demonstrate the effectiveness of our approach on a variety of ODEs and compare it to a range of both classic and probabilistic numerical ODE solvers."
"Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI.","2024","CoRR","Theodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence Aitchison, Julyan Arbel, David B. Dunson, Maurizio Filippone, Vincent Fortuin, Philipp Hennig, José Miguel Hernández-Lobato, Aliaksandr Hubin, Alexander Immer, Theofanis Karaletsos, Mohammad Emtiyaz Khan, Agustinus Kristiadi, Yingzhen Li, Stephan Mandt, Christopher Nemeth, Michael A. Osborne, Tim G. J. Rudner, David Rügamer, Yee Whye Teh, Max Welling, Andrew Gordon Wilson, Ruqi Zhang","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2402-00809","Abstract
        In this work, minibatch MCMC sampling
for feedforward neural networks is made more feasible.
To this end,
it is proposed to sample subgroups of parameters
via a blocked Gibbs sampling scheme.
By partitioning the parameter space,
sampling is possible irrespective of layer width.
It is also possible to alleviate vanishing acceptance rates
for increasing depth
by reducing the proposal variance in deeper layers.
Increasing the length of a non-convergent chain
increases the predictive accuracy in classification tasks,
so avoiding vanishing acceptance rates and
consequently enabling longer chain runs
have practical benefits.
Moreover, non-convergent chain realizations
aid in the quantification of predictive uncertainty.
An open problem is how to perform minibatch MCMC sampling
for feedforward neural networks
in the presence of augmented data."
"Position: Bayesian Deep Learning is Needed in the Age of Large-Scale AI.","2024","ICML","Theodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence Aitchison, Julyan Arbel, David B. Dunson, Maurizio Filippone, Vincent Fortuin, Philipp Hennig, José Miguel Hernández-Lobato, Aliaksandr Hubin, Alexander Immer, Theofanis Karaletsos, Mohammad Emtiyaz Khan, Agustinus Kristiadi, Yingzhen Li, Stephan Mandt, Christopher Nemeth, Michael A. Osborne, Tim G. J. Rudner, David Rügamer, Yee Whye Teh, Max Welling, Andrew Gordon Wilson, Ruqi Zhang","Conference and Workshop Papers","https://dblp.org/rec/conf/icml/PapamarkouSPAAD24","In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential."
"Probabilistic ODE solvers for integration error-aware numerical optimal control.","2024","L4DC","Amon Lahr, Filip Tronarp, Nathanael Bosch, Jonathan Schmidt, Philipp Hennig, Melanie N. Zeilinger","Conference and Workshop Papers","https://dblp.org/rec/conf/l4dc/LahrTBSHZ24",""
"Reparameterization invariance in approximate Bayesian inference.","2024","CoRR","Hrittik Roy, Marco Miani, Carl Henrik Ek, Philipp Hennig, Marvin Pförtner, Lukas Tatzel, Søren Hauberg","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2406-03334","Current approximate posteriors in Bayesian neural networks (BNNs) exhibit a crucial limitation: they fail to maintain invariance under reparameterization, i.e. BNNs assign different posterior densities to different parametrizations of identical functions. This creates a fundamental flaw in the application of Bayesian principles as it breaks the correspondence between uncertainty over the parameters with uncertainty over the parametrized function. In this paper, we investigate this issue in the context of the increasingly popular linearized Laplace approximation. Specifically, it has been observed that linearized predictives alleviate the common underfitting problems of the Laplace approximation. We develop a new geometric view of reparametrizations from which we explain the success of linearization. Moreover, we demonstrate that these reparameterization invariance properties can be extended to the original neural network predictive using a Riemannian diffusion process giving a straightforward algorithm for approximate posterior sampling, which empirically improves posterior fit."
"Scaling up Probabilistic PDE Simulators with Structured Volumetric Information.","2024","CoRR","Tim Weiland, Marvin Pförtner, Philipp Hennig","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2406-05020","Modeling real-world problems with partial differential equations (PDEs) is a prominent topic in scientific machine learning. Classic solvers for this task continue to play a central role, e.g. to generate training data for deep learning analogues. Any such numerical solution is subject to multiple sources of uncertainty, both from limited computational resources and limited data (including unknown parameters). Gaussian process analogues to classic PDE simulation methods have recently emerged as a framework to construct fully probabilistic estimates of all these types of uncertainty. So far, much of this work focused on theoretical foundations, and as such is not particularly data efficient or scalable. Here we propose a framework combining a discretization scheme based on the popular Finite Volume Method with complementary numerical linear algebra techniques. Practical experiments, including a spatiotemporal tsunami simulation, demonstrate substantially improved scaling behavior of this approach over previous collocation-based techniques."
"Stable Implementation of Probabilistic ODE Solvers.","2024","J. Mach. Learn. Res.","Nicholas Krämer, Philipp Hennig","Journal Articles","https://dblp.org/rec/journals/jmlr/KramerH24","AbstractUnderstanding neural computation on the mechanistic level requires models of neurons and neuronal networks. To analyze such models one typically has to solve coupled ordinary differential equations (ODEs), which describe the dynamics of the underlying neural system. These ODEs are solved numerically with deterministic ODE solvers that yield single solutions with either no, or only a global scalar bound on precision. It can therefore be challenging to estimate the effect of numerical uncertainty on quantities of interest, such as spike-times and the number of spikes. To overcome this problem, we propose to use recently developed sampling-based probabilistic solvers, which are able to quantify such numerical uncertainties. They neither require detailed insights into the kinetics of the models, nor are they difficult to implement. We show that numerical uncertainty can affect the outcome of typical neuroscience simulations, e.g. jittering spikes by milliseconds or even adding or removing individual spikes from simulations altogether, and demonstrate that probabilistic solvers reveal these numerical uncertainties with only moderate computational overhead."
"Uncertainty-Guided Optimization on Large Language Model Search Trees.","2024","CoRR","Julia Grosse, Ruotian Wu, Ahmad Rashid, Philipp Hennig, Pascal Poupart, Agustinus Kristiadi","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2407-03951",""
"VIPurPCA: Visualizing and Propagating Uncertainty in Principal Component Analysis.","2024","IEEE Trans. Vis. Comput. Graph.","Susanne Zabel, Philipp Hennig, Kay Nieselt","Journal Articles","https://dblp.org/rec/journals/tvcg/ZabelHN24","Variables obtained by experimental measurements or statistical inference typically carry uncertainties. When an algorithm uses such quantities as input variables, this uncertainty should propagate to the algorithm's output. Concretely, we consider the classic notion of principal component analysis (PCA): If it is applied to a finite data matrix containing imperfect (i.e., uncertain) multidimensional measurements, its output—a lower-dimensional representation—is itself subject to uncertainty. We demonstrate that this uncertainty can be approximated by appropriate linearization of the algorithm's nonlinear functionality, using automatic differentiation. By itself, however, this structured, uncertain output is difficult to interpret for users. We provide an animation method that effectively visualizes the uncertainty of the lower dimensional map. Implemented as an open-source software package, it allows researchers to assess the reliability of PCA embeddings."
"Anatomy-guided domain adaptation for 3D in-bed human pose estimation.","2023","Medical Image Anal.","Alexander Bigalke, Lasse Hansen, Jasper Diesel, Carlotta Hennigs, Philipp Rostalski, Mattias P. Heinrich","Journal Articles","https://dblp.org/rec/journals/mia/BigalkeHDHRH23",""
"MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset.","2023","ACL","Leonhard Hennig, Philippe Thomas 0002, Sebastian Möller 0001","Conference and Workshop Papers","https://dblp.org/rec/conf/acl/Hennig0023","Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance."
"Optimistic Optimization of Gaussian Process Samples.","2023","Trans. Mach. Learn. Res.","Julia Grosse, Cheng Zhang, Philipp Hennig","Journal Articles","https://dblp.org/rec/journals/tmlr/GrosseZH23","Bayesian optimization is a popular formalism for global optimization, but its computational costs limit it to expensive-to-evaluate functions. A competing, computationally more efficient, global optimization framework is optimistic optimization, which exploits prior knowledge about the geometry of the search space in form of a dissimilarity function. We investigate to which degree the conceptual advantages of Bayesian Optimization can be combined with the computational efficiency of optimistic optimization. By mapping the kernel to a dissimilarity, we obtain an optimistic optimization algorithm for the Bayesian Optimization setting with a run-time of up to $\mathcal{O}(N \log N)$. As a high-level take-away we find that, when using stationary kernels on objectives of relatively low evaluation cost, optimistic optimization can be strongly preferable over Bayesian optimization, while for strongly coupled and parametric models, good implementations of Bayesian optimization can perform much better, even at low evaluation cost. We argue that there is a new research domain between geometric and probabilistic search, i.e. methods that run drastically faster than traditional Bayesian optimization, while retaining some of the crucial functionality of Bayesian optimization."
"ViViT: Curvature Access Through The Generalized Gauss-Newton's Low-Rank Structure.","2023","Trans. Mach. Learn. Res.","Felix Dangel, Lukas Tatzel, Philipp Hennig","Journal Articles","https://dblp.org/rec/journals/tmlr/DangelTH23","Curvature in form of the Hessian or its generalized Gauss-Newton (GGN) approximation is valuable for algorithms that rely on a local model for the loss to train, compress, or explain deep networks. Existing methods based on implicit multiplication via automatic differentiation or Kronecker-factored block diagonal approximations do not consider noise in the mini-batch. We present ViViT, a curvature model that leverages the GGN's low-rank structure without further approximations. It allows for efficient computation of eigenvalues, eigenvectors, as well as per-sample first- and second-order directional derivatives. The representation is computed in parallel with gradients in one backward pass and offers a fine-grained cost-accuracy trade-off, which allows it to scale. We demonstrate this by conducting performance benchmarks and substantiate ViViT's usefulness by studying the impact of noise on the GGN's structural properties during neural network training."
"Accelerating Generalized Linear Models by Trading off Computation for Uncertainty.","2023","CoRR","Lukas Tatzel, Jonathan Wenger, Frank Schneider 0001, Philipp Hennig","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2310-20285","Bayesian Generalized Linear Models (GLMs) define a flexible probabilistic framework to model categorical, ordinal and continuous data, and are widely used in practice. However, exact inference in GLMs is prohibitively expensive for large datasets, thus requiring approximations in practice. The resulting approximation error adversely impacts the reliability of the model and is not accounted for in the uncertainty of the prediction. In this work, we introduce a family of iterative methods that explicitly model this error. They are uniquely suited to parallel modern computing hardware, efficiently recycle computations, and compress information to reduce both the time and memory requirements for GLMs. As we demonstrate on a realistically large classification problem, our method significantly accelerates training compared to competitive baselines by trading off reduced computation for increased uncertainty."
"Bayesian Numerical Integration with Neural Networks.","2023","CoRR","Katharina Ott, Michael Tiemann, Philipp Hennig, François-Xavier Briol","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2305-13248","Bayesian probabilistic numerical methods for numerical integration offer significant advantages over their non-Bayesian counterparts: they can encode prior information about the integrand, and can quantify uncertainty over estimates of an integral. However, the most popular algorithm in this class, Bayesian quadrature, is based on Gaussian process models and is therefore associated with a high computational cost. To improve scalability, we propose an alternative approach based on Bayesian neural networks which we call Bayesian Stein networks. The key ingredients are a neural network architecture based on Stein operators, and an approximation of the Bayesian posterior based on the Laplace approximation. We show that this leads to orders of magnitude speed-ups on the popular Genz functions benchmark, and on challenging problems arising in the Bayesian analysis of dynamical systems, and the prediction of energy production for a large-scale wind farm."
"Baysian numerical integration with neural networks.","2023","UAI","Katharina Ott, Michael Tiemann, Philipp Hennig, François-Xavier Briol","Conference and Workshop Papers","https://dblp.org/rec/conf/uai/OttTHB23",""
"Benchmarking Neural Network Training Algorithms.","2023","CoRR","George E. Dahl, Frank Schneider 0001, Zachary Nado, Naman Agarwal, Chandramouli Shama Sastry, Philipp Hennig, Sourabh Medapati, Runa Eschenhagen, Priya Kasimbeg, Daniel Suo, Juhan Bae, Justin Gilmer, Abel L. Peirson, Bilal Khan, Rohan Anil, Mike Rabbat, Shankar Krishnan, Daniel Snider, Ehsan Amid, Kongtao Chen, Chris J. Maddison, Rakshith Vasudev, Michal Badura, Ankush Garg, Peter Mattson","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2306-07179","Training algorithms, broadly construed, are an essential part of every deep learning pipeline. Training algorithm improvements that speed up training across a wide variety of workloads (e.g., better update rules, tuning protocols, learning rate schedules, or data selection schemes) could save time, save computational resources, and lead to better, more accurate, models. Unfortunately, as a community, we are currently unable to reliably identify training algorithm improvements, or even determine the state-of-the-art training algorithm. In this work, using concrete experiments, we argue that real progress in speeding up training requires new benchmarks that resolve three basic challenges faced by empirical comparisons of training algorithms: (1) how to decide when training is complete and precisely measure training time, (2) how to handle the sensitivity of measurements to exact workload details, and (3) how to fairly compare algorithms that require hyperparameter tuning. In order to address these challenges, we introduce a new, competitive, time-to-result benchmark using multiple workloads running on fixed hardware, the AlgoPerf: Training Algorithms benchmark. Our benchmark includes a set of workload variants that make it possible to detect benchmark submissions that are more robust to workload changes than current widely-used methods. Finally, we evaluate baseline submissions constructed using various optimizers that represent current practice, as well as other optimizers that have recently received attention in the literature. These baseline results collectively demonstrate the feasibility of our benchmark, show that non-trivial gaps between methods exist, and set a provisional state-of-the-art for future benchmark submissions to try and surpass."
"Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures.","2023","NeurIPS","Runa Eschenhagen, Alexander Immer, Richard E. Turner, Frank Schneider 0001, Philipp Hennig","Conference and Workshop Papers","https://dblp.org/rec/conf/nips/EschenhagenIT0H23",""
"Parallel-in-Time Probabilistic Numerical ODE Solvers.","2023","CoRR","Nathanael Bosch, Adrien Corenflos, Fatemeh Yaghoobi, Filip Tronarp, Philipp Hennig, Simo Särkkä","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2310-01145","Probabilistic numerical solvers for ordinary differential equations (ODEs) treat the numerical simulation of dynamical systems as problems of Bayesian state estimation. Aside from producing posterior distributions over ODE solutions and thereby quantifying the numerical approximation error of the method itself, one less-often noted advantage of this formalism is the algorithmic flexibility gained by formulating numerical simulation in the framework of Bayesian filtering and smoothing. In this paper, we leverage this flexibility and build on the time-parallel formulation of iterated extended Kalman smoothers to formulate a parallel-in-time probabilistic numerical ODE solver. Instead of simulating the dynamical system sequentially in time, as done by current probabilistic solvers, the proposed method processes all time steps in parallel and thereby reduces the span cost from linear to logarithmic in the number of time steps. We demonstrate the effectiveness of our approach on a variety of ODEs and compare it to a range of both classic and probabilistic numerical ODE solvers."
"Probabilistic Exponential Integrators.","2023","NeurIPS","Nathanael Bosch, Philipp Hennig, Filip Tronarp","Conference and Workshop Papers","https://dblp.org/rec/conf/nips/BoschHT23","Probabilistic solvers provide a flexible and efficient framework for simulation, uncertainty quantification, and inference in dynamical systems. However, like standard solvers, they suffer performance penalties for certain stiff systems, where small steps are required not for reasons of numerical accuracy but for the sake of stability. This issue is greatly alleviated in semi-linear problems by the probabilistic exponential integrators developed in this paper. By including the fast, linear dynamics in the prior, we arrive at a class of probabilistic integrators with favorable properties. Namely, they are proven to be L-stable, and in a certain case reduce to a classic exponential integrator -- with the added benefit of providing a probabilistic account of the numerical error. The method is also generalized to arbitrary non-linear systems by imposing piece-wise semi-linearity on the prior via Jacobians of the vector field at the previous estimates, resulting in probabilistic exponential Rosenbrock methods. We evaluate the proposed methods on multiple stiff differential equations and demonstrate their improved stability and efficiency over established probabilistic solvers. The present contribution thus expands the range of problems that can be effectively tackled within probabilistic numerics."
"Sample Path Regularity of Gaussian Processes from the Covariance Kernel.","2023","CoRR","Nathaël Da Costa, Marvin Pförtner, Lancelot Da Costa, Philipp Hennig","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2312-14886",""
"The Geometry of Neural Nets' Parameter Spaces Under Reparametrization.","2023","NeurIPS","Agustinus Kristiadi, Felix Dangel, Philipp Hennig","Conference and Workshop Papers","https://dblp.org/rec/conf/nips/KristiadiDH23","Model reparametrization, which follows the change-of-variable rule of calculus, is a popular way to improve the training of neural nets. But it can also be problematic since it can induce inconsistencies in, e.g., Hessian-based flatness measures, optimization trajectories, and modes of probability densities. This complicates downstream analyses: e.g. one cannot definitively relate flatness with generalization since arbitrary reparametrization changes their relationship. In this work, we study the invariance of neural nets under reparametrization from the perspective of Riemannian geometry. From this point of view, invariance is an inherent property of any neural net if one explicitly represents the metric and uses the correct associated transformation rules. This is important since although the metric is always present, it is often implicitly assumed as identity, and thus dropped from the notation, then lost under reparametrization. We discuss implications for measuring the flatness of minima, optimization, and for probability-density maximization. Finally, we explore some interesting directions where invariance is useful."
"The Rank-Reduced Kalman Filter: Approximate Dynamical-Low-Rank Filtering In High Dimensions.","2023","NeurIPS","Jonathan Schmidt, Philipp Hennig, Jörg Nick, Filip Tronarp","Conference and Workshop Papers","https://dblp.org/rec/conf/nips/SchmidtHNT23",""
"Uncertainty and Structure in Neural Ordinary Differential Equations.","2023","CoRR","Katharina Ott, Michael Tiemann, Philipp Hennig","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2305-13290","Neural ordinary differential equations (ODEs) are an emerging class of deep learning models for dynamical systems. They are particularly useful for learning an ODE vector field from observed trajectories (i.e., inverse problems). We here consider aspects of these models relevant for their application in science and engineering. Scientific predictions generally require structured uncertainty estimates. As a first contribution, we show that basic and lightweight Bayesian deep learning techniques like the Laplace approximation can be applied to neural ODEs to yield structured and meaningful uncertainty quantification. But, in the scientific domain, available information often goes beyond raw trajectories, and also includes mechanistic knowledge, e.g., in the form of conservation laws. We explore how mechanistic knowledge and uncertainty quantification interact on two recently proposed neural ODE frameworks - symplectic neural ODEs and physical models augmented with neural ODEs. In particular, uncertainty reflects the effect of mechanistic information more directly than the predictive power of the trained model could. And vice versa, structure can improve the extrapolation abilities of neural ODEs, a fact that can be best assessed in practice through uncertainty estimates. Our experimental analysis demonstrates the effectiveness of the Laplace approach on both low dimensional ODE problems and a high dimensional partial differential equation."
