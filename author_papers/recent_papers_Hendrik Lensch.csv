"title","year","venue","authors","type","url","abstract"
"Disentangling representations of retinal images with generative models.","2024","CoRR","Sarah Müller, Lisa M. Koch, Hendrik P. A. Lensch, Philipp Berens","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2402-19186","Retinal fundus images play a crucial role in the early detection of eye diseases. However, the impact of technical factors on these images can pose challenges for reliable AI applications in ophthalmology. For example, large fundus cohorts are often confounded by factors like camera type, bearing the risk of learning shortcuts rather than the causal relationships behind the image generation process. Here, we introduce a population model for retinal fundus images that effectively disentangles patient attributes from camera effects, enabling controllable and highly realistic image generation. To achieve this, we propose a disentanglement loss based on distance correlation. Through qualitative and quantitative analyses, we show that our models encode desired information in disentangled subspaces and enable controllable image generation based on the learned subspaces, demonstrating the effectiveness of our disentanglement loss. The project's code is publicly available: https://github.com/berenslab/disentangling-retinal-images."
"Dualad: Disentangling the Dynamic and Static World for End-to-End Driving.","2024","CVPR","Simon Doll, Niklas Hanselmann, Lukas Schneider, Richard Schulz, Marius Cordts, Markus Enzweiler, Hendrik P. A. Lensch","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/DollHSSCEL24","State-of-the-art approaches for autonomous driving integrate multiple sub-tasks of the overall driving task into a single pipeline that can be trained in an end-to-end fashion by passing latent representations between the different modules. In contrast to previous approaches that rely on a unified grid to represent the belief state of the scene, we propose dedicated representations to disentangle dynamic agents and static scene elements. This allows us to explicitly compensate for the effect of both ego and object motion between consecutive time steps and to flexibly propagate the belief state through time. Furthermore, dynamic objects can not only attend to the input camera images, but also directly benefit from the inferred static scene structure via a novel dynamic-static cross-attention. Extensive experiments on the challenging nuScenes benchmark demonstrate the benefits of the proposed dual-stream design, especially for modelling highly dynamic agents in the scene, and highlight the improved temporal consistency of our approach. Our method titled DualAD not only outperforms independently trained single-task networks, but also improves over previous state-of-the-art end-to-end models by a large margin on all tasks along the functional chain of driving."
"Iterative Cluster Harvesting for Wafer Map Defect Patterns.","2024","CoRR","Alina Pleli, Simon Baeuerle, Michel Janus, Jonas Barth, Ralf Mikut, Hendrik P. A. Lensch","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2404-15436","Unsupervised clustering of wafer map defect patterns is challenging because the appearance of certain defect patterns varies significantly. This includes changing shape, location, density, and rotation of the defect area on the wafer. We present a harvesting approach, which can cluster even challenging defect patterns of wafer maps well. Our approach makes use of a well-known, three-step procedure: feature extraction, dimension reduction, and clustering. The novelty in our approach lies in repeating dimensionality reduction and clustering iteratively while filtering out one cluster per iteration according to its silhouette score. This method leads to an improvement of clustering performance in general and is especially useful for difficult defect patterns. The low computational effort allows for a quick assessment of large datasets and can be used to support manual labeling efforts. We benchmark against related approaches from the literature and show improved results on a real-world industrial dataset."
"RISE-SDF: a Relightable Information-Shared Signed Distance Field for Glossy Object Inverse Rendering.","2024","CoRR","Deheng Zhang, Jingyu Wang, Shaofei Wang, Marko Mihajlovic, Sergey Prokudin, Hendrik P. A. Lensch, Siyu Tang 0001","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2409-20140","In this paper, we propose a novel end-to-end relightable neural inverse rendering system that achieves high-quality reconstruction of geometry and material properties, thus enabling high-quality relighting. The cornerstone of our method is a two-stage approach for learning a better factorization of scene parameters. In the first stage, we develop a reflection-aware radiance field using a neural signed distance field (SDF) as the geometry representation and deploy an MLP (multilayer perceptron) to estimate indirect illumination. In the second stage, we introduce a novel information-sharing network structure to jointly learn the radiance field and the physically based factorization of the scene. For the physically based factorization, to reduce the noise caused by Monte Carlo sampling, we apply a split-sum approximation with a simplified Disney BRDF and cube mipmap as the environment light representation. In the relighting phase, to enhance the quality of indirect illumination, we propose a second split-sum algorithm to trace secondary rays under the split-sum rendering framework. Furthermore, there is no dataset or protocol available to quantitatively evaluate the inverse rendering performance for glossy objects. To assess the quality of material reconstruction and relighting, we have created a new dataset with ground truth BRDF parameters and relighting results. Our experiments demonstrate that our algorithm achieves state-of-the-art performance in inverse rendering and relighting, with particularly strong results in the reconstruction of highly reflective objects."
"S.T.A.R.-Track: Latent Motion Models for End-to-End 3D Object Tracking With Adaptive Spatio-Temporal Appearance Representations.","2024","IEEE Robotics Autom. Lett.","Simon Doll, Niklas Hanselmann, Lukas Schneider, Richard Schulz, Markus Enzweiler, Hendrik P. A. Lensch","Journal Articles","https://dblp.org/rec/journals/ral/DollHSSEL24","Following the tracking-by-attention paradigm, this letter introduces an object-centric, transformer-based framework for tracking in 3D. Traditional model-based tracking approaches incorporate the geometric effect of object- and ego motion between frames with a geometric motion model. Inspired by this, we propose STAR-TRACK, which uses a novel latent motion model (LMM) to additionally adjust object queries to account for changes in viewing direction and lighting conditions directly in the latent space, while still modeling the geometric motion explicitly. Combined with a novel learnable track embedding that aids in modeling the existence probability of tracks, this results in a generic tracking framework that can be integrated with any query-based detector. Extensive experiments on the nuScenes benchmark demonstrate the benefits of our approach, showing state-of-the-art (SOTA) performance for DETR3D-based trackers while drastically reducing the number of identity switches of tracks at the same time."
"SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild.","2024","CVPR","Andreas Engelhardt, Amit Raj, Mark Boss, Yunzhi Zhang, Abhishek Kar, Yuanzhen Li, Deqing Sun, Ricardo Martin-Brualla, Jonathan T. Barron, Hendrik P. A. Lensch, Varun Jampani","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/EngelhardtRBZKL24","We present SHINOBI, an end-to-end frameworkfor the re-construction of shape, material, and illumination from object images captured with varying lighting, pose, and background. Inverse rendering of an object based on unconstrained image collections is a long-standing challenge in computer vision and graphics and requires a joint optimization over shape, radiance, and pose. We show that an implicit shape repre-sentation based on a multi-resolution hash encoding enables faster and robust shape reconstruction with joint camera alignment optimization that outperforms prior work. Further, to enable the editing of illumination and object reflectance (i.e. material) we jointly optimize BRDF and illumination to-gether with the object's shape. Our method is class-agnostic and works on in-the-wild image collections of objects to produce relightable 3D assets for several use cases such as AR/VR, movies, games, etc."
"SIGNeRF: Scene Integrated Generation for Neural Radiance Fields.","2024","CVPR","Jan-Niklas Dihlmann, Andreas Engelhardt, Hendrik P. A. Lensch","Conference and Workshop Papers","https://dblp.org/rec/conf/cvpr/DihlmannEL24","Advances in image diffusion models have recently led to notable improvements in the generation of high-quality images. In combination with Neural Radiance Fields (NeRFs), they enabled new opportunities in 3D generation. However, most generative 3D approaches are object-centric and applying them to editing existing photorealistic scenes is not trivial. We propose SIGNeRF, a novel approach for fast and controllable NeRF scene editing and scene-integrated object generation. A new generative update strategy ensures 3D consistency across the edited images, without requiring iterative optimization. We find that depth-conditioned diffusion models inherently possess the capability to generate 3D consistent views by requesting a grid of images instead of single views. Based on these insights, we introduce a multi-view reference sheet of modified images. Our method updates an image collection consistently based on the ref-erence sheet and refines the original NeRF with the newly generated image set in one go. By exploiting the depth conditioning mechanism of the image diffusion model, we gain fine control over the spatial location of the edit and enforce shape guidance by a selected region or an external mesh."
"Subsurface Scattering for 3D Gaussian Splatting.","2024","CoRR","Jan-Niklas Dihlmann, Arjun Majumdar, Andreas Engelhardt, Raphael Braun, Hendrik P. A. Lensch","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2408-12282",""
"An Enhanced Synthetic Cystoscopic Environment for Use in Monocular Depth Estimation.","2023","EMBC","Peter Somers, Mario Deutschmann, Simon Holdenried-Krafft, Samuel Tovey, Johannes Schüle, Carina Veil, Valese Aslani, Oliver Sawodny, Hendrik P. A. Lensch, Cristina Tarín","Conference and Workshop Papers","https://dblp.org/rec/conf/embc/SomersDHTSVASLT23","As technology advances and sensing devices improve, it is becoming more and more pertinent to ensure accurate positioning of these devices, especially within the human body. This task remains particularly difficult during manual, minimally invasive surgeries such as cystoscopies where only a monocular, endoscopic camera image is available and driven by hand. Tracking relies on optical localization methods, however, existing classical options do not function well in such a dynamic, non-rigid environment. This work builds on recent works using neural networks to learn a supervised depth estimation from synthetically generated images and, in a second training step, use adversarial training to then apply the network on real images. The improvements made to a synthetic cystoscopic environment are done in such a way to reduce the domain gap between the synthetic images and the real ones. Training with the proposed enhanced environment shows distinct improvements over previously published work when applied to real test images."
"CANDID: Correspondence AligNment for Deep-burst Image Denoising.","2023","CRV","Arijit Mallick, Raphael Braun, Hendrik P. A. Lensch","Conference and Workshop Papers","https://dblp.org/rec/conf/crv/MallickBL23","With the advent of mobile phone photography and point-and-shoot cameras, deep-burst imaging is widely used for a number of photographic effects such as depth of field, super-resolution, motion deblurring, and image denoising. In this work, we propose to solve the problem of deep-burst image denoising by including an optical flow-based correspondence estimation module which aligns all the input burst images with respect to a reference frame. In order to deal with varying noise levels the individual burst images are pre-filtered with different settings. Exploiting the established correspondences one network block predicts a pixel-wise spatially-varying filter kernel to smooth each image in the original and prefiltered bursts before fusing all images to generate the final denoised output. The resulting pipeline achieves state-of-the-art results by combining all available information provided by the burst."
"Dual-Query Multiple Instance Learning for Dynamic Meta-Embedding based Tumor Classification.","2023","BMVC","Simon Holdenried-Krafft, Peter Somers, Ivonne Montes-Mojarro, Diana Silimon, Cristina Tarín, Falko Fend, Hendrik P. A. Lensch","Conference and Workshop Papers","https://dblp.org/rec/conf/bmvc/Holdenried-Krafft23","Whole slide image (WSI) assessment is a challenging and crucial step in cancer diagnosis and treatment planning. WSIs require high magnifications to facilitate sub-cellular analysis. Precise annotations for patch- or even pixel-level classifications in the context of gigapixel WSIs are tedious to acquire and require domain experts. Coarse-grained labels, on the other hand, are easily accessible, which makes WSI classification an ideal use case for multiple instance learning (MIL). In our work, we propose a novel embedding-based Dual-Query MIL pipeline (DQ-MIL). We contribute to both the embedding and aggregation steps. Since all-purpose visual feature representations are not yet available, embedding models are currently limited in terms of generalizability. With our work, we explore the potential of dynamic meta-embedding based on cutting-edge self-supervised pre-trained models in the context of MIL. Moreover, we propose a new MIL architecture capable of combining MIL-attention with correlated self-attention. The Dual-Query Perceiver design of our approach allows us to leverage the concept of self-distillation and to combine the advantages of a small model in the context of a low data regime with the rich feature representation of a larger model. We demonstrate the superior performance of our approach on three histopathological datasets, where we show improvement of up to 10% over state-of-the-art approaches."
"Entwicklung eines automatischen Monitoringsystems für die Geburtsüberwachung bei Sauen.","2023","GIL Jahrestagung","Martin Wutke, Clara Lensches, Jan Hendrik Witte, Johann Gerberding, Marc-Alexander Lieboldt, Imke Traulsen","Conference and Workshop Papers","https://dblp.org/rec/conf/gil/WutkeLWGLT23",""
"GGNN: Graph-Based GPU Nearest Neighbor Search.","2023","IEEE Trans. Big Data","Fabian Groh, Lukas Ruppert, Patrick Wieschollek, Hendrik P. A. Lensch","Journal Articles","https://dblp.org/rec/journals/tbd/GrohRWL23","Approximate nearest neighbor (ANN) search in high dimensions is an integral part of several computer vision systems and gains importance in deep learning with explicit memory representations. Since PQT (Wieschollek et al., 2016), FAISS (Johnson et al., 2021), and SONG (Zhao et al., 2020) started to leverage the massive parallelism offered by GPUs, GPU-based implementations are a crucial resource for today’s state-of-the-art ANN methods. While most of these methods allow for faster queries, less emphasis is devoted to accelerating the construction of the underlying index structures. In this paper, we propose a novel GPU-friendly search structure based on nearest neighbor graphs and information propagation on graphs. Our method is designed to take advantage of GPU architectures to accelerate the hierarchical construction of the index structure and for performing the query. Empirical evaluation shows that GGNN significantly surpasses the state-of-the-art CPU- and GPU-based systems in terms of build-time, accuracy and search speed."
"S.T.A.R.-Track: Latent Motion Models for End-to-End 3D Object Tracking with Adaptive Spatio-Temporal Appearance Representations.","2023","CoRR","Simon Doll, Niklas Hanselmann, Lukas Schneider, Richard Schulz, Markus Enzweiler, Hendrik P. A. Lensch","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2306-17602","Following the tracking-by-attention paradigm, this letter introduces an object-centric, transformer-based framework for tracking in 3D. Traditional model-based tracking approaches incorporate the geometric effect of object- and ego motion between frames with a geometric motion model. Inspired by this, we propose STAR-TRACK, which uses a novel latent motion model (LMM) to additionally adjust object queries to account for changes in viewing direction and lighting conditions directly in the latent space, while still modeling the geometric motion explicitly. Combined with a novel learnable track embedding that aids in modeling the existence probability of tracks, this results in a generic tracking framework that can be integrated with any query-based detector. Extensive experiments on the nuScenes benchmark demonstrate the benefits of our approach, showing state-of-the-art (SOTA) performance for DETR3D-based trackers while drastically reducing the number of identity switches of tracks at the same time."
"ViPE: Visualise Pretty-much Everything.","2023","EMNLP","Hassan Shahmohammadi, Adhiraj Ghosh, Hendrik P. A. Lensch","Conference and Workshop Papers","https://dblp.org/rec/conf/emnlp/ShahmohammadiGL23","Figurative and non-literal expressions are profoundly integrated in human communication. Visualising such expressions allow us to convey our creative thoughts, and evoke nuanced emotions. Recent text-to-image models like Stable Diffusion, on the other hand, struggle to depict non-literal expressions. Recent works primarily deal with this issue by compiling humanly annotated datasets on a small scale, which not only demands specialised expertise but also proves highly inefficient. To address this issue, we introduce ViPE: Visualise Pretty-much Everything. ViPE offers a series of lightweight and robust language models that have been trained on a large-scale set of lyrics with noisy visual descriptions that represent their implicit meaning. The synthetic visual descriptions are generated by GPT3.5 relying on neither human annotations nor images. ViPE effectively expresses any arbitrary piece of text into a visualisable description, enabling meaningful and high-quality image generation. We provide compelling evidence that ViPE is more robust than GPT3.5 in synthesising visual elaborations. ViPE also exhibits an understanding of figurative expressions comparable to human experts, providing a powerful and open-source backbone to many downstream applications such as music video and caption generation."
"Zero-Shot Translation of Attention Patterns in VQA Models to Natural Language.","2023","DAGM","Leonard Salewski, A. Sophia Koepke, Hendrik P. A. Lensch, Zeynep Akata","Conference and Workshop Papers","https://dblp.org/rec/conf/dagm/SalewskiKLA23","Converting a model's internals to text can yield human-understandable insights about the model. Inspired by the recent success of training-free approaches for image captioning, we propose ZS-A2T, a zero-shot framework that translates the transformer attention of a given model into natural language without requiring any training. We consider this in the context of Visual Question Answering (VQA). ZS-A2T builds on a pre-trained large language model (LLM), which receives a task prompt, question, and predicted answer, as inputs. The LLM is guided to select tokens which describe the regions in the input image that the VQA model attended to. Crucially, we determine this similarity by exploiting the text-image matching capabilities of the underlying VQA model. Our framework does not require any training and allows the drop-in replacement of different guiding sources (e.g. attribution instead of attention maps), or language models. We evaluate this novel task on textual explanation datasets for VQA, giving state-of-the-art performances for the zero-shot setting on GQA-REX and VQA-X. Our code is available at: https://github.com/ExplainableML/ZS-A2T."
