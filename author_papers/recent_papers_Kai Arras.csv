"title","year","venue","authors","type","url","abstract"
"STARK: A Unified Framework for Strongly Coupled Simulation of Rigid and Deformable Bodies with Frictional Contact.","2024","ICRA","José Antonio Fernández-Fernández, Ralph Lange, Stefan Laible, Kai O. Arras, Jan Bender","Conference and Workshop Papers","https://dblp.org/rec/conf/icra/Fernandez-Fernandez24",""
"The Child Factor in Child-Robot Interaction: Discovering the Impact of Developmental Stage and Individual Characteristics.","2024","Int. J. Soc. Robotics","Irina Rudenko, Andrey Rudenko, Achim J. Lilienthal, Kai O. Arras, Barbara Bruno","Journal Articles","https://dblp.org/rec/journals/ijsr/RudenkoRLAB24","Social robots, owing to their embodied physical presence in human spaces and the ability to directly interact with the users and their environment, have a great potential to support children in various activities in education, healthcare and daily life. Child–Robot Interaction (CRI), as any domain involving children, inevitably faces the major challenge of designing generalized strategies to work with unique, turbulent and very diverse individuals. Addressing this challenging endeavor requires to combine the standpoint of the robot-centered perspective, i.e. what robots technically can and are best positioned to do, with that of the child-centered perspective, i.e. what children may gain from the robot and how the robot should act to best support them in reaching the goals of the interaction. This article aims to help researchers bridge the two perspectives and proposes to address the development of CRI scenarios with insights from child psychology and child development theories. To that end, we review the outcomes of the CRI studies, outline common trends and challenges, and identify two key factors from child psychology that impact child-robot interactions, especially in a long-term perspective: developmental stage and individual characteristics. For both of them we discuss prospective experiment designs which support building naturally engaging and sustainable interactions."
"The ILIAD Safety Stack: Human-Aware Infrastructure-Free Navigation of Industrial Mobile Robots.","2024","IEEE Robotics Autom. Mag.","Sergi Molina Mellado, Anna Mannucci, Martin Magnusson 0002, Daniel Adolfsson, Henrik Andreasson, Mazin Hamad, Saeed Abdolshah, Ravi Teja Chadalavada, Luigi Palmieri, Timm Linder, Chittaranjan Srinivas Swaminathan, Tomasz Piotr Kucner, Marc Hanheide, Manuel Fernández-Carmona, Grzegorz Cielniak, Tom Duckett, Federico Pecora, Simon Bokesand, Kai O. Arras, Sami Haddadin, Achim J. Lilienthal","Journal Articles","https://dblp.org/rec/journals/ram/MelladoMMAAHACPLSKHFCDP24","Current intralogistics services require keeping up with e-commerce demands, reducing delivery times and waste, and increasing overall flexibility. As a consequence, the use of automated guided vehicles (AGVs) and, more recently, autonomous mobile robots (AMRs) for logistics operations is steadily increasing."
"The e-Bike motor assembly: Towards advanced robotic manipulation for flexible manufacturing.","2024","Robotics Comput. Integr. Manuf.","Leonel Rozo, Andras G. Kupcsik, Philipp Schillinger, Meng Guo 0002, Robert Krug 0003, Niels van Duijkeren, Markus Spies, Patrick Kesper, Sabrina Hoppe, Hanna Ziesche, Mathias Bürger, Kai O. Arras","Journal Articles","https://dblp.org/rec/journals/rcim/RozoKSGKDSKHZBA24","Robotic manipulation is currently undergoing a profound paradigm shift due to the increasing needs for flexible manufacturing systems, and at the same time, because of the advances in enabling technologies such as sensing, learning, optimization, and hardware. This demands for robots that can observe and reason about their workspace, and that are skillfull enough to complete various assembly processes in weakly-structured settings. Moreover, it remains a great challenge to enable operators for teaching robots on-site, while managing the inherent complexity of perception, control, motion planning and reaction to unexpected situations. Motivated by real-world industrial applications, this paper demonstrates the potential of such a paradigm shift in robotics on the industrial case of an e-Bike motor assembly. The paper presents a concept for teaching and programming adaptive robots on-site and demonstrates their potential for the named applications. The framework includes: (i) a method to teach perception systems onsite in a self-supervised manner, (ii) a general representation of object-centric motion skills and force-sensitive assembly skills, both learned from demonstration, (iii) a sequencing approach that exploits a human-designed plan to perform complex tasks, and (iv) a system solution for adapting and optimizing skills online. The aforementioned components are interfaced through a four-layer software architecture that makes our framework a tangible industrial technology. To demonstrate the generality of the proposed framework, we provide, in addition to the motivating e-Bike motor assembly, a further case study on dense box packing for logistics automation."
"Advantages of Multimodal versus Verbal-Only Robot-to-Human Communication with an Anthropomorphic Robotic Mock Driver.","2023","RO-MAN","Tim Schreiter, Lucas Morillo-Méndez, Ravi T. Chadalavada, Andrey Rudenko, Erik Billing, Martin Magnusson 0002, Kai O. Arras, Achim J. Lilienthal","Conference and Workshop Papers","https://dblp.org/rec/conf/ro-man/SchreiterMCRBMAL23","Robots are increasingly used in shared environments with humans, making effective communication a necessity for successful human-robot interaction. In our work, we study a crucial component: active communication of robot intent. Here, we present an anthropomorphic solution where a humanoid robot communicates the intent of its host robot acting as an “Anthropomorphic Robotic Mock Driver” (ARMoD). We evaluate this approach in two experiments in which participants work alongside a mobile robot on various tasks, while the ARMoD communicates a need for human attention, when required, or gives instructions to collaborate on a joint task. The experiments feature two interaction styles of the ARMoD: a verbal-only mode using only speech and a multimodal mode, additionally including robotic gaze and pointing gestures to support communication and register intent in space. Our results show that the multimodal interaction style, including head movements and eye gaze as well as pointing gestures, leads to more natural fixation behavior. Participants naturally identified and fixated longer on the areas relevant for intent communication, and reacted faster to instructions in collaborative tasks. Our research further indicates that the ARMoD intent communication improves engagement and social interaction with mobile robots in workplace settings."
"CLiFF-LHMP: Using Spatial Dynamics Patterns for Long- Term Human Motion Prediction.","2023","IROS","Yufei Zhu, Andrey Rudenko, Tomasz Piotr Kucner, Luigi Palmieri, Kai O. Arras, Achim J. Lilienthal, Martin Magnusson 0002","Conference and Workshop Papers","https://dblp.org/rec/conf/iros/ZhuRKPAL023","Human motion prediction is important for mobile service robots and intelligent vehicles to operate safely and smoothly around people. The more accurate predictions are, particularly over extended periods of time, the better a system can, e.g., assess collision risks and plan ahead. In this paper, we propose to exploit maps of dynamics (MoDs, a class of general representations of place-dependent spatial motion patterns, learned from prior observations) for long-term human motion prediction (LHMP). We present a new MoD-informed human motion prediction approach, named CLiFF-LHMP, which is data efficient, explainable, and insensitive to errors from an upstream tracking system. Our approach uses CLiFF -map, a specific MoD trained with human motion data recorded in the same environment. We bias a constant velocity prediction with samples from the CLiFF-map to generate multi-modal trajectory predictions. In two public datasets we show that this algorithm outperforms the state of the art for predictions over very extended periods of time, achieving 45 % more accurate prediction performance at 50s compared to the baseline."
"CLiFF-LHMP: Using Spatial Dynamics Patterns for Long-Term Human Motion Prediction.","2023","CoRR","Yufei Zhu, Andrey Rudenko, Tomasz Piotr Kucner, Luigi Palmieri, Kai O. Arras, Achim J. Lilienthal, Martin Magnusson 0002","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2309-07066","Human motion prediction is important for mobile service robots and intelligent vehicles to operate safely and smoothly around people. The more accurate predictions are, particularly over extended periods of time, the better a system can, e.g., assess collision risks and plan ahead. In this paper, we propose to exploit maps of dynamics (MoDs, a class of general representations of place-dependent spatial motion patterns, learned from prior observations) for long-term human motion prediction (LHMP). We present a new MoD-informed human motion prediction approach, named CLiFF-LHMP, which is data efficient, explainable, and insensitive to errors from an upstream tracking system. Our approach uses CLiFF -map, a specific MoD trained with human motion data recorded in the same environment. We bias a constant velocity prediction with samples from the CLiFF-map to generate multi-modal trajectory predictions. In two public datasets we show that this algorithm outperforms the state of the art for predictions over very extended periods of time, achieving 45 % more accurate prediction performance at 50s compared to the baseline."
"Proactive Model Predictive Control with Multi-Modal Human Motion Prediction in Cluttered Dynamic Environments.","2023","IROS","Lukas Heuer, Luigi Palmieri, Andrey Rudenko, Anna Mannucci, Martin Magnusson 0002, Kai O. Arras","Conference and Workshop Papers","https://dblp.org/rec/conf/iros/HeuerPRM0A23","For robots navigating in dynamic environments, exploiting and understanding uncertain human motion prediction is key to generate efficient, safe and legible actions. The robot may perform poorly and cause hindrances if it does not reason over possible, multi-modal future social interactions. With the goal of enhancing autonomous navigation in cluttered environments, we propose a novel formulation for nonlinear model predictive control including multi-modal predictions of human motion. As a result, our approach leads to less conservative, smooth and intuitive human-aware navigation with reduced risk of collisions, and shows a good balance between task efficiency, collision avoidance and human comfort. To show its effectiveness, we compare our approach against the state of the art in crowded simulated environments, and with real-world human motion data from the THOR dataset. This comparison shows that we are able to improve task efficiency, keep a larger distance to humans and significantly reduce the collision time, when navigating in cluttered dynamic environ-ments. Furthermore, the method is shown to work robustly with different state-of-the-art human motion predictors."
"Semantically Informed MPC for Context-Aware Robot Exploration.","2023","IROS","Yash Goel, Narunas Vaskevicius, Luigi Palmieri, Nived Chebrolu, Kai O. Arras, Cyrill Stachniss","Conference and Workshop Papers","https://dblp.org/rec/conf/iros/GoelVPCAS23","We investigate the task of object goal navigation in unknown environments where a target object is given as a semantic label (e.g. find a couch). This task is challenging as it requires the robot to consider the semantic context in diverse settings (e.g. TVs are often nearby couches). Most of the prior work tackles this problem under the assumption of a discrete action policy whereas we present an approach with continuous control which brings it closer to real world applications. In this paper, we use information-theoretic model predictive control on dense cost maps to bring object goal navigation closer to real robots with kinodynamic constraints. We propose a deep neural network framework to learn cost maps that encode semantic context and guide the robot towards the target object. We also present a novel way of fusing mid-level visual representations in our architecture to provide additional semantic cues for cost map prediction. The experiments show that our method leads to more efficient and accurate goal navigation with higher quality paths than the reported baselines. The results also indicate the importance of mid-level representations for navigation by improving the success rate by 8 percentage points."
"THÖR-Magni: Comparative Analysis of Deep Learning Models for Role-conditioned Human Mtion Prediction.","2023","ICCV","Tiago Rodrigues de Almeida 0001, Andrey Rudenko, Tim Schreiter, Yufei Zhu, Eduardo Gutiérrez-Maestro, Lucas Morillo-Méndez, Tomasz Piotr Kucner, Óscar Martínez Mozos, Martin Magnusson 0002, Luigi Palmieri, Kai O. Arras, Achim J. Lilienthal","Conference and Workshop Papers","https://dblp.org/rec/conf/iccvw/0001RSZGMKM0PAL23","Autonomous systems, that need to operate in human environments and interact with the users, rely on understanding and anticipating human activity and motion. Among the many factors which influence human motion, semantic attributes, such as the roles and ongoing activities of the detected people, provide a powerful cue on their future motion, actions, and intentions. In this work we adapt several popular deep learning models for trajectory prediction with labels corresponding to the roles of the people. To this end we use the novel THÖR-Magni dataset, which captures human activity in industrial settings and includes the relevant semantic labels for people who navigate complex environments, interact with objects and robots, work alone and in groups. In qualitative and quantitative experiments we show that the role-conditioned LSTM, Transformer, GAN and VAE methods can effectively incorporate the semantic categories, better capture the underlying input distribution and therefore produce more accurate motion predictions in terms of Top-K ADE/FDE and log-likelihood metrics."
"The e-Bike Motor Assembly: Towards Advanced Robotic Manipulation for Flexible Manufacturing.","2023","CoRR","Leonel Rozo, Andras G. Kupcsik, Philipp Schillinger, Meng Guo 0002, Robert Krug 0003, Niels van Duijkeren, Markus Spies, Patrick Kesper, Sabrina Hoppe, Hanna Ziesche, Mathias Bürger, Kai O. Arras","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2304-10595","Robotic manipulation is currently undergoing a profound paradigm shift due to the increasing needs for flexible manufacturing systems, and at the same time, because of the advances in enabling technologies such as sensing, learning, optimization, and hardware. This demands for robots that can observe and reason about their workspace, and that are skillfull enough to complete various assembly processes in weakly-structured settings. Moreover, it remains a great challenge to enable operators for teaching robots on-site, while managing the inherent complexity of perception, control, motion planning and reaction to unexpected situations. Motivated by real-world industrial applications, this paper demonstrates the potential of such a paradigm shift in robotics on the industrial case of an e-Bike motor assembly. The paper presents a concept for teaching and programming adaptive robots on-site and demonstrates their potential for the named applications. The framework includes: (i) a method to teach perception systems onsite in a self-supervised manner, (ii) a general representation of object-centric motion skills and force-sensitive assembly skills, both learned from demonstration, (iii) a sequencing approach that exploits a human-designed plan to perform complex tasks, and (iv) a system solution for adapting and optimizing skills online. The aforementioned components are interfaced through a four-layer software architecture that makes our framework a tangible industrial technology. To demonstrate the generality of the proposed framework, we provide, in addition to the motivating e-Bike motor assembly, a further case study on dense box packing for logistics automation."
