"title","year","venue","authors","type","url","abstract"
"""We Are Visual Thinkers, Not Verbal Thinkers!"": A Thematic Analysis of How Professional Designers Use Generative AI Image Generation Tools.","2024","NordiCHI","Hyerim Park, Joscha Eirich, Andre Luckow, Michael Sedlmair","Conference and Workshop Papers","https://dblp.org/rec/conf/nordichi/ParkELS24",""
"A Systematic Review of Ability-diverse Collaboration through Ability-based Lens in HCI.","2024","CHI","Lan Xiao, Maryam Bandukda, Katrin Angerbauer, Weiyue Lin, Tigmanshu Bhatnagar, Michael Sedlmair, Catherine Holloway","Conference and Workshop Papers","https://dblp.org/rec/conf/chi/XiaoBALBSH24",""
"A study on the influence of situations on personal avatar characteristics.","2024","Vis. Comput. Ind. Biomed. Art","Natalie Hube, Melissa Reinelt, Kresimir Vidackovic, Michael Sedlmair","Journal Articles","https://dblp.org/rec/journals/vciba/HubeRVS24","AbstractAvatars play a key role in how persons interact within virtual environments, acting as the digital selves. There are many types of avatars, each serving the purpose of representing users or others in these immersive spaces. However, the optimal approach for these avatars remains unclear. Although consumer applications often use cartoon-like avatars, this trend is not as common in work settings. To gain a better understanding of the kinds of avatars people prefer, three studies were conducted involving both screen-based and virtual reality setups, looking into how social settings might affect the way people choose their avatars. Personalized avatars were created for 91 participants, including 71 employees in the automotive field and 20 participants not affiliated with the company. The research shows that work-type situations influence the chosen avatar. At the same time, a correlation between the type of display medium used to display the avatar or the person’s personality and their avatar choice was not found. Based on the findings, recommendations are made for future avatar representations in work environments and implications and research questions derived that can guide future research."
"ChoreoVis: Planning and Assessing Formations in Dance Choreographies.","2024","Comput. Graph. Forum","Samuel Beck, Nina Doerr, Kuno Kurzhals, Alexander Riedlinger, Fabian Schmierer, Michael Sedlmair, Steffen Koch 0001","Journal Articles","https://dblp.org/rec/journals/cgf/BeckDKRSSK24","Sports visualization has developed into an active research field over the last decades. Many approaches focus on analyzing movement data recorded from unstructured situations, such as soccer. For the analysis of choreographed activities like formation dancing, however, the goal differs, as dancers follow specific formations in coordinated movement trajectories. To date, little work exists on how visual analytics methods can support such choreographed performances. To fill this gap, we introduce a new visual approach for planning and assessing dance choreographies. In terms of planning choreographies, we contribute a web application with interactive authoring tools and views for the dancers' positions and orientations, movement trajectories, poses, dance floor utilization, and movement distances. For assessing dancers' real‐world movement trajectories, extracted by manual bounding box annotations, we developed a timeline showing aggregated trajectory deviations and a dance floor view for detailed trajectory comparison. Our approach was developed and evaluated in collaboration with dance instructors, showing that introducing visual analytics into this domain promises improvements in training efficiency for the future."
"ClustML: A measure of cluster pattern complexity in scatterplots learnt from human-labeled groupings.","2024","Inf. Vis.","Mostafa M. Hamza, Ehsan Ullah, Abdelkader Baggag, Halima Bensmail, Michael Sedlmair, Michaël Aupetit 0001","Journal Articles","https://dblp.org/rec/journals/ivs/HamzaUBBSA24","Visual quality measures (VQMs) are designed to support analysts by automatically detecting and quantifying patterns in visualizations. We propose a new VQM for visual grouping patterns in scatterplots, called ClustML, which is trained on previously collected human subject judgments. Our model encodes scatterplots in the parametric space of a Gaussian Mixture Model and uses a classifier trained on human judgment data to estimate the perceptual complexity of grouping patterns. The numbers of initial mixture components and final combined groups quantify visual cluster patterns in scatterplots. It improves on existing VQMs, first, by better estimating human judgments on two-Gaussian cluster patterns and, second, by giving higher accuracy when ranking general cluster patterns in scatterplots. We use it to analyze kinship data for genome-wide association studies, in which experts rely on the visual analysis of large sets of scatterplots. We make the benchmark datasets and the new VQM available for practical use and further improvements."
"Configuring augmented reality users: analysing YouTube commercials to understand industry expectations.","2024","Behav. Inf. Technol.","Ann-Kathrin Wortmeier, Aimée Sousa Calepso, Cordula Kropp, Michael Sedlmair, Daniel Weiskopf","Journal Articles","https://dblp.org/rec/journals/behaviourIT/WortmeierCKSW24","ABSTRACT Commercial videos are often used to familiarise potential buyers and users with new technologies and their possibilities. In addition, presenting visions of future applications is a way to configure users and define social worlds of technology use. We analyse 30 YouTube videos featuring augmented reality (AR) devices in industrial manufacturing and construction, to explore how these commercial videos situate AR technology and future users by showcasing techno-euphoric promises and imagined use cases. With a video analysis based on Grounded Theory and Situational Analysis, we untangle the promises of AR for manufacturing and construction work; second, we present two prevailing configurations of AR users: ‘experts in situ’ and ‘smart dummies’; and third, we discuss how YouTube videos put forward developmental expectations. In addition, we identify discrepancies between expectations and foreseeable requirements in construction work. Finally, our research could contribute to a more holistic understanding of workplaces and socially robust AR applications."
"Design Patterns for Situated Visualization in Augmented Reality.","2024","IEEE Trans. Vis. Comput. Graph.","Benjamin Lee, Michael Sedlmair, Dieter Schmalstieg","Journal Articles","https://dblp.org/rec/journals/tvcg/LeeSS24",""
"Design Space of Visual Feedforward And Corrective Feedback in XR-Based Motion Guidance Systems.","2024","CHI","Xingyao Yu, Benjamin Lee, Michael Sedlmair","Conference and Workshop Papers","https://dblp.org/rec/conf/chi/YuLS24","Extended reality (XR) technologies are highly suited in assisting individuals in learning motor skills and movements—referred to as motion guidance. In motion guidance, the “feedforward’’ provides instructional cues of the motions that are to be performed, whereas the “feedback’’ provides cues which help correct mistakes and minimize errors. Designing synergistic feedforward and feedback is vital to providing an effective learning experience, but this interplay between the two has not yet been adequately explored. Based on a survey of the literature, we propose design space for both motion feedforward and corrective feedback in XR, and describe the interaction effects between them. We identify common design approaches of XR-based motion guidance found in our literature corpus, and discuss them through the lens of our design dimensions. We then discuss additional contextual factors and considerations that influence this design, together with future research opportunities for motion guidance in XR."
"HiveFive360: Extending the VR Gaze Guidance Technique HiveFive to Highlight Out-Of-FOV Targets.","2024","MuC","Sophie Kergaßner, Nina Doerr, Markus Wieland, Martin Fuchs 0001, Michael Sedlmair","Conference and Workshop Papers","https://dblp.org/rec/conf/mc/KergassnerDW0S24",""
"Is it Part of Me? Exploring Experiences of Inclusive Avatar Use For Visible and Invisible Disabilities in Social VR.","2024","ASSETS","Katrin Angerbauer, Phoenix Van Wagoner, Tim Halach, Jonas Vogelsang, Natalie Hube, Andria L. Smith, Ksenia Keplinger, Michael Sedlmair","Conference and Workshop Papers","https://dblp.org/rec/conf/assets/AngerbauerWHVHS24",""
"Optimally Ordered Orthogonal Neighbor Joining Trees for Hierarchical Cluster Analysis.","2024","IEEE Trans. Vis. Comput. Graph.","Tong Ge, Xu Luo, Yunhai Wang, Michael Sedlmair, Zhanglin Cheng, Ying Zhao 0001, Xin Liu 0007, Oliver Deussen, Baoquan Chen","Journal Articles","https://dblp.org/rec/journals/tvcg/GeLWSCZLDC24","We propose to use optimally ordered orthogonal neighbor-joining (O <inline-formula><tex-math notation=""LaTeX"">$^{3}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>3</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=""wang-ieq1-3284499.gif""/></alternatives></inline-formula> NJ) trees as a new way to visually explore cluster structures and outliers in multi-dimensional data. Neighbor-joining (NJ) trees are widely used in biology, and their visual representation is similar to that of dendrograms. The core difference to dendrograms, however, is that NJ trees correctly encode distances between data points, resulting in trees with varying edge lengths. We optimize NJ trees for their use in visual analysis in two ways. First, we propose to use a novel leaf sorting algorithm that helps users to better interpret adjacencies and proximities within such a tree. Second, we provide a new method to visually distill the cluster tree from an ordered NJ tree. Numerical evaluation and three case studies illustrate the benefits of this approach for exploring multi-dimensional data in areas such as biology or image analysis."
"Scalability in Visualization.","2024","IEEE Trans. Vis. Comput. Graph.","Gaëlle Richer, Alexis Pister, Moataz Abdelaal, Jean-Daniel Fekete, Michael Sedlmair, Daniel Weiskopf","Journal Articles","https://dblp.org/rec/journals/tvcg/RicherPAFSW24","We introduce a conceptual model for scalability designed for visualization research. With this model, we systematically analyze over 120 visualization publications from 1990 to 2020 to characterize the different notions of scalability in these works. While many article have addressed scalability issues, our survey identifies a lack of consistency in the use of the term in the visualization research community. We address this issue by introducing a consistent terminology meant to help visualization researchers better characterize the scalability aspects in their research. It also helps in providing multiple methods for supporting the claim that a work is “scalable.” Our model is centered around an effort function with inputs and outputs. The inputs are the problem size and resources, whereas the outputs are the actual efforts, for instance, in terms of computational run time or visual clutter. We select representative examples to illustrate different approaches and facets of what scalability can mean in visualization literature. Finally, targeting the diverse crowd of visualization researchers without a scalability tradition, we provide a set of recommendations for how scalability can be presented in a clear and consistent way to improve fair comparison between visualization techniques and systems and foster reproducibility."
"Sitting Posture Recognition and Feedback: A Literature Review.","2024","CHI","Christian Krauter, Katrin Angerbauer, Aimée Sousa Calepso, Alexander Achberger, Sven Mayer, Michael Sedlmair","Conference and Workshop Papers","https://dblp.org/rec/conf/chi/KrauterACAMS24","Extensive sitting is unhealthy; thus, countermeasures are needed to react to the ongoing trend toward more prolonged sitting. A variety of studies and guidelines have long addressed the question of how we can improve our sitting habits. Nevertheless, sitting time is still increasing. Here, smart devices can provide a general overview of sitting habits for more nuanced feedback on the user’s sitting posture. Based on a literature review (N=223), including publications from engineering, computer science, medical sciences, electronics, and more, our work guides developers of posture systems. There is a large variety of approaches, with pressure-sensing hardware and visual feedback being the most prominent. We found factors like environment, cost, privacy concerns, portability, and accuracy important for deciding hardware and feedback types. Further, one should consider the user’s capabilities, preferences, and tasks. Regarding user studies for sitting posture feedback, there is a need for better comparability and for investigating long-term effects."
"Visual Highlighting for Situated Brushing and Linking.","2024","Comput. Graph. Forum","Nina Doerr, Benjamin Lee, Katarina Baricova, Dieter Schmalstieg, Michael Sedlmair","Journal Articles","https://dblp.org/rec/journals/cgf/DoerrLBSS24","Brushing and linking is widely used for visual analytics in desktop environments. However, using this approach to link many data items between situated (e.g., a virtual screen with data) and embedded views (e.g., highlighted objects in the physical environment) is largely unexplored. To this end, we study the effectiveness of visual highlighting techniques in helping users identify and link physical referents to brushed data marks in a situated scatterplot. In an exploratory virtual reality user study (N=20), we evaluated four highlighting techniques under different physical layouts and tasks. We discuss the effectiveness of these techniques, as well as implications for the design of brushing and linking operations in situated analytics."
"Visualizing, Exploring and Analyzing Big Data: A 6-Year Story.","2024","SIGMOD Rec.","Nikos Bikakis, George Papastefanatos, Panos K. Chrysanthis, Olga Papemmanuil, David Auber, Steffen Frey, Issei Fujishiro, Hanna Hauptmann, Shixia Liu, Kwan-Liu Ma, Tobias Schreck, Michael Sedlmair, Mohamed A. Sharaf","Journal Articles","https://dblp.org/rec/journals/sigmod/BikakisPCPAFFHLMSSS24","Information Visualization has been one of the cornerstones of Data Science, turning the abundance of Big Data being produced through modern systems into actionable knowledge. Indeed, the Big Data era has realized the availability of voluminous datasets that are dynamic, multidimensional, noisy and heterogeneous in nature. Transforming a data-curious user into someone who can access and analyze that data is even more burdensome now for a great number of users with little or no support and expertise on the data processing part."
"chARpack: The Chemistry Augmented Reality Package.","2024","J. Chem. Inf. Model.","Tobias Rau, Michael Sedlmair, Andreas Köhn","Journal Articles","https://dblp.org/rec/journals/jcisd/RauSK24","Off-loading visualization and interaction into virtual reality (VR) using head-mounted displays (HMDs) has gained considerable popularity in simulation sciences, particularly in chemical modeling. Because of its unique way of soft immersion, augmented reality (AR) HMD technology has even more potential to be integrated into the everyday workflow of computational chemists. In this work, we present our environment to explore the prospects of AR in chemistry and general molecular sciences: The chemistry in Augmented Reality package (chARpack). Besides providing an extensible framework, our software focuses on a seamless transition between a 3D stereoscopic view with true 3D interactions and the traditional desktop PC setup to provide users with the best setup for all tasks in their workflow. Using feedback from domain experts, we discuss our design requirements for this kind of hybrid working environment (AR + PC), regarding input, features, degree of immersion, and collaboration."
"Been There, Seen That: Visualization of Movement and 3D Eye Tracking Data from Real-World Environments.","2023","Comput. Graph. Forum","Nelusa Pathmanathan, Seyda Öney, Michael Becher, Michael Sedlmair, Daniel Weiskopf, Kuno Kurzhals","Journal Articles","https://dblp.org/rec/journals/cgf/PathmanathanOBSWK23","The distribution of visual attention can be evaluated using eye tracking, providing valuable insights into usability issues and interaction patterns. However, when used in real, augmented, and collaborative environments, new challenges arise that go beyond desktop scenarios and purely virtual environments. Toward addressing these challenges, we present a visualization technique that provides complementary views on the movement and eye tracking data recorded from multiple people in real‐world environments. Our method is based on a space‐time cube visualization and a linked 3D replay of recorded data. We showcase our approach with an experiment that examines how people investigate an artwork collection. The visualization provides insights into how people moved and inspected individual pictures in their spatial context over time. In contrast to existing methods, this analysis is possible for multiple participants without extensive annotation of areas of interest. Our technique was evaluated with a think‐aloud experiment to investigate analysis strategies and an interview with domain experts to examine the applicability in other research fields."
"Replication Data for: Been There, Seen That: Visualization of Movement and 3D Eye Tracking Data from Real-World Environments.","2023","DaRUS","Nelusa Pathmanathan, Seyda Öney, Michael Becher, Michael Sedlmair, Daniel Weiskopf, Kuno Kurzhals","Data and Artifacts","https://dblp.org/rec/data/10/PathmanathanOBSWK23a","AbstractThe distribution of visual attention can be evaluated using eye tracking, providing valuable insights into usability issues and interaction patterns. However, when used in real, augmented, and collaborative environments, new challenges arise that go beyond desktop scenarios and purely virtual environments. Toward addressing these challenges, we present a visualization technique that provides complementary views on the movement and eye tracking data recorded from multiple people in real‐world environments. Our method is based on a space‐time cube visualization and a linked 3D replay of recorded data. We showcase our approach with an experiment that examines how people investigate an artwork collection. The visualization provides insights into how people moved and inspected individual pictures in their spatial context over time. In contrast to existing methods, this analysis is possible for multiple participants without extensive annotation of areas of interest. Our technique was evaluated with a think‐aloud experiment to investigate analysis strategies and an interview with domain experts to examine the applicability in other research fields."
"Replication Data for: Visual Gaze Labeling for Augmented Reality Studies.","2023","DaRUS","Seyda Öney, Nelusa Pathmanathan, Michael Becher, Michael Sedlmair, Daniel Weiskopf, Kuno Kurzhals","Data and Artifacts","https://dblp.org/rec/data/10/OneyPBSWK23a","Augmented Reality (AR) provides new ways for situated visualization and human‐computer interaction in physical environments. Current evaluation procedures for AR applications rely primarily on questionnaires and interviews, providing qualitative means to assess usability and task solution strategies. Eye tracking extends these existing evaluation methodologies by providing indicators for visual attention to virtual and real elements in the environment. However, the analysis of viewing behavior, especially the comparison of multiple participants, is difficult to achieve in AR. Specifically, the definition of areas of interest (AOIs), which is often a prerequisite for such analysis, is cumbersome and tedious with existing approaches. To address this issue, we present a new visualization approach to define AOIs, label fixations, and investigate the resulting annotated scanpaths. Our approach utilizes automatic annotation of gaze on virtual objects and an image‐based approach that also considers spatial context for the manual annotation of objects in the real world. Our results show, that with our approach, eye tracking data from AR scenes can be annotated and analyzed flexibly with respect to data aspects and annotation strategies."
"Visual Gaze Labeling for Augmented Reality Studies.","2023","Comput. Graph. Forum","Seyda Öney, Nelusa Pathmanathan, Michael Becher, Michael Sedlmair, Daniel Weiskopf, Kuno Kurzhals","Journal Articles","https://dblp.org/rec/journals/cgf/OneyPBSWK23","Augmented Reality (AR) provides new ways for situated visualization and human‐computer interaction in physical environments. Current evaluation procedures for AR applications rely primarily on questionnaires and interviews, providing qualitative means to assess usability and task solution strategies. Eye tracking extends these existing evaluation methodologies by providing indicators for visual attention to virtual and real elements in the environment. However, the analysis of viewing behavior, especially the comparison of multiple participants, is difficult to achieve in AR. Specifically, the definition of areas of interest (AOIs), which is often a prerequisite for such analysis, is cumbersome and tedious with existing approaches. To address this issue, we present a new visualization approach to define AOIs, label fixations, and investigate the resulting annotated scanpaths. Our approach utilizes automatic annotation of gaze on virtual objects and an image‐based approach that also considers spatial context for the manual annotation of objects in the real world. Our results show, that with our approach, eye tracking data from AR scenes can be annotated and analyzed flexibly with respect to data aspects and annotation strategies."
"Active Haptic Feedback for a Virtual Wrist-Anchored User Interface.","2024","UIST","Jan Ulrich Bartels, Natalia Sanchez-Tamayo, Michael Sedlmair, Katherine J. Kuchenbecker","Conference and Workshop Papers","https://dblp.org/rec/conf/uist/BartelsSSK24",""
"An Image Quality Dataset with Triplet Comparisons for Multi-dimensional Scaling.","2024","QoMEX","Mohsen Jenadeleh, Frederik L. Dennig, René Cutura, Quynh Quang Ngo, Daniel A. Keim, Michael Sedlmair, Dietmar Saupe","Conference and Workshop Papers","https://dblp.org/rec/conf/qomex/JenadelehDCNKSS24","In the early days of perceptual image quality research more than 30 years ago, the multidimensionality of distortions in perceptual space was considered important. However, research focused on scalar quality as measured by mean opinion scores. With our work, we intend to revive interest in this relevant area by presenting a first pilot dataset of annotated triplet comparisons for image quality assessment. It contains one source stimulus together with distorted versions derived from 7 distortion types at 12 levels each. Our crowdsourced and curated dataset contains roughly 50,000 responses to 7,000 triplet comparisons. We show that the multidimensional embedding of the dataset poses a challenge for many established triplet embedding algorithms. Finally, we propose a new reconstruction algorithm, dubbed logistic triplet embedding (LTE) with Tikhonov regularization. It shows promising performance. This study helps researchers to create larger datasets and better embedding techniques for multidimensional image quality. The dataset includes images and ratings and can be accessed at https://github.com/jenadeleh/multidimensionalIQA-dataset/tree/main."
"An Image-based Typology for Visualization.","2024","CoRR","Jian Chen 0006, Petra Isenberg, Robert S. Laramee, Tobias Isenberg 0001, Michael Sedlmair, Torsten Möller, Rui Li 0067","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2403-05594",""
"Brushing and Linking for Situated Analytics.","2024","VR Workshops","Carlos Quijano-Chavez, Nina Doerr, Benjamin Lee, Dieter Schmalstieg, Michael Sedlmair","Conference and Workshop Papers","https://dblp.org/rec/conf/vr/Quijano-ChavezD24","Situated analytics is visual analysis that is embedded in the physical world. Conventionally, the data related to referents (i.e., physical objects) is manipulated indirectly, through dedicated interaction devices or using separate abstract representations. The natural way of interacting in 3D space using direct manipulation with one's hands is hardly employed when abstract data is concerned. In this paper, we explore the idea of directly brushing and linking referents in the real world to analyze abstract data related to the referents. We discuss what brushing & linking means when applied to the real world, including multiple ways of selecting referents (brushing) in augmented reality, as well as different ways of linking with an abstract data view, presented on a dedicated display (a tablet). A proof of concept was implemented to test and demonstrate the capabilities of brushing & linking of referents. We conclude with a set of open research challenges that exist in this new and emerging area."
"Eyes on the Task: Gaze Analysis of Situated Visualization for Collaborative Tasks.","2024","VR","Nelusa Pathmanathan, Tobias Rau, Xiliu Yang, Aimée Sousa Calepso, Felix Amtsberg, Achim Menges, Michael Sedlmair, Kuno Kurzhals","Conference and Workshop Papers","https://dblp.org/rec/conf/vr/PathmanathanRYCAMSK24","The use of augmented reality technology to support humans with situated visualization in complex tasks such as navigation or assembly has gained increasing importance in research and industrial applications. One important line of research regards supporting and understanding collaborative tasks. Analyzing collaboration patterns is usually done by conducting observations and interviews. To expand these methods, we argue that eye tracking can be used to extract further insights and quantify behavior. To this end, we contribute a study that uses eye tracking to investigate participant strategies for solving collaborative sorting and assembly tasks. We compare participants’ visual attention during situated instructions in AR and traditional paper-based instructions as a baseline. By investigating the performance and gaze behavior of the participants, different strategies for solving the provided tasks are revealed. Our results show that with situated visualization, participants focus more on task-relevant areas and require less discussion between collaboration partners to solve the task at hand."
"""In Your Face!"": Visualizing Fitness Tracker Data in Augmented Reality.","2023","CHI Extended Abstracts","Sebastian Rigling, Xingyao Yu, Michael Sedlmair","Conference and Workshop Papers","https://dblp.org/rec/conf/chi/RiglingYS23",""
"A Virtual Reality Simulator for Timber Fabrication Tasks Using Industrial Robotic Arms.","2023","MuC","Eric Bossecker, Aimée Sousa Calepso, Benjamin Kaiser, Alexander Verl, Michael Sedlmair","Conference and Workshop Papers","https://dblp.org/rec/conf/mc/BosseckerCKVS23",""
"Bees, Birds and Butterflies: Investigating the Influence of Distractors on Visual Attention Guidance Techniques.","2023","CHI Extended Abstracts","Nina Doerr, Katrin Angerbauer, Melissa Reinelt, Michael Sedlmair","Conference and Workshop Papers","https://dblp.org/rec/conf/chi/DoerrARS23","Visual attention guidance methods direct the viewer’s gaze in immersive environments by visually highlighting elements of interest. The highlighting can be done, for instance, by adding a colored circle around elements, adding animated swarms (HiveFive), or removing objects from one eye in a stereoscopic display (Deadeye). We contribute a controlled user experiment (N=30) comparing these three techniques under the influence of visual distractors, such as bees flying by. Our results show that Circle and HiveFive performed best in terms of task performance and qualitative feedback, and were largely robust against different levels of distractions. Furthermore, we discovered a high mental demand for Deadeye."
"Comparative Evaluation of Bipartite, Node-Link, and Matrix-Based Network Representations.","2023","IEEE Trans. Vis. Comput. Graph.","Moataz Abdelaal, Nathan Daniel Schiele, Katrin Angerbauer, Kuno Kurzhals, Michael Sedlmair, Daniel Weiskopf","Journal Articles","https://dblp.org/rec/journals/tvcg/AbdelaalSAKSW23",""
"Designing Situated Dashboards: Challenges and Opportunities.","2023","ISMAR-Adjunct","Anika Sayara, Benjamin Lee, Carlos Quijano-Chavez, Michael Sedlmair","Conference and Workshop Papers","https://dblp.org/rec/conf/ismar/SayaraLQS23","Situated Visualization is an emerging field that unites several areas - visualization, augmented reality, human-computer interaction, and internet-of-things, to support human data activities within the ubiquitous world. Likewise, dashboards are broadly used to simplify complex data through multiple views. However, dashboards are only adapted for desktop settings, and requires visual strategies to support situatedness. We propose the concept of AR-based situated dashboards and present design considerations and challenges developed over interviews with experts. These challenges aim to propose directions and opportunities for facilitating the effective designing and authoring of situated dashboards."
"ManuKnowVis: How to Support Different User Groups in Contextualizing and Leveraging Knowledge Repositories.","2023","IEEE Trans. Vis. Comput. Graph.","Joscha Eirich, Dominik Jäckle, Michael Sedlmair, Christoph Wehner, Ute Schmid, Jürgen Bernard, Tobias Schreck","Journal Articles","https://dblp.org/rec/journals/tvcg/EirichJSWSBS23",""
"Predicting User Preferences of Dimensionality Reduction Embedding Quality.","2023","IEEE Trans. Vis. Comput. Graph.","Cristina Morariu, Adrien Bibal, René Cutura, Benoît Frénay, Michael Sedlmair","Journal Articles","https://dblp.org/rec/journals/tvcg/MorariuBCFS23","A plethora of dimensionality reduction techniques have emerged over the past decades, leaving researchers and analysts with a wide variety of choices for reducing their data, all the more so given some techniques come with additional hyper-parametrization (e.g., t-SNE, UMAP, etc.). Recent studies are showing that people often use dimensionality reduction as a black-box regardless of the specific properties the method itself preserves. Hence, evaluating and comparing 2D embeddings is usually qualitatively decided, by setting embeddings side-by-side and letting human judgment decide which embedding is the best. In this work, we propose a quantitative way of evaluating embeddings, that nonetheless places human perception at the center. We run a comparative study, where we ask people to select “good” and “misleading” views between scatterplots of low-dimensional embeddings of image datasets, simulating the way people usually select embeddings. We use the study data as labels for a set of quality metrics for a supervised machine learning model whose purpose is to discover and quantify what exactly people are looking for when deciding between embeddings. With the model as a proxy for human judgments, we use it to rank embeddings on new datasets, explain why they are relevant, and quantify the degree of subjectivity when people select preferred embeddings."
"RagRug: A Toolkit for Situated Analytics.","2023","IEEE Trans. Vis. Comput. Graph.","Philipp Fleck, Aimée Sousa Calepso, Sebastian Hubenschmid, Michael Sedlmair, Dieter Schmalstieg","Journal Articles","https://dblp.org/rec/journals/tvcg/FleckCHSS23",""
"Reading Strategies for Graph Visualizations that Wrap Around in Torus Topology.","2023","ETRA","Kun-Ting Chen, Quynh Quang Ngo, Kuno Kurzhals, Kim Marriott, Tim Dwyer, Michael Sedlmair, Daniel Weiskopf","Conference and Workshop Papers","https://dblp.org/rec/conf/etra/ChenNKMDSW23","We investigate reading strategies for node-link diagrams that wrap around the boundaries in a flattened torus topology by examining eye tracking data recorded in a previous controlled study. Prior work showed that torus drawing affords greater flexibility in clutter reduction than traditional node-link representations, but impedes link-and-path exploration tasks, while repeating tiles around boundaries aids comprehension. However, it remains unclear what strategies users apply in different wrapping settings. This is important for design implications for future work on more effective wrapped visualizations for network applications, and cyclic data that could benefit from wrapping. We perform visual-exploratory data analysis of gaze data, and conduct statistical tests derived from the patterns identified. Results show distinguishable gaze behaviors, with more visual glances and transitions between areas of interest in the non-replicated layout. Full-context has more successful visual searches than partial-context, but the gaze allocation indicates that the layout could be more space-efficient."
"Touching data with PropellerHand.","2023","J. Vis.","Alexander Achberger, Frank Heyen, Kresimir Vidackovic, Michael Sedlmair","Journal Articles","https://dblp.org/rec/journals/jvis/AchbergerHVS23","Abstract
                Immersive analytics often takes place in virtual environments which promise the users immersion. To fulfill this promise, sensory feedback, such as haptics, is an important component, which is however not well supported yet. Existing haptic devices are often expensive, stationary, or occupy the user’s hand, preventing them from grasping objects or using a controller. We propose PropellerHand, an ungrounded hand-mounted haptic device with two rotatable propellers, that allows exerting forces on the hand without obstructing hand use. PropellerHand is able to simulate feedback such as weight and torque by generating thrust up to 11 N in 2-DOF and a torque of 1.87 Nm in 2-DOF. Its design builds on our experience from quantitative and qualitative experiments with different form factors and parts. We evaluated our prototype through a qualitative user study in various VR scenarios that required participants to manipulate virtual objects in different ways, while changing between torques and directional forces. Results show that PropellerHand improves users’ immersion in virtual reality. Additionally, we conducted a second user study in the field of immersive visualization to investigate the potential benefits of PropellerHand there.
              
                Graphical abstract"
"Usability Evaluation of an Augmented Reality System for Collaborative Fabrication between Multiple Humans and Industrial Robots.","2023","SUI","Xiliu Yang, Aimée Sousa Calepso, Felix Amtsberg, Achim Menges, Michael Sedlmair","Conference and Workshop Papers","https://dblp.org/rec/conf/sui/YangCAMS23","Semi-automated timber fabrication tasks demand the expertise and dexterity of human workers in addition to the use of automated robotic systems. In this paper, we introduce a human-robot collaborative system based on Augmented Reality (AR). To assess our approach, we conducted an exploratory user study on a head-mounted display (HMD) interface for task sharing between humans and an industrial robotic platform (N=16). Instead of screen-based interfaces, HMDs allowed users to receive information in-situ, regardless of their location in the workspace and the need to use their hands to handle tools or carry out tasks. We analyzed the resulting open-ended, qualitative user feedback as well as the quantitative user experience data. From the results, we derived challenges to tackle in future implementations and questions that need to be investigated to improve AR-based HRI in fabrication scenarios. The results also suggest that some aspects of human-robot interaction, like communication and trust, are more prominent when implementing a non-dyadic scenario and dealing with larger robots. The study is intended as a prequel to future work into AR-based collaboration between multiple humans and industrial robots."
"VR, Gaze, and Visual Impairment: An Exploratory Study of the Perception of Eye Contact across different Sensory Modalities for People with Visual Impairments in Virtual Reality.","2023","CHI Extended Abstracts","Markus Wieland, Michael Sedlmair, Tonja-Katrin Machulla","Conference and Workshop Papers","https://dblp.org/rec/conf/chi/WielandSM23","As social virtual reality (VR) becomes more popular, avatars are being designed with realistic behaviors incorporating non-verbal cues like eye contact. However, perceiving eye contact during a conversation can be challenging for people with visual impairments. VR presents an opportunity to display eye contact cues in alternative ways, making them perceivable for people with visual impairments. We performed an exploratory study to gain initial insights on designing eye contact cues for people with visual impairments, including a focus group for a deeper understanding of the topic. We implemented eye contact cues via visual, auditory, and tactile sensory modalities in VR and tested these approaches with eleven participants with visual impairments and collected qualitative feedback. The results show that visual cues indicating the gaze direction were preferred, but auditory and tactile cues were also prevalent as they do not superimpose additional visual information."
"Visual Overviews for Sheet Music Structure.","2023","ISMIR","Frank Heyen, Quynh Quang Ngo, Michael Sedlmair","Conference and Workshop Papers","https://dblp.org/rec/conf/ismir/HeyenNS23",""
"What's (Not) Tracking? Factors of Influence in Industrial Augmented Reality Tracking: A Use Case Study in an Automotive Environment.","2023","AutomotiveUI","Jonas Haischt, Michael Sedlmair","Conference and Workshop Papers","https://dblp.org/rec/conf/automotiveUI/HaischtS23","Augmented Reality (AR) is a key technology for digitization in enterprises. However, often there is a lack of stable tracking solutions when used inside manufacturing environments. Many different tracking technologies are available, yet, it can be difficult to choose the most appropriate tracking solution for different use cases with their varying conditions. In order to shed light on common tracking requirements and conditions for automotive AR use cases we conducted a use case study spanning 61 use cases within the complete product life-cycle of a large automotive manufacturer. By analyzing the gathered data we were able to note the frequency of different tracking requirements and conditions within automotive AR use cases. Based on these use cases we could also derive common factors of influence for AR tracking in the automotive industry, which show the various challenges automotive AR tracking is currently facing."
"guitARhero: Interactive Augmented Reality Guitar Tutorials.","2023","IEEE Trans. Vis. Comput. Graph.","Lucchas Ribeiro Skreinig, Denis Kalkofen, Ana Stanescu 0003, Peter Mohr, Frank Heyen, Shohei Mori, Michael Sedlmair, Dieter Schmalstieg, Alexander Plopski","Journal Articles","https://dblp.org/rec/journals/tvcg/SkreinigKSMHMSSP23",""
"Auxiliary Means to Improve Motion Guidance Memorability in Extended Reality.","2023","VR Workshops","Patrick Gebhardt, Maximilian Weiß, Pascal Huszár, Xingyao Yu, Alexander Achberger, Xiaobing Zhang, Michael Sedlmair","Conference and Workshop Papers","https://dblp.org/rec/conf/vr/GebhardtWHYAZS23","VR-based motion guidance systems can provide 3D movement instructions and real-time feedback for practicing movement without a live instructor. However, the precise visualization of movement paths or postures may be insufficient to learn a new motor skill, as they might make users too dependent and lead to poor performance when there is no guidance. In this paper, we propose to use enhanced error visualization, asymptotic path, increasing transparency, and haptic constraint to improve the memorability of motion guidance. Our study results indicated that adding an enhanced error feedback visualization helped the users with short-term retention."
"Design Patterns for Situated Visualization in Augmented Reality.","2023","CoRR","Benjamin Lee, Michael Sedlmair, Dieter Schmalstieg","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2307-09157",""
"Exploring Augmented Reality for Situated Analytics with Many Movable Physical Referents.","2023","VRST","Aimée Sousa Calepso, Philipp Fleck, Dieter Schmalstieg, Michael Sedlmair","Conference and Workshop Papers","https://dblp.org/rec/conf/vrst/CalepsoFSS23","Situated analytics (SitA) uses visualization in the context of physical referents, typically by using augmented reality (AR). We want to pave the way toward studying SitA in more suitable and realistic settings. Toward this goal, we contribute a testbed to evaluate SitA based on a scenario in which participants play the role of a museum curator and need to organize an exhibition of music artifacts. We conducted two experiments: First, we evaluated an AR headset interface and the testbed itself in an exploratory manner. Second, we compared the AR headset to a tablet interface. We summarize the lessons learned as guidance for designing and evaluating SitA."
"LSDvis: Hallucinatory Data Visualisations in Real World Environments.","2023","CoRR","Ari Kouts, Lonni Besançon, Michael Sedlmair, Benjamin Lee","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2312-11144","We propose the concept of""LSDvis"": the (highly exaggerated) visual blending of situated visualisations and the real-world environment to produce data representations that resemble hallucinations. Such hallucinatory visualisations incorporate elements of the physical environment, twisting and morphing their appearance such that they become part of the visualisation itself. We demonstrate LSDvis in a ``proof of proof of concept'', where we use Stable Diffusion to modify images of real environments with abstract data visualisations as input. We conclude by discussing considerations of LSDvis. We hope that our work promotes visualisation designs which deprioritise saliency in favour of quirkiness and ambience."
"Visual Ensemble Analysis of Fluid Flow in Porous Media across Simulation Codes and Experiment.","2023","CoRR","Ruben Bauer, Quynh Quang Ngo, Guido Reina, Steffen Frey, Bernd Flemisch, Helwig Hauser, Thomas Ertl, Michael Sedlmair","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2302-05447","AbstractWe study the question of how visual analysis can support the comparison of spatio-temporal ensemble data of liquid and gas flow in porous media. To this end, we focus on a case study, in which nine different research groups concurrently simulated the process of injecting CO$$_2$$
                  
                    
                    2
                  
                 into the subsurface. We explore different data aggregation and interactive visualization approaches to compare and analyze these nine simulations. In terms of data aggregation, one key component is the choice of similarity metrics that define the relationship between different simulations. We test different metrics and find that using the machine-learning model “S4” (tailored to the present study) as metric provides the best visualization results. Based on that, we propose different visualization methods. For overviewing the data, we use dimensionality reduction methods that allow us to plot and compare the different simulations in a scatterplot. To show details about the spatio-temporal data of each individual simulation, we employ a space-time cube volume rendering. All views support linking and brushing interaction to allow users to select and highlight subsets of the data simultaneously across multiple views. We use the resulting interactive, multi-view visual analysis tool to explore the nine simulations and also to compare them to data from experimental setups. Our main findings include new insights into ranking of simulation results with respect to experimental data, and the development of gravity fingers in simulations."
"Visual Guitar Tab Comparison.","2023","CoRR","Frank Heyen, Alejandro Gabino Diaz Mendoza, Quynh Quang Ngo, Michael Sedlmair","Informal and Other Publications","https://dblp.org/rec/journals/corr/abs-2311-14726",""
"Visual Planning and Analysis of Latin Formation Dance Patterns.","2023","EuroVis","Samuel Beck, Nina Doerr, Fabian Schmierer, Michael Sedlmair, Steffen Koch 0001","Conference and Workshop Papers","https://dblp.org/rec/conf/vissym/BeckDSS023","Latin formation dancing is a team sport in which up to eight couples perform a coordinated choreography. A central part are the patterns formed by the dancers on the dance floor and the transitions between them. Planning and practicing patterns are some of the most challenging aspects of Latin formation dancing. Interactive visualization approaches can support instructors as well as dancers in tackling these challenges. We present a web-based visualization prototype that assists with the planning, training, and analysis of patterns. Its design was iteratively developed with the involvement of experienced formation instructors. The interface offers views of the dancers’ positions and orientations, pattern transitions, poses, and analytical information like dance floor utilization and movement distances. In a first expert study with formation instructors, the prototype was well received."
"Work vs. Leisure - Differences in Avatar Characteristics Depending on Social Situations.","2023","VINCI","Natalie Hube, Melissa Reinelt, Kresimir Vidackovic, Michael Sedlmair","Conference and Workshop Papers","https://dblp.org/rec/conf/vinci/HubeRVS23","User avatars are a critical component in collaborative Virtual Environments. A multitude of tools exist, employing various avatar types for self-representation and the representation of others. However, the optimal appearance for these avatars remains unclear. Consumer applications predominantly utilize stylized avatars, which are less prevalent in enterprise sectors. To investigate users’ avatar preferences, we conducted three user studies in both non-immersive and immersive environments, exploring whether social contexts influence virtual self-avatar selection. We generated individual avatars based on photographs of 91 participants, comprising 71 employees in the automotive industry and 20 individuals external to the company. Our findings indicate that work situations, irrespective of the occupational domain, significantly impact self-avatar choices. Conversely, we observed no correlation between the display medium or personality dimensions and avatar selection. Drawing upon our results, we provide recommendations for future avatar representation in work environments."
